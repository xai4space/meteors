{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\u2604\ufe0f\ud83d\udef0\ufe0f Meteors","text":""},{"location":"#introduction","title":"\ud83d\udef0\ufe0f Introduction","text":"<p>Meteors is an open-source package for creating explanations of hyperspectral and multispectral images. Developed primarily for Pytorch models, Meteors was inspired by the Captum library. Our goal is to provide not only the ability to create explanations for hyperspectral images but also to visualize them in a user-friendly way.</p> <p>Please note that this package is still in the development phase, and we welcome any feedback and suggestions to help improve the library.</p> <p>Meteors emerged from a research grant project between the Warsaw University of Technology research group MI2.ai and Kp Labs, financially supported by the European Space Agency (ESA).</p>"},{"location":"#target-audience","title":"\ud83c\udfaf Target Audience","text":"<p>Meteors is designed for:</p> <ul> <li>Researchers, data scientists, and developers who work with hyperspectral and multispectral images and want to understand the decisions made by their models.</li> <li>Engineers who build models for production and want to troubleshoot through improved model interpretability.</li> <li>Developers seeking to deliver better explanations to end users on why they're seeing specific content.</li> </ul>"},{"location":"#installation","title":"\ud83d\udce6 Installation","text":"<p>Requirements</p> <ul> <li>Python &gt;= 3.9</li> <li>PyTorch &gt;= 1.10</li> <li>Captum &gt;= 0.7.0</li> </ul> <p>Install with <code>pip</code>:</p> <pre><code>pip install meteors\n</code></pre> <p>With conda: Coming soon</p>"},{"location":"#documentation","title":"\ud83d\udcda Documentation","text":"<p>Please refer to the documentation for more information on how to use Meteors.</p>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>As an open-source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation.</p> <p>We use rye as our project and package management tool. To start developing, follow these steps:</p> <pre><code>curl -sSf https://rye.astral.sh/get | bash # Install Rye\nrye pin &lt;python version &gt;=3.9&gt; # Pin the python version\nrye sync # Sync the environment\n</code></pre> <p>Before pushing your changes, please run the tests and the linter:</p> <pre><code>rye test\nrye run pre-commit run --all-files\n</code></pre> <p>For more information on how to contribute, please refer to our Contributing Guide.</p> <p>Thank you for considering contributing to Meteors!</p>"},{"location":"#contributors","title":"\ud83d\udcab Contributors","text":""},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v012-2024-11-21","title":"v0.1.2 (2024-11-21)","text":""},{"location":"changelog/#chores","title":"\ud83d\udece\ufe0f Chores","text":"<ul> <li>update the package version (#142)</li> </ul>"},{"location":"changelog/#bug-fixes","title":"\ud83e\ude7a Bug Fixes","text":"<ul> <li>documentation version alias update fixed (#139)</li> </ul>"},{"location":"changelog/#documentation","title":"\ud83d\udcda Documentation","text":"<ul> <li>tutorials fixes (#129)</li> <li>fixed documentation versioning (#135)</li> <li>Documentation template update (#134)</li> <li>fix reference docs for visualizer modules (#133)</li> <li>fix navbar for new tutorials (#132)</li> <li>update changelog.md for v0.1.1 [skip ci] (#131)</li> </ul>"},{"location":"changelog/#features","title":"\ud83d\udd28 Features","text":"<ul> <li>Added Hyperlinks to the PR in changelog (#136)</li> </ul>"},{"location":"changelog/#v011-2024-11-18","title":"v0.1.1 (2024-11-18)","text":""},{"location":"changelog/#documentation_1","title":"\ud83d\udcda Documentation","text":"<ul> <li>refine the tutorial for lime and add tutorial for attributions methods (#128)</li> <li>refine the example for segmentation problem type (#124)</li> </ul>"},{"location":"changelog/#bug-fixes_1","title":"\ud83e\ude7a Bug Fixes","text":"<ul> <li>Moved the postprocessing to the ExplainableModel (#123)</li> <li>Corrected visualisers (#118)</li> <li>Corrected Attributes Functionalities (#117)</li> <li>changelog.md and release notes do not contain all commits due to the commit processor not parsing multi-line commit messages. (#116)</li> </ul>"},{"location":"changelog/#features_1","title":"\ud83d\udd28 Features","text":"<ul> <li>Updated release and docs GitHub actions to trigger when GitHub tag is pushed (#119)</li> </ul>"},{"location":"changelog/#v010-2024-10-29","title":"v0.1.0 (2024-10-29)","text":""},{"location":"changelog/#chores_1","title":"\ud83d\udece\ufe0f Chores","text":"<ul> <li>update the project version (#112)</li> <li>reduced coverage threshold (#94)</li> </ul>"},{"location":"changelog/#features_2","title":"\ud83d\udd28 Features","text":"<ul> <li>96 feat segmentation support for attribution methods (#103)</li> <li>refactored the package structure (#111)</li> <li>Custom errors (#101)</li> </ul>"},{"location":"changelog/#bug-fixes_2","title":"\ud83e\ude7a Bug Fixes","text":"<ul> <li>corrected visualizations (#106)</li> <li>Updated occlusion (#93)</li> <li>91 docs the lime illustration image is missing in the docs (#92)</li> </ul>"},{"location":"changelog/#documentation_2","title":"\ud83d\udcda Documentation","text":"<ul> <li>update changelog.md for 0.0.4 [skip ci] (#90)</li> </ul>"},{"location":"changelog/#v004-2024-09-25","title":"v0.0.4 (2024-09-25)","text":""},{"location":"changelog/#bug-fixes_3","title":"\ud83e\ude7a Bug Fixes","text":"<ul> <li>infinite loop in segmentation (#87)</li> </ul>"},{"location":"changelog/#features_3","title":"\ud83d\udd28 Features","text":"<ul> <li>HyperNoiseTunnel and captum attribution methods (#51)</li> </ul>"},{"location":"changelog/#v003-2024-09-23","title":"v0.0.3 (2024-09-23)","text":""},{"location":"changelog/#bug-fixes_4","title":"\ud83e\ude7a Bug Fixes","text":"<ul> <li>github action release workflow to pypi (#83)</li> </ul>"},{"location":"changelog/#meteors-002-2024-08-11","title":"meteors 0.0.2 (2024-08-11)","text":"<ul> <li>No release</li> <li>Refined package structure - simple modules for models and visualisation, installation using toml file</li> <li>Spectral attributions using LIME</li> <li>CUDA compatibility of LIME</li> </ul>"},{"location":"changelog/#meteors-001-2024-06-02","title":"meteors 0.0.1 (2024-06-02)","text":"<ul> <li>No release</li> <li>Prepared a simple draft of package along with some ideas and sample files for implementation of LIME for hyperspectral images.</li> <li>Segmentation mask for LIME using slic</li> <li>Spatial attributions using LIME</li> </ul>"},{"location":"how-to-guides/","title":"\ud83e\udd1d How to Contribute","text":"<p>Thank you for your interest in contributing to Meteors! We welcome contributions from the community to help improve and expand the library. This guide will walk you through the process of contributing to the project.</p>"},{"location":"how-to-guides/#types-of-contributions","title":"\ud83d\udccb Types of Contributions","text":"<p>There are several ways you can contribute to Meteors:</p> <ul> <li>\ud83d\udc1b Reporting bugs</li> <li>\ud83d\udca1 Suggesting new features or enhancements</li> <li>\ud83d\udcd6 Improving documentation</li> <li>\ud83d\udcbb Writing code (fixing bugs, implementing new features)</li> <li>\ud83e\uddea Adding or improving test cases</li> <li>\ud83d\udce3 Spreading the word about Meteors</li> </ul>"},{"location":"how-to-guides/#getting-started","title":"\ud83c\udf3f Getting Started","text":"<p>To start contributing to Meteors, follow these steps:</p> <ol> <li>\ud83c\udf74 Fork the Meteors repository on GitHub.</li> <li>\ud83d\udce5 Clone your forked repository to your local machine.</li> <li>\ud83d\udd00 Create a new branch for your contribution.</li> <li>\ud83d\udce6 Set rye <code>rye pin &lt;python_version&gt; &amp;&amp; rye sync</code>.</li> <li>\ud83d\udee0\ufe0f Make your changes or additions.</li> <li>\u2705 Test your changes to ensure they work as expected <code>rye test</code>.</li> <li>\ud83e\uddf9 Lint your code <code>rye run pre-commit run --all-files</code>.</li> <li>\ud83d\udcdd Commit your changes with a clear and descriptive commit message.</li> <li>\ud83d\udce4 Push your changes to your forked repository.</li> <li>\ud83d\udd0c Submit a pull request to the main Meteors repository.</li> <li>\ud83d\udcdd Fill out the pull request template with the necessary information.</li> </ol>"},{"location":"how-to-guides/#code-guidelines","title":"\ud83d\udcd0 Code Guidelines","text":"<p>When contributing code to Meteors, please follow these guidelines:</p> <ul> <li>\ud83d\udcda Document your code using docstrings and comments.</li> <li>\u2705 Write unit tests for new features or bug fixes.</li> <li>\ud83e\uddf9 Ensure your code is clean, readable, and well-structured.</li> <li>\ud83d\udea8 Run the existing tests and make sure they pass before submitting a pull request.</li> <li>\ud83d\udc0d Run linter to clean the code.</li> </ul>"},{"location":"how-to-guides/#code-of-conduct","title":"\ud83d\udcdc Code of Conduct","text":"<p>Please note that by contributing to Meteors, you are expected to adhere to our Code of Conduct. Be respectful, inclusive, and considerate in your interactions with others.</p>"},{"location":"how-to-guides/#recognition","title":"\ud83d\ude4c Recognition","text":"<p>We appreciate all contributions to Meteors, and we make sure to recognize our contributors. Your name will be added to the list of contributors in the project's README file.</p> <p>If you have any questions or need further assistance, feel free to reach out to the maintainers or open an issue on the GitHub repository.</p> <p>Thank you for your contribution to Meteors! \ud83c\udf89\ud83d\ude80</p>"},{"location":"quickstart/","title":"\ud83d\ude80 Quickstart","text":"<p>Welcome to the Quickstart Guide for Meteors! This guide will walk you through the basic steps to get started with using Meteors for explaining your hyperspectral and multispectral image models.</p>"},{"location":"quickstart/#prerequisites","title":"\ud83d\udccb Prerequisites","text":"<p>Before you begin, make sure you have the following installed:</p> <ul> <li>Python &gt;= 3.9</li> <li>PyTorch &gt;= 1.10</li> <li>Captum &gt;= 0.7.0</li> </ul>"},{"location":"quickstart/#installation","title":"\ud83d\udce5 Installation","text":"<p>To install Meteors, simply run the following command:</p> <pre><code>pip install meteors\n</code></pre> <p>For conda users, we will provide a conda installation method in the near future. We promise!\ud83e\udd1e</p>"},{"location":"quickstart/#basic-hyperspectral-or-multispectral-data-object","title":"\ud83c\udf1f Basic Hyperspectral or Multispectral Data Object","text":"<p>Meteors provide an easy-to-use object for handling hyperspectral and multispectral images. The <code>HSI</code> object is a simple way to process and manipulate your data.</p> <pre><code>from meteors import HSI\n</code></pre> <p>Remember, when providing data to the model, make sure it is in the final format that the model expects, without the batch dimension. The <code>HSI</code> object will handle the rest. We also recommend providing the image data channel orientation, height, width, and the number of channels in the format <code>(CHW)</code>. For example:</p> <ul> <li>Number of channels: C</li> <li>Height: H</li> <li>Width: W</li> </ul>"},{"location":"quickstart/#explanation-methods","title":"\ud83d\udd0d Explanation Methods","text":"<p>Meteors provides several explanation methods for hyperspectral and multispectral images, including:</p> <ul> <li>LIME: Local Interpretable Model-agnostic Explanations</li> <li>More methods coming soon!</li> </ul> <p>To use a specific explanation method in tutorials we provide for each method, example code.</p>"},{"location":"quickstart/#visualization-options","title":"\ud83c\udfa8 Visualization Options","text":"<p>Meteors offers various visualization options to help you understand and interpret the explanations in package <code>meteors.visualize</code>.</p> <pre><code>from meteors.visualize import visualize_spectral_attributes, visualize_spatial_attributes\n</code></pre>"},{"location":"quickstart/#tutorials","title":"\ud83d\udcda Tutorials","text":"<p>We have several tutorials to help get you off the ground with Meteors. The tutorials are Jupyter notebooks and cover the basics along with demonstrating usage of Meteors .</p> <p>View the tutorials page here.</p>"},{"location":"quickstart/#api-reference","title":"\ud83d\udcd6 API Reference","text":"<p>For an in-depth reference of the various Meteors internals, see our API Reference.</p>"},{"location":"quickstart/#contributing","title":"\ud83d\ude4c Contributing","text":"<p>We welcome contributions to Meteors! Please refer to our Contributing Guide for more information on how to get involved.</p>"},{"location":"reference/","title":"API Reference","text":"<p>The architecture of the package can be seen on the UML diagram: </p>"},{"location":"reference/#hyperspectral-image","title":"HyperSpectral Image","text":""},{"location":"reference/#src.meteors.hsi.HSI","title":"<code>HSI</code>","text":"<p>A dataclass for hyperspectral image data, including the image, wavelengths, and binary mask.</p> <p>Attributes:</p> Name Type Description <code>image</code> <code>Tensor</code> <p>The hyperspectral image data as a PyTorch tensor.</p> <code>wavelengths</code> <code>Tensor</code> <p>The wavelengths present in the image.</p> <code>orientation</code> <code>tuple[str, str, str]</code> <p>The orientation of the image data.</p> <code>device</code> <code>device</code> <p>The device to be used for inference.</p> <code>binary_mask</code> <code>Tensor</code> <p>A binary mask used to cover unimportant parts of the image.</p> Source code in <code>src/meteors/hsi.py</code> <pre><code>class HSI(BaseModel):\n    \"\"\"A dataclass for hyperspectral image data, including the image, wavelengths, and binary mask.\n\n    Attributes:\n        image (torch.Tensor): The hyperspectral image data as a PyTorch tensor.\n        wavelengths (torch.Tensor): The wavelengths present in the image.\n        orientation (tuple[str, str, str]): The orientation of the image data.\n        device (torch.device): The device to be used for inference.\n        binary_mask (torch.Tensor): A binary mask used to cover unimportant parts of the image.\n    \"\"\"\n\n    image: Annotated[  # Should always be a first field\n        torch.Tensor,\n        PlainValidator(ensure_image_tensor),\n        Field(description=\"Hyperspectral image. Converted to torch tensor.\"),\n    ]\n    wavelengths: Annotated[\n        torch.Tensor,\n        PlainValidator(ensure_wavelengths_tensor),\n        Field(description=\"Wavelengths present in the image. Defaults to None.\"),\n    ]\n    orientation: Annotated[\n        tuple[str, str, str],\n        PlainValidator(validate_orientation),\n        Field(\n            description=(\n                'Orientation of the image - sequence of three one-letter strings in any order: \"C\", \"H\", \"W\" '\n                'meaning respectively channels, height and width of the image. Defaults to (\"C\", \"H\", \"W\").'\n            ),\n        ),\n    ] = (\"C\", \"H\", \"W\")\n    device: Annotated[\n        torch.device,\n        PlainValidator(resolve_inference_device_hsi),\n        Field(\n            validate_default=True,\n            exclude=True,\n            description=\"Device to be used for inference. If None, the device of the input image will be used. Defaults to None.\",\n        ),\n    ] = None\n    binary_mask: Annotated[\n        torch.Tensor,\n        PlainValidator(process_and_validate_binary_mask),\n        Field(\n            validate_default=True,\n            description=(\n                \"Binary mask used to cover not important parts of the base image, masked parts have values equals to 0. \"\n                \"Converted to torch tensor. Defaults to None.\"\n            ),\n        ),\n    ] = None\n\n    @property\n    def spectral_axis(self) -&gt; int:\n        \"\"\"Returns the index of the spectral (wavelength) axis based on the current data orientation.\n\n        In hyperspectral imaging, the spectral axis represents the dimension along which\n        different spectral bands or wavelengths are arranged. This property dynamically\n        determines the index of this axis based on the current orientation of the data.\n\n        Returns:\n            int: The index of the spectral axis in the current data structure.\n                - 0 for 'CHW' or 'CWH' orientations (Channel/Wavelength first)\n                - 2 for 'HWC' or 'WHC' orientations (Channel/Wavelength last)\n                - 1 for 'HCW' or 'WCH' orientations (Channel/Wavelength in the middle)\n\n        Note:\n            The orientation is typically represented as a string where:\n            - 'C' represents the spectral/wavelength dimension\n            - 'H' represents the height (rows) of the image\n            - 'W' represents the width (columns) of the image\n\n        Examples:\n            &gt;&gt;&gt; hsi_image = HSI()\n            &gt;&gt;&gt; hsi_image.orientation = \"CHW\"\n            &gt;&gt;&gt; hsi_image.spectral_axis\n            0\n            &gt;&gt;&gt; hsi_image.orientation = \"HWC\"\n            &gt;&gt;&gt; hsi_image.spectral_axis\n            2\n        \"\"\"\n        return get_channel_axis(self.orientation)\n\n    @property\n    def spatial_binary_mask(self) -&gt; torch.Tensor:\n        \"\"\"Returns a 2D spatial representation of the binary mask.\n\n        This property extracts a single 2D slice from the 3D binary mask, assuming that\n        the mask is identical across all spectral bands. It handles different data\n        orientations by first ensuring the spectral dimension is the last dimension\n        before extracting the 2D spatial mask.\n\n        Returns:\n            torch.Tensor: A 2D tensor representing the spatial binary mask.\n                The shape will be (H, W) where H is height and W is width of the image.\n\n        Note:\n            - This assumes that the binary mask is consistent across all spectral bands.\n            - The returned mask is always 2D, regardless of the original data orientation.\n\n        Examples:\n            &gt;&gt;&gt; # If self.binary_mask has shape (100, 100, 5) with spectral_axis=2:\n            &gt;&gt;&gt; hsi_image = HSI(binary_mask=torch.rand(100, 100, 5), orientation=(\"H\", \"W\", \"C\"))\n            &gt;&gt;&gt; hsi_image.spatial_binary_mask.shape\n            torch.Size([100, 100])\n            &gt;&gt;&gt; If self.binary_mask has shape (5, 100, 100) with spectral_axis=0:\n            &gt;&gt;&gt; hsi_image = HSI(binary_mask=torch.rand(5, 100, 100), orientation=(\"C\", \"H\", \"W\"))\n            &gt;&gt;&gt; hsi_image.spatial_binary_mask.shape\n            torch.Size([100, 100])\n        \"\"\"\n        mask = self.binary_mask if self.binary_mask is not None else torch.ones_like(self.image)\n        return mask.select(dim=self.spectral_axis, index=0)\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @model_validator(mode=\"after\")\n    def validate_image_data(self) -&gt; Self:\n        \"\"\"Validates the image data by checking the shape of the wavelengths, image, and spectral_axis.\n\n        Returns:\n            Self: The instance of the class.\n        \"\"\"\n        validate_shapes(self.wavelengths, self.image, self.spectral_axis)\n        return self\n\n    def to(self, device: str | torch.device) -&gt; Self:\n        \"\"\"Moves the image and binary mask (if available) to the specified device.\n\n        Args:\n            device (str or torch.device): The device to move the image and binary mask to.\n\n        Returns:\n            Self: The updated HSI object.\n\n        Examples:\n            &gt;&gt;&gt; # Create an HSI object\n            &gt;&gt;&gt; hsi_image = HSI(image=torch.rand(10, 10, 10), wavelengths=np.arange(10))\n            &gt;&gt;&gt; # Move the image to cpu\n            &gt;&gt;&gt; hsi_image = hsi_image.to(\"cpu\")\n            &gt;&gt;&gt; hsi_image.device\n            device(type='cpu')\n            &gt;&gt;&gt; # Move the image to cuda\n            &gt;&gt;&gt; hsi_image = hsi_image.to(\"cuda\")\n            &gt;&gt;&gt; hsi_image.device\n            device(type='cuda', index=0)\n        \"\"\"\n        self.image = self.image.to(device)\n        self.binary_mask = self.binary_mask.to(device)\n        self.device = self.image.device\n        return self\n\n    def get_image(self, apply_mask: bool = True) -&gt; torch.Tensor:\n        \"\"\"Returns the hyperspectral image data with optional masking applied.\n\n        Args:\n            apply_mask (bool, optional): Whether to apply the binary mask to the image.\n                Defaults to True.\n        Returns:\n            torch.Tensor: The hyperspectral image data.\n\n        Notes:\n            - If apply_mask is True, the binary mask will be applied to the image based on the `binary_mask` attribute.\n\n        Examples:\n            &gt;&gt;&gt; hsi_image = HSI(image=torch.rand(10, 100, 100), wavelengths=np.linspace(400, 1000, 10))\n            &gt;&gt;&gt; image = hsi_image.get_image()\n            &gt;&gt;&gt; image.shape\n            torch.Size([10, 100, 100])\n            &gt;&gt;&gt; image = hsi_image.get_image(apply_mask=False)\n            &gt;&gt;&gt; image.shape\n            torch.Size([10, 100, 100])\n        \"\"\"\n        if apply_mask and self.binary_mask is not None:\n            return self.image * self.binary_mask\n        return self.image\n\n    def get_rgb_image(\n        self,\n        apply_mask: bool = True,\n        apply_min_cutoff: bool = False,\n        output_channel_axis: int | None = None,\n        normalize: bool = True,\n    ) -&gt; torch.Tensor:\n        \"\"\"Extracts an RGB representation from the hyperspectral image data.\n\n        This method creates a 3-channel RGB image by selecting appropriate bands\n        corresponding to red, green, and blue wavelengths from the hyperspectral data.\n\n        Args:\n            apply_mask (bool, optional): Whether to apply the binary mask to the image.\n                Defaults to True.\n            apply_min_cutoff (bool, optional): Whether to apply a minimum intensity\n                cutoff to the image. Defaults to False.\n            output_channel_axis (int | None, optional): The axis where the RGB channels\n                should be placed in the output tensor. If None, uses the current spectral\n                axis of the hyperspectral data. Defaults to None.\n            normalize (bool, optional): Whether to normalize the band values to the [0, 1] range.\n                Defaults to True.\n\n        Returns:\n            torch.Tensor: The RGB representation of the hyperspectral image.\n                Shape will be either (H, W, 3), (3, H, W), or (H, 3, W) depending on\n                the specified output_channel_axis, where H is height and W is width.\n\n        Notes:\n            - The RGB bands are extracted using predefined wavelength ranges for R, G, and B.\n            - Each band is normalized independently before combining into the RGB image.\n            - If apply_mask is True, masked areas will be set to zero in the output.\n            - If apply_min_cutoff is True, a minimum intensity threshold is applied to each band.\n\n        Examples:\n            &gt;&gt;&gt; hsi_image = HSI(image=torch.rand(10, 100, 100), wavelengths=np.linspace(400, 1000, 10))\n            &gt;&gt;&gt; rgb_image = hsi_image.get_rgb_image()\n            &gt;&gt;&gt; rgb_image.shape\n            torch.Size([100, 100, 3])\n\n            &gt;&gt;&gt; rgb_image = hsi_image.get_rgb_image(output_channel_axis=0)\n            &gt;&gt;&gt; rgb_image.shape\n            torch.Size([3, 100, 100])\n\n            &gt;&gt;&gt; rgb_image = hsi_image.get_rgb_image(apply_mask=False, apply_min_cutoff=True)\n            &gt;&gt;&gt; rgb_image.shape\n            torch.Size([100, 100, 3])\n        \"\"\"\n        if output_channel_axis is None:\n            output_channel_axis = self.spectral_axis\n\n        rgb_img = torch.stack(\n            [\n                self.extract_band_by_name(\n                    band, apply_mask=apply_mask, apply_min_cutoff=apply_min_cutoff, normalize=normalize\n                )\n                for band in [\"R\", \"G\", \"B\"]\n            ],\n            dim=self.spectral_axis,\n        )\n\n        return (\n            rgb_img\n            if output_channel_axis == self.spectral_axis\n            else torch.moveaxis(rgb_img, self.spectral_axis, output_channel_axis)\n        )\n\n    def _extract_central_slice_from_band(\n        self,\n        band_wavelengths: torch.Tensor,\n        apply_mask: bool = True,\n        apply_min_cutoff: bool = False,\n        normalize: bool = True,\n    ) -&gt; torch.Tensor:\n        \"\"\"Extracts and processes the central wavelength band from a given range in the hyperspectral image.\n\n        This method selects the central band from a specified range of wavelengths,\n        applies optional processing steps (masking, normalization, and minimum cutoff),\n        and returns the resulting 2D image slice.\n\n        Args:\n            band_wavelengths (torch.Tensor): The selected wavelengths that define the whole band\n                from which the central slice will be extracted.\n                All of the passed wavelengths must be present in the image.\n            apply_mask (bool, optional): Whether to apply the binary mask to the extracted band.\n                Defaults to True.\n            apply_min_cutoff (bool, optional): Whether to apply a minimum intensity cutoff.\n                If True, sets the minimum non-zero value to zero after normalization.\n                Defaults to False.\n            normalize (bool, optional): Whether to normalize the band values to [0, 1] range.\n                Defaults to True.\n\n        Returns:\n            torch.Tensor: A 2D tensor representing the processed central wavelength band.\n                Shape will be (H, W), where H is height and W is width of the image.\n\n        Notes:\n            - The central wavelength is determined as the middle index of the provided wavelengths list.\n            - If normalization is applied, it's done before masking and cutoff operations.\n            - The binary mask, if applied, is expected to have the same spatial dimensions as the image.\n\n        Examples:\n            &gt;&gt;&gt; hsi_image = HSI(image=torch.rand(13, 100, 100), wavelengths=np.linspace(400, 1000, 13))\n            &gt;&gt;&gt; band_wavelengths = torch.tensor([500, 600, 650, 700])\n            &gt;&gt;&gt; central_slice = hsi_image._extract_central_slice_from_band(band_wavelengths)\n            &gt;&gt;&gt; central_slice.shape\n            torch.Size([100, 100])\n\n            &gt;&gt;&gt; # Extract a slice without normalization or masking\n            &gt;&gt;&gt; raw_band = hsi_image._extract_central_slice_from_band(band_wavelengths, apply_mask=False, normalize=False)\n        \"\"\"\n        # check if all wavelengths from the `band_wavelengths` are present in the image\n        if not all(wave in self.wavelengths for wave in band_wavelengths):\n            raise ValueError(\"All of the passed wavelengths must be present in the image\")\n\n        # sort the `band_wavelengths` to ensure the central band is selected\n        band_wavelengths = torch.sort(band_wavelengths).values\n\n        start_index = np.where(self.wavelengths == band_wavelengths[0])[0][0]\n        relative_center_band_index = len(band_wavelengths) // 2\n        central_band_index = start_index + relative_center_band_index\n\n        # Ensure the spectral dimension is the last\n        image = self.image if self.spectral_axis == 2 else torch.moveaxis(self.image, self.spectral_axis, 2)\n\n        slice = image[..., central_band_index]\n\n        if normalize:\n            if apply_min_cutoff:\n                slice_min = slice[slice != 0].min()\n            else:\n                slice_min = slice.min()\n\n            slice_max = slice.max()\n            if slice_max &gt; slice_min:  # Avoid division by zero\n                slice = (slice - slice_min) / (slice_max - slice_min)\n\n            if apply_min_cutoff:\n                slice[slice == slice.min()] = 0  # Set minimum values to zero\n\n        if apply_mask:\n            mask = (\n                self.binary_mask if self.spectral_axis == 2 else torch.moveaxis(self.binary_mask, self.spectral_axis, 2)\n            )\n            slice = slice * mask[..., central_band_index]\n\n        return slice\n\n    def extract_band_by_name(\n        self,\n        band_name: str,\n        selection_method: str = \"center\",\n        apply_mask: bool = True,\n        apply_min_cutoff: bool = False,\n        normalize: bool = True,\n    ) -&gt; torch.Tensor:\n        \"\"\"Extracts a single spectral band from the hyperspectral image based on a standardized band name.\n\n        This method uses the spyndex library to map standardized band names to wavelength ranges,\n        then extracts the corresponding band from the hyperspectral data.\n\n        Args:\n            band_name (str): The standardized name of the band to extract (e.g., \"Red\", \"NIR\", \"SWIR1\").\n            selection_method (str, optional): The method to use for selecting the band within the wavelength range.\n                Currently, only \"center\" is supported, which selects the central wavelength.\n                Defaults to \"center\".\n            apply_mask (bool, optional): Whether to apply the binary mask to the extracted band.\n                Defaults to True.\n            apply_min_cutoff (bool, optional): Whether to apply a minimum intensity cutoff after normalization.\n                If True, sets the minimum non-zero value to zero. Defaults to False.\n            normalize (bool, optional): Whether to normalize the band values to the [0, 1] range.\n                Defaults to True.\n\n        Returns:\n            torch.Tensor: A 2D tensor representing the extracted and processed spectral band.\n                Shape will be (H, W), where H is height and W is width of the image.\n\n        Raises:\n            BandSelectionError: If the specified band name is not found in the spyndex library.\n            NotImplementedError: If a selection method other than \"center\" is specified.\n\n        Notes:\n            - The spyndex library is used to map band names to wavelength ranges.\n            - Currently, only the \"center\" selection method is implemented, which chooses\n            the central wavelength within the specified range.\n            - Processing steps are applied in the order: normalization, cutoff, masking.\n\n        Examples:\n            &gt;&gt;&gt; hsi_image = HSI(image=torch.rand(200, 100, 100), wavelengths=np.linspace(400, 2500, 200))\n            &gt;&gt;&gt; red_band = hsi_image.extract_band_by_name(\"Red\")\n            &gt;&gt;&gt; red_band.shape\n            torch.Size([100, 100])\n\n            &gt;&gt;&gt; # Extract NIR band without normalization or masking\n            &gt;&gt;&gt; nir_band = hsi_image.extract_band_by_name(\"NIR\", apply_mask=False, normalize=False)\n        \"\"\"\n        band_info = spyndex.bands.get(band_name)\n        if band_info is None:\n            raise BandSelectionError(f\"Band name '{band_name}' not found in the spyndex library\")\n\n        min_wave, max_wave = band_info.min_wavelength, band_info.max_wavelength\n        selected_wavelengths = self.wavelengths[(self.wavelengths &gt;= min_wave) &amp; (self.wavelengths &lt;= max_wave)]\n\n        if selection_method == \"center\":\n            return self._extract_central_slice_from_band(\n                selected_wavelengths, apply_mask=apply_mask, apply_min_cutoff=apply_min_cutoff, normalize=normalize\n            )\n        else:\n            raise NotImplementedError(\n                f\"Selection method '{selection_method}' is not supported. Only 'center' is currently available.\"\n            )\n\n    def change_orientation(self, target_orientation: tuple[str, str, str] | list[str] | str, inplace=False) -&gt; Self:\n        \"\"\"Changes the orientation of the hsi data to the target orientation.\n\n        Args:\n            target_orientation (tuple[str, str, str], list[str], str): The target orientation for the hsi data.\n                This should be a tuple of three one-letter strings in any order: \"C\", \"H\", \"W\".\n            inplace (bool, optional): Whether to modify the hsi data in place or return a new object.\n\n        Returns:\n            Self: The updated HSI object with the new orientation.\n\n        Raises:\n            ValueError: If the target orientation is not a valid tuple of three one-letter strings.\n        \"\"\"\n        target_orientation = validate_orientation(target_orientation)\n\n        if inplace:\n            hsi = self\n        else:\n            hsi = self.model_copy()\n\n        if target_orientation == self.orientation:\n            return hsi\n\n        permute_dims = [hsi.orientation.index(dim) for dim in target_orientation]\n\n        # permute the image\n        hsi.image = hsi.image.permute(permute_dims)\n\n        # permute the binary mask\n        if hsi.binary_mask is not None:\n            hsi.binary_mask = hsi.binary_mask.permute(permute_dims)\n\n        hsi.orientation = target_orientation\n\n        return hsi\n</code></pre>"},{"location":"reference/#src.meteors.hsi.HSI.spatial_binary_mask","title":"<code>spatial_binary_mask: torch.Tensor</code>  <code>property</code>","text":"<p>Returns a 2D spatial representation of the binary mask.</p> <p>This property extracts a single 2D slice from the 3D binary mask, assuming that the mask is identical across all spectral bands. It handles different data orientations by first ensuring the spectral dimension is the last dimension before extracting the 2D spatial mask.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A 2D tensor representing the spatial binary mask. The shape will be (H, W) where H is height and W is width of the image.</p> Note <ul> <li>This assumes that the binary mask is consistent across all spectral bands.</li> <li>The returned mask is always 2D, regardless of the original data orientation.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # If self.binary_mask has shape (100, 100, 5) with spectral_axis=2:\n&gt;&gt;&gt; hsi_image = HSI(binary_mask=torch.rand(100, 100, 5), orientation=(\"H\", \"W\", \"C\"))\n&gt;&gt;&gt; hsi_image.spatial_binary_mask.shape\ntorch.Size([100, 100])\n&gt;&gt;&gt; If self.binary_mask has shape (5, 100, 100) with spectral_axis=0:\n&gt;&gt;&gt; hsi_image = HSI(binary_mask=torch.rand(5, 100, 100), orientation=(\"C\", \"H\", \"W\"))\n&gt;&gt;&gt; hsi_image.spatial_binary_mask.shape\ntorch.Size([100, 100])\n</code></pre>"},{"location":"reference/#src.meteors.hsi.HSI.spectral_axis","title":"<code>spectral_axis: int</code>  <code>property</code>","text":"<p>Returns the index of the spectral (wavelength) axis based on the current data orientation.</p> <p>In hyperspectral imaging, the spectral axis represents the dimension along which different spectral bands or wavelengths are arranged. This property dynamically determines the index of this axis based on the current orientation of the data.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The index of the spectral axis in the current data structure. - 0 for 'CHW' or 'CWH' orientations (Channel/Wavelength first) - 2 for 'HWC' or 'WHC' orientations (Channel/Wavelength last) - 1 for 'HCW' or 'WCH' orientations (Channel/Wavelength in the middle)</p> Note <p>The orientation is typically represented as a string where: - 'C' represents the spectral/wavelength dimension - 'H' represents the height (rows) of the image - 'W' represents the width (columns) of the image</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hsi_image = HSI()\n&gt;&gt;&gt; hsi_image.orientation = \"CHW\"\n&gt;&gt;&gt; hsi_image.spectral_axis\n0\n&gt;&gt;&gt; hsi_image.orientation = \"HWC\"\n&gt;&gt;&gt; hsi_image.spectral_axis\n2\n</code></pre>"},{"location":"reference/#src.meteors.hsi.HSI.change_orientation","title":"<code>change_orientation(target_orientation, inplace=False)</code>","text":"<p>Changes the orientation of the hsi data to the target orientation.</p> <p>Parameters:</p> Name Type Description Default <code>target_orientation</code> <code>(tuple[str, str, str], list[str], str)</code> <p>The target orientation for the hsi data. This should be a tuple of three one-letter strings in any order: \"C\", \"H\", \"W\".</p> required <code>inplace</code> <code>bool</code> <p>Whether to modify the hsi data in place or return a new object.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>The updated HSI object with the new orientation.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the target orientation is not a valid tuple of three one-letter strings.</p> Source code in <code>src/meteors/hsi.py</code> <pre><code>def change_orientation(self, target_orientation: tuple[str, str, str] | list[str] | str, inplace=False) -&gt; Self:\n    \"\"\"Changes the orientation of the hsi data to the target orientation.\n\n    Args:\n        target_orientation (tuple[str, str, str], list[str], str): The target orientation for the hsi data.\n            This should be a tuple of three one-letter strings in any order: \"C\", \"H\", \"W\".\n        inplace (bool, optional): Whether to modify the hsi data in place or return a new object.\n\n    Returns:\n        Self: The updated HSI object with the new orientation.\n\n    Raises:\n        ValueError: If the target orientation is not a valid tuple of three one-letter strings.\n    \"\"\"\n    target_orientation = validate_orientation(target_orientation)\n\n    if inplace:\n        hsi = self\n    else:\n        hsi = self.model_copy()\n\n    if target_orientation == self.orientation:\n        return hsi\n\n    permute_dims = [hsi.orientation.index(dim) for dim in target_orientation]\n\n    # permute the image\n    hsi.image = hsi.image.permute(permute_dims)\n\n    # permute the binary mask\n    if hsi.binary_mask is not None:\n        hsi.binary_mask = hsi.binary_mask.permute(permute_dims)\n\n    hsi.orientation = target_orientation\n\n    return hsi\n</code></pre>"},{"location":"reference/#src.meteors.hsi.HSI.extract_band_by_name","title":"<code>extract_band_by_name(band_name, selection_method='center', apply_mask=True, apply_min_cutoff=False, normalize=True)</code>","text":"<p>Extracts a single spectral band from the hyperspectral image based on a standardized band name.</p> <p>This method uses the spyndex library to map standardized band names to wavelength ranges, then extracts the corresponding band from the hyperspectral data.</p> <p>Parameters:</p> Name Type Description Default <code>band_name</code> <code>str</code> <p>The standardized name of the band to extract (e.g., \"Red\", \"NIR\", \"SWIR1\").</p> required <code>selection_method</code> <code>str</code> <p>The method to use for selecting the band within the wavelength range. Currently, only \"center\" is supported, which selects the central wavelength. Defaults to \"center\".</p> <code>'center'</code> <code>apply_mask</code> <code>bool</code> <p>Whether to apply the binary mask to the extracted band. Defaults to True.</p> <code>True</code> <code>apply_min_cutoff</code> <code>bool</code> <p>Whether to apply a minimum intensity cutoff after normalization. If True, sets the minimum non-zero value to zero. Defaults to False.</p> <code>False</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize the band values to the [0, 1] range. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A 2D tensor representing the extracted and processed spectral band. Shape will be (H, W), where H is height and W is width of the image.</p> <p>Raises:</p> Type Description <code>BandSelectionError</code> <p>If the specified band name is not found in the spyndex library.</p> <code>NotImplementedError</code> <p>If a selection method other than \"center\" is specified.</p> Notes <ul> <li>The spyndex library is used to map band names to wavelength ranges.</li> <li>Currently, only the \"center\" selection method is implemented, which chooses the central wavelength within the specified range.</li> <li>Processing steps are applied in the order: normalization, cutoff, masking.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hsi_image = HSI(image=torch.rand(200, 100, 100), wavelengths=np.linspace(400, 2500, 200))\n&gt;&gt;&gt; red_band = hsi_image.extract_band_by_name(\"Red\")\n&gt;&gt;&gt; red_band.shape\ntorch.Size([100, 100])\n</code></pre> <pre><code>&gt;&gt;&gt; # Extract NIR band without normalization or masking\n&gt;&gt;&gt; nir_band = hsi_image.extract_band_by_name(\"NIR\", apply_mask=False, normalize=False)\n</code></pre> Source code in <code>src/meteors/hsi.py</code> <pre><code>def extract_band_by_name(\n    self,\n    band_name: str,\n    selection_method: str = \"center\",\n    apply_mask: bool = True,\n    apply_min_cutoff: bool = False,\n    normalize: bool = True,\n) -&gt; torch.Tensor:\n    \"\"\"Extracts a single spectral band from the hyperspectral image based on a standardized band name.\n\n    This method uses the spyndex library to map standardized band names to wavelength ranges,\n    then extracts the corresponding band from the hyperspectral data.\n\n    Args:\n        band_name (str): The standardized name of the band to extract (e.g., \"Red\", \"NIR\", \"SWIR1\").\n        selection_method (str, optional): The method to use for selecting the band within the wavelength range.\n            Currently, only \"center\" is supported, which selects the central wavelength.\n            Defaults to \"center\".\n        apply_mask (bool, optional): Whether to apply the binary mask to the extracted band.\n            Defaults to True.\n        apply_min_cutoff (bool, optional): Whether to apply a minimum intensity cutoff after normalization.\n            If True, sets the minimum non-zero value to zero. Defaults to False.\n        normalize (bool, optional): Whether to normalize the band values to the [0, 1] range.\n            Defaults to True.\n\n    Returns:\n        torch.Tensor: A 2D tensor representing the extracted and processed spectral band.\n            Shape will be (H, W), where H is height and W is width of the image.\n\n    Raises:\n        BandSelectionError: If the specified band name is not found in the spyndex library.\n        NotImplementedError: If a selection method other than \"center\" is specified.\n\n    Notes:\n        - The spyndex library is used to map band names to wavelength ranges.\n        - Currently, only the \"center\" selection method is implemented, which chooses\n        the central wavelength within the specified range.\n        - Processing steps are applied in the order: normalization, cutoff, masking.\n\n    Examples:\n        &gt;&gt;&gt; hsi_image = HSI(image=torch.rand(200, 100, 100), wavelengths=np.linspace(400, 2500, 200))\n        &gt;&gt;&gt; red_band = hsi_image.extract_band_by_name(\"Red\")\n        &gt;&gt;&gt; red_band.shape\n        torch.Size([100, 100])\n\n        &gt;&gt;&gt; # Extract NIR band without normalization or masking\n        &gt;&gt;&gt; nir_band = hsi_image.extract_band_by_name(\"NIR\", apply_mask=False, normalize=False)\n    \"\"\"\n    band_info = spyndex.bands.get(band_name)\n    if band_info is None:\n        raise BandSelectionError(f\"Band name '{band_name}' not found in the spyndex library\")\n\n    min_wave, max_wave = band_info.min_wavelength, band_info.max_wavelength\n    selected_wavelengths = self.wavelengths[(self.wavelengths &gt;= min_wave) &amp; (self.wavelengths &lt;= max_wave)]\n\n    if selection_method == \"center\":\n        return self._extract_central_slice_from_band(\n            selected_wavelengths, apply_mask=apply_mask, apply_min_cutoff=apply_min_cutoff, normalize=normalize\n        )\n    else:\n        raise NotImplementedError(\n            f\"Selection method '{selection_method}' is not supported. Only 'center' is currently available.\"\n        )\n</code></pre>"},{"location":"reference/#src.meteors.hsi.HSI.get_image","title":"<code>get_image(apply_mask=True)</code>","text":"<p>Returns the hyperspectral image data with optional masking applied.</p> <p>Parameters:</p> Name Type Description Default <code>apply_mask</code> <code>bool</code> <p>Whether to apply the binary mask to the image. Defaults to True.</p> <code>True</code> <p>Returns:     torch.Tensor: The hyperspectral image data.</p> Notes <ul> <li>If apply_mask is True, the binary mask will be applied to the image based on the <code>binary_mask</code> attribute.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hsi_image = HSI(image=torch.rand(10, 100, 100), wavelengths=np.linspace(400, 1000, 10))\n&gt;&gt;&gt; image = hsi_image.get_image()\n&gt;&gt;&gt; image.shape\ntorch.Size([10, 100, 100])\n&gt;&gt;&gt; image = hsi_image.get_image(apply_mask=False)\n&gt;&gt;&gt; image.shape\ntorch.Size([10, 100, 100])\n</code></pre> Source code in <code>src/meteors/hsi.py</code> <pre><code>def get_image(self, apply_mask: bool = True) -&gt; torch.Tensor:\n    \"\"\"Returns the hyperspectral image data with optional masking applied.\n\n    Args:\n        apply_mask (bool, optional): Whether to apply the binary mask to the image.\n            Defaults to True.\n    Returns:\n        torch.Tensor: The hyperspectral image data.\n\n    Notes:\n        - If apply_mask is True, the binary mask will be applied to the image based on the `binary_mask` attribute.\n\n    Examples:\n        &gt;&gt;&gt; hsi_image = HSI(image=torch.rand(10, 100, 100), wavelengths=np.linspace(400, 1000, 10))\n        &gt;&gt;&gt; image = hsi_image.get_image()\n        &gt;&gt;&gt; image.shape\n        torch.Size([10, 100, 100])\n        &gt;&gt;&gt; image = hsi_image.get_image(apply_mask=False)\n        &gt;&gt;&gt; image.shape\n        torch.Size([10, 100, 100])\n    \"\"\"\n    if apply_mask and self.binary_mask is not None:\n        return self.image * self.binary_mask\n    return self.image\n</code></pre>"},{"location":"reference/#src.meteors.hsi.HSI.get_rgb_image","title":"<code>get_rgb_image(apply_mask=True, apply_min_cutoff=False, output_channel_axis=None, normalize=True)</code>","text":"<p>Extracts an RGB representation from the hyperspectral image data.</p> <p>This method creates a 3-channel RGB image by selecting appropriate bands corresponding to red, green, and blue wavelengths from the hyperspectral data.</p> <p>Parameters:</p> Name Type Description Default <code>apply_mask</code> <code>bool</code> <p>Whether to apply the binary mask to the image. Defaults to True.</p> <code>True</code> <code>apply_min_cutoff</code> <code>bool</code> <p>Whether to apply a minimum intensity cutoff to the image. Defaults to False.</p> <code>False</code> <code>output_channel_axis</code> <code>int | None</code> <p>The axis where the RGB channels should be placed in the output tensor. If None, uses the current spectral axis of the hyperspectral data. Defaults to None.</p> <code>None</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize the band values to the [0, 1] range. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The RGB representation of the hyperspectral image. Shape will be either (H, W, 3), (3, H, W), or (H, 3, W) depending on the specified output_channel_axis, where H is height and W is width.</p> Notes <ul> <li>The RGB bands are extracted using predefined wavelength ranges for R, G, and B.</li> <li>Each band is normalized independently before combining into the RGB image.</li> <li>If apply_mask is True, masked areas will be set to zero in the output.</li> <li>If apply_min_cutoff is True, a minimum intensity threshold is applied to each band.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hsi_image = HSI(image=torch.rand(10, 100, 100), wavelengths=np.linspace(400, 1000, 10))\n&gt;&gt;&gt; rgb_image = hsi_image.get_rgb_image()\n&gt;&gt;&gt; rgb_image.shape\ntorch.Size([100, 100, 3])\n</code></pre> <pre><code>&gt;&gt;&gt; rgb_image = hsi_image.get_rgb_image(output_channel_axis=0)\n&gt;&gt;&gt; rgb_image.shape\ntorch.Size([3, 100, 100])\n</code></pre> <pre><code>&gt;&gt;&gt; rgb_image = hsi_image.get_rgb_image(apply_mask=False, apply_min_cutoff=True)\n&gt;&gt;&gt; rgb_image.shape\ntorch.Size([100, 100, 3])\n</code></pre> Source code in <code>src/meteors/hsi.py</code> <pre><code>def get_rgb_image(\n    self,\n    apply_mask: bool = True,\n    apply_min_cutoff: bool = False,\n    output_channel_axis: int | None = None,\n    normalize: bool = True,\n) -&gt; torch.Tensor:\n    \"\"\"Extracts an RGB representation from the hyperspectral image data.\n\n    This method creates a 3-channel RGB image by selecting appropriate bands\n    corresponding to red, green, and blue wavelengths from the hyperspectral data.\n\n    Args:\n        apply_mask (bool, optional): Whether to apply the binary mask to the image.\n            Defaults to True.\n        apply_min_cutoff (bool, optional): Whether to apply a minimum intensity\n            cutoff to the image. Defaults to False.\n        output_channel_axis (int | None, optional): The axis where the RGB channels\n            should be placed in the output tensor. If None, uses the current spectral\n            axis of the hyperspectral data. Defaults to None.\n        normalize (bool, optional): Whether to normalize the band values to the [0, 1] range.\n            Defaults to True.\n\n    Returns:\n        torch.Tensor: The RGB representation of the hyperspectral image.\n            Shape will be either (H, W, 3), (3, H, W), or (H, 3, W) depending on\n            the specified output_channel_axis, where H is height and W is width.\n\n    Notes:\n        - The RGB bands are extracted using predefined wavelength ranges for R, G, and B.\n        - Each band is normalized independently before combining into the RGB image.\n        - If apply_mask is True, masked areas will be set to zero in the output.\n        - If apply_min_cutoff is True, a minimum intensity threshold is applied to each band.\n\n    Examples:\n        &gt;&gt;&gt; hsi_image = HSI(image=torch.rand(10, 100, 100), wavelengths=np.linspace(400, 1000, 10))\n        &gt;&gt;&gt; rgb_image = hsi_image.get_rgb_image()\n        &gt;&gt;&gt; rgb_image.shape\n        torch.Size([100, 100, 3])\n\n        &gt;&gt;&gt; rgb_image = hsi_image.get_rgb_image(output_channel_axis=0)\n        &gt;&gt;&gt; rgb_image.shape\n        torch.Size([3, 100, 100])\n\n        &gt;&gt;&gt; rgb_image = hsi_image.get_rgb_image(apply_mask=False, apply_min_cutoff=True)\n        &gt;&gt;&gt; rgb_image.shape\n        torch.Size([100, 100, 3])\n    \"\"\"\n    if output_channel_axis is None:\n        output_channel_axis = self.spectral_axis\n\n    rgb_img = torch.stack(\n        [\n            self.extract_band_by_name(\n                band, apply_mask=apply_mask, apply_min_cutoff=apply_min_cutoff, normalize=normalize\n            )\n            for band in [\"R\", \"G\", \"B\"]\n        ],\n        dim=self.spectral_axis,\n    )\n\n    return (\n        rgb_img\n        if output_channel_axis == self.spectral_axis\n        else torch.moveaxis(rgb_img, self.spectral_axis, output_channel_axis)\n    )\n</code></pre>"},{"location":"reference/#src.meteors.hsi.HSI.to","title":"<code>to(device)</code>","text":"<p>Moves the image and binary mask (if available) to the specified device.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str or device</code> <p>The device to move the image and binary mask to.</p> required <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>The updated HSI object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Create an HSI object\n&gt;&gt;&gt; hsi_image = HSI(image=torch.rand(10, 10, 10), wavelengths=np.arange(10))\n&gt;&gt;&gt; # Move the image to cpu\n&gt;&gt;&gt; hsi_image = hsi_image.to(\"cpu\")\n&gt;&gt;&gt; hsi_image.device\ndevice(type='cpu')\n&gt;&gt;&gt; # Move the image to cuda\n&gt;&gt;&gt; hsi_image = hsi_image.to(\"cuda\")\n&gt;&gt;&gt; hsi_image.device\ndevice(type='cuda', index=0)\n</code></pre> Source code in <code>src/meteors/hsi.py</code> <pre><code>def to(self, device: str | torch.device) -&gt; Self:\n    \"\"\"Moves the image and binary mask (if available) to the specified device.\n\n    Args:\n        device (str or torch.device): The device to move the image and binary mask to.\n\n    Returns:\n        Self: The updated HSI object.\n\n    Examples:\n        &gt;&gt;&gt; # Create an HSI object\n        &gt;&gt;&gt; hsi_image = HSI(image=torch.rand(10, 10, 10), wavelengths=np.arange(10))\n        &gt;&gt;&gt; # Move the image to cpu\n        &gt;&gt;&gt; hsi_image = hsi_image.to(\"cpu\")\n        &gt;&gt;&gt; hsi_image.device\n        device(type='cpu')\n        &gt;&gt;&gt; # Move the image to cuda\n        &gt;&gt;&gt; hsi_image = hsi_image.to(\"cuda\")\n        &gt;&gt;&gt; hsi_image.device\n        device(type='cuda', index=0)\n    \"\"\"\n    self.image = self.image.to(device)\n    self.binary_mask = self.binary_mask.to(device)\n    self.device = self.image.device\n    return self\n</code></pre>"},{"location":"reference/#visualizations","title":"Visualizations","text":"<p>Visualizes a Hyperspectral image object on the given axes. It uses either the object from HSI class or a field from the HSIAttributes class.</p> <p>Parameters:</p> Name Type Description Default <code>hsi_or_attributes</code> <code>HSI | HSIAttributes</code> <p>The hyperspectral image, or the attributes to be visualized.</p> required <code>ax</code> <code>Axes | None</code> <p>The axes on which the image will be plotted. If None, the current axes will be used.</p> <code>None</code> <code>use_mask</code> <code>bool</code> <p>Whether to use the image mask if provided for the visualization.</p> <code>True</code> <p>Returns:</p> Type Description <code>Axes</code> <p>matplotlib.figure.Figure | None: If use_pyplot is False, returns the figure and axes objects. If use_pyplot is True, returns None.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If hsi_or_attributes is not an instance of HSI or HSIAttributes.</p> Source code in <code>src/meteors/visualize/hsi_visualize.py</code> <pre><code>def visualize_hsi(hsi_or_attributes: HSI | HSIAttributes, ax: Axes | None = None, use_mask: bool = True) -&gt; Axes:\n    \"\"\"Visualizes a Hyperspectral image object on the given axes. It uses either the object from HSI class or a field\n    from the HSIAttributes class.\n\n    Parameters:\n        hsi_or_attributes (HSI | HSIAttributes): The hyperspectral image, or the attributes to be visualized.\n        ax (matplotlib.axes.Axes | None): The axes on which the image will be plotted.\n            If None, the current axes will be used.\n        use_mask (bool): Whether to use the image mask if provided for the visualization.\n\n    Returns:\n        matplotlib.figure.Figure | None:\n            If use_pyplot is False, returns the figure and axes objects.\n            If use_pyplot is True, returns None.\n\n    Raises:\n        TypeError: If hsi_or_attributes is not an instance of HSI or HSIAttributes.\n    \"\"\"\n    if isinstance(hsi_or_attributes, HSIAttributes):\n        hsi = hsi_or_attributes.hsi\n    else:\n        hsi = hsi_or_attributes\n\n    if not isinstance(hsi, HSI):\n        raise TypeError(\"hsi_or_attributes must be an instance of HSI or HSIAttributes.\")\n\n    hsi = hsi.change_orientation(\"HWC\", inplace=False)\n\n    rgb = hsi.get_rgb_image(output_channel_axis=2, apply_mask=use_mask, normalize=True).cpu().numpy()\n    ax = ax or plt.gca()\n    ax.imshow(rgb)\n\n    return ax\n</code></pre>"},{"location":"reference/#src.meteors.visualize.attr_visualize.visualize_attributes","title":"<code>visualize_attributes(image_attributes, ax=None, use_pyplot=False)</code>","text":"<p>Visualizes the attributes of an image on the given axes.</p> <p>Parameters:</p> Name Type Description Default <code>image_attributes</code> <code>HSIAttributes</code> <p>The image attributes to be visualized.</p> required <code>ax</code> <code>Axes | None</code> <p>The axes to visualize the image on. If None, creates a new figure and axes.</p> <code>None</code> <code>use_pyplot</code> <code>bool</code> <p>If True, uses pyplot to display the image. If False, returns the figure and axes objects. if ax is not None, use_pyplot is ignored.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[Figure, Axes] | Axes | None</code> <p>matplotlib.figure.Figure | matplotlib.axes.Axes | None: The figure and axes objects. If use_pyplot is False and ax is None, returns the figure and axes objects. If use_pyplot is True and ax is None, returns None, and displays the image using pyplot. if ax is not None, returns the axes object. If all the attributions are zero, returns None.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the axes have less than 2 rows and 2 columns</p> <code>ValueError</code> <p>If the axes object is not a list of axes objects</p> Source code in <code>src/meteors/visualize/attr_visualize.py</code> <pre><code>def visualize_attributes(\n    image_attributes: HSIAttributes, ax: Axes | None = None, use_pyplot: bool = False\n) -&gt; tuple[Figure, Axes] | Axes | None:\n    \"\"\"Visualizes the attributes of an image on the given axes.\n\n    Parameters:\n        image_attributes (HSIAttributes): The image attributes to be visualized.\n        ax (Axes | None): The axes to visualize the image on. If None, creates a new figure and axes.\n        use_pyplot (bool): If True, uses pyplot to display the image. If False, returns the figure and axes objects.\n            if ax is not None, use_pyplot is ignored.\n\n    Returns:\n        matplotlib.figure.Figure | matplotlib.axes.Axes | None: The figure and axes objects.\n            If use_pyplot is False and ax is None, returns the figure and axes objects.\n            If use_pyplot is True and ax is None, returns None, and displays the image using pyplot.\n            if ax is not None, returns the axes object.\n            If all the attributions are zero, returns None.\n\n    Raises:\n        ValueError: If the axes have less than 2 rows and 2 columns\n        ValueError: If the axes object is not a list of axes objects\n    \"\"\"\n    if image_attributes.hsi.orientation != (\"H\", \"W\", \"C\"):\n        logger.info(\n            f\"The orientation of the image is not (H, W, C): {image_attributes.hsi.orientation}. \"\n            f\"Changing it to (H, W, C) for visualization.\"\n        )\n        rotated_attributes_dataclass = image_attributes.change_orientation(\"HWC\", inplace=False)\n    else:\n        rotated_attributes_dataclass = image_attributes\n\n    rotated_attributes = rotated_attributes_dataclass.attributes.detach().cpu().numpy()\n    if np.all(rotated_attributes == 0):\n        warnings.warn(\"All the attributions are zero. There is nothing to visualize.\")\n        return None\n\n    used_ax = True\n    if ax is None:\n        used_ax = False\n        fig, ax = plt.subplots(2, 2, figsize=(9, 7))\n\n    if not hasattr(ax, \"shape\"):\n        raise ValueError(\"Provided ax parameter is only one axes object, but it should be a list of axes objects\")\n    elif len(ax.shape) != 2 or ax.shape[0] &lt; 2 or ax.shape[1] &lt; 2:\n        raise ValueError(\"The axes should have at least 2 rows and 2 columns.\")\n    else:\n        fig = ax[0, 0].get_figure()\n\n    ax[0, 0].set_title(\"Attribution Heatmap\")\n    ax[0, 0].grid(False)\n    ax[0, 0].axis(\"off\")\n\n    fig.suptitle(f\"HSI Attributes of: {rotated_attributes_dataclass.attribution_method}\")\n\n    _ = viz.visualize_image_attr(\n        rotated_attributes,\n        method=\"heat_map\",\n        sign=\"all\",\n        plt_fig_axis=(fig, ax[0, 0]),\n        show_colorbar=True,\n        use_pyplot=False,\n    )\n\n    ax[0, 1].set_title(\"Attribution Module Values\")\n    ax[0, 1].grid(False)\n    ax[0, 1].axis(\"off\")\n\n    # Attributions module values\n    _ = viz.visualize_image_attr(\n        rotated_attributes,\n        method=\"heat_map\",\n        sign=\"absolute_value\",\n        plt_fig_axis=(fig, ax[0, 1]),\n        show_colorbar=True,\n        use_pyplot=False,\n    )\n\n    attr_all = rotated_attributes.sum(axis=(0, 1))\n    ax[1, 0].scatter(rotated_attributes_dataclass.hsi.wavelengths, attr_all, c=\"r\")\n    ax[1, 0].set_title(\"Spectral Attribution\")\n    ax[1, 0].set_xlabel(\"Wavelength\")\n    ax[1, 0].set_ylabel(\"Attribution\")\n    ax[1, 0].grid(True)\n\n    attr_abs = np.abs(rotated_attributes).sum(axis=(0, 1))\n    ax[1, 1].scatter(rotated_attributes_dataclass.hsi.wavelengths, attr_abs, c=\"b\")\n    ax[1, 1].set_title(\"Spectral Attribution Absolute Values\")\n    ax[1, 1].set_xlabel(\"Wavelength\")\n    ax[1, 1].set_ylabel(\"Attribution Absolute Value\")\n    ax[1, 1].grid(True)\n\n    plt.tight_layout()\n\n    if used_ax:\n        return ax\n\n    if use_pyplot:\n        plt.show()  # pragma: no cover\n        return None  # pragma: no cover\n\n    return fig, ax\n</code></pre>"},{"location":"reference/#src.meteors.visualize.attr_visualize.visualize_spatial_aggregated_attributes","title":"<code>visualize_spatial_aggregated_attributes(attributes, aggregated_mask, ax=None, use_pyplot=False, aggregate_func=torch.mean)</code>","text":"<p>Visualizes the spatial attributes of an hsi object aggregated by a custom mask.</p> <p>Parameters:</p> Name Type Description Default <code>attributes</code> <code>HSIAttributes</code> <p>The spatial attributes of the hsi object to visualize.</p> required <code>aggregated_mask</code> <code>Tensor | ndarray</code> <p>The mask used to aggregate the spatial attributes.</p> required <code>ax</code> <code>Axes | None</code> <p>The axes object to plot the visualization on. If None, a new axes will be created.</p> <code>None</code> <code>use_pyplot</code> <code>bool</code> <p>If True, displays the visualization using pyplot. If ax is not None, use_pyplot is ignored. If False, returns the figure and axes objects. Defaults to False.</p> <code>False</code> <code>aggregate_func</code> <code>Callable[[Tensor], Tensor]</code> <p>The aggregation function to be applied. The function should take a tensor as input and return a tensor as output. We recommend using torch functions. Defaults to torch.mean.</p> <code>mean</code> <p>Raises:</p> Type Description <code>ShapeMismatchError</code> <p>If the shape of the aggregated mask does not match the shape of the spatial attributes.</p> <p>Returns:</p> Type Description <code>tuple[Figure, Axes] | Axes | None</code> <p>tuple[Figure, Axes] | Axes | None: If ax is not None, returns the axes object. If use_pyplot is True, returns None. If use_pyplot is False, returns the figure and axes objects.</p> Source code in <code>src/meteors/visualize/attr_visualize.py</code> <pre><code>def visualize_spatial_aggregated_attributes(\n    attributes: HSIAttributes,\n    aggregated_mask: torch.Tensor | np.ndarray,\n    ax: Axes | None = None,\n    use_pyplot: bool = False,\n    aggregate_func: Callable[[torch.Tensor], torch.Tensor] = torch.mean,\n) -&gt; tuple[Figure, Axes] | Axes | None:\n    \"\"\"Visualizes the spatial attributes of an hsi object aggregated by a custom mask.\n\n    Args:\n        attributes (HSIAttributes): The spatial attributes of the hsi object to visualize.\n        aggregated_mask (torch.Tensor | np.ndarray): The mask used to aggregate the spatial attributes.\n        ax (Axes | None, optional): The axes object to plot the visualization on. If None, a new axes will be created.\n        use_pyplot (bool, optional): If True, displays the visualization using pyplot.\n            If ax is not None, use_pyplot is ignored.\n            If False, returns the figure and axes objects. Defaults to False.\n        aggregate_func (Callable[[torch.Tensor], torch.Tensor], optional): The aggregation function to be applied.\n            The function should take a tensor as input and return a tensor as output.\n            We recommend using torch functions. Defaults to torch.mean.\n\n    Raises:\n        ShapeMismatchError: If the shape of the aggregated mask does not match the shape of the spatial attributes.\n\n    Returns:\n        tuple[Figure, Axes] | Axes | None: If ax is not None, returns the axes object.\n            If use_pyplot is True, returns None. If use_pyplot is False, returns the figure and axes objects.\n    \"\"\"\n    if isinstance(aggregated_mask, np.ndarray):\n        aggregated_mask = torch.from_numpy(aggregated_mask)\n\n    if aggregated_mask.shape != attributes.hsi.image.shape:\n        aggregated_mask = aggregated_mask.expand_as(attributes.attributes)\n\n    new_attrs = aggregate_by_mask(attributes.attributes, aggregated_mask, aggregate_func)\n\n    new_spatial_attributes = HSISpatialAttributes(\n        hsi=attributes.hsi,\n        attributes=new_attrs,\n        mask=aggregated_mask,\n        score=attributes.score,\n    )\n\n    out = visualize_spatial_attributes(new_spatial_attributes, ax=ax, use_pyplot=False)\n    if ax is not None:\n        return out\n\n    fig, ax = out  # type: ignore\n    fig.suptitle(\"Spatial Attributes Visualization Aggregated\")\n\n    if use_pyplot:\n        plt.show()  # pragma: no cover\n        return None  # pragma: no cover\n\n    return fig, ax\n</code></pre>"},{"location":"reference/#src.meteors.visualize.attr_visualize.visualize_spectral_aggregated_attributes","title":"<code>visualize_spectral_aggregated_attributes(attributes, band_names, band_mask, ax=None, use_pyplot=False, color_palette=None, show_not_included=True, aggregate_func=torch.mean)</code>","text":"<p>Visualizes the spectral attributes of an hsi object aggregated by a custom band mask.</p> <p>Parameters:</p> Name Type Description Default <code>attributes</code> <code>HSIAttributes | list[HSIAttributes]</code> <p>The spectral attributes of the hsi object to visualize.</p> required <code>band_names</code> <code>dict[str | tuple[str, ...], int]</code> <p>A dictionary mapping band names to their indices.</p> required <code>band_mask</code> <code>Tensor | ndarray</code> <p>The mask used to aggregate the spectral attributes.</p> required <code>ax</code> <code>Axes | None</code> <p>The axes object to plot the visualization on. If None, a new axes will be created.</p> <code>None</code> <code>use_pyplot</code> <code>bool</code> <p>If True, displays the visualization using pyplot. If ax is not None, use_pyplot is ignored. If False, returns the figure and axes objects. Defaults to False.</p> <code>False</code> <code>color_palette</code> <code>list[str] | None</code> <p>The color palette to use for visualizing different spectral bands. If None, a default color palette is used. Defaults to None.</p> <code>None</code> <code>show_not_included</code> <code>bool</code> <p>If True, includes the spectral bands that are not included in the visualization. If False, only includes the spectral bands that are included in the visualization. Defaults to True.</p> <code>True</code> <code>aggregate_func</code> <code>Callable[[Tensor], Tensor]</code> <p>The aggregation function to be applied. The function should take a tensor as input and return a tensor as output. We recommend using torch functions. Defaults to torch.mean.</p> <code>mean</code> <p>Raises:</p> Type Description <code>ShapeMismatchError</code> <p>If the shape of the band mask does not match the shape of the spectral attributes.</p> <p>Returns:</p> Type Description <code>tuple[Figure, Axes] | Axes | None</code> <p>tuple[Figure, Axes] | Axes | None: If ax is not None, returns the axes object. If use_pyplot is True, returns None. If use_pyplot is False, returns the figure and axes objects</p> Source code in <code>src/meteors/visualize/attr_visualize.py</code> <pre><code>def visualize_spectral_aggregated_attributes(\n    attributes: HSIAttributes | list[HSIAttributes],\n    band_names: dict[str | tuple[str, ...], int],\n    band_mask: torch.Tensor | np.ndarray,\n    ax: Axes | None = None,\n    use_pyplot: bool = False,\n    color_palette: list[str] | None = None,\n    show_not_included: bool = True,\n    aggregate_func: Callable[[torch.Tensor], torch.Tensor] = torch.mean,\n) -&gt; tuple[Figure, Axes] | Axes | None:\n    \"\"\"Visualizes the spectral attributes of an hsi object aggregated by a custom band mask.\n\n    Args:\n        attributes (HSIAttributes | list[HSIAttributes]): The spectral attributes of the hsi object to visualize.\n        band_names (dict[str | tuple[str, ...], int]): A dictionary mapping band names to their indices.\n        band_mask (torch.Tensor | np.ndarray): The mask used to aggregate the spectral attributes.\n        ax (Axes | None, optional): The axes object to plot the visualization on. If None, a new axes will be created.\n        use_pyplot (bool, optional): If True, displays the visualization using pyplot.\n            If ax is not None, use_pyplot is ignored. If False, returns the figure and axes objects. Defaults to False.\n        color_palette (list[str] | None, optional): The color palette to use for visualizing different spectral bands.\n            If None, a default color palette is used. Defaults to None.\n        show_not_included (bool, optional): If True, includes the spectral bands that are not included in the visualization.\n            If False, only includes the spectral bands that are included in the visualization. Defaults to True.\n        aggregate_func (Callable[[torch.Tensor], torch.Tensor], optional): The aggregation function to be applied.\n            The function should take a tensor as input and return a tensor as output.\n            We recommend using torch functions. Defaults to torch.mean.\n\n    Raises:\n        ShapeMismatchError: If the shape of the band mask does not match the shape of the spectral attributes.\n\n    Returns:\n        tuple[Figure, Axes] | Axes | None: If ax is not None, returns the axes object.\n            If use_pyplot is True, returns None. If use_pyplot is False, returns the figure and axes objects\n    \"\"\"\n    attributes_example = attributes if isinstance(attributes, HSIAttributes) else attributes[0]\n    if isinstance(band_mask, np.ndarray):\n        band_mask = torch.from_numpy(band_mask)\n\n    if band_mask.shape != attributes_example.hsi.image.shape:\n        band_mask = expand_spectral_mask(attributes_example.hsi, band_mask, repeat_dimensions=True)\n\n    band_names = align_band_names_with_mask(band_names, band_mask)\n\n    new_attrs = aggregate_by_mask(attributes_example.attributes, band_mask, aggregate_func)\n\n    new_spectral_attributes: HSISpectralAttributes | list[HSISpectralAttributes]\n    if isinstance(attributes, HSIAttributes):\n        new_spectral_attributes = HSISpectralAttributes(\n            hsi=attributes.hsi,\n            attributes=new_attrs,\n            mask=band_mask,\n            band_names=band_names,\n            score=attributes.score,\n        )\n    else:\n        new_spectral_attributes = [\n            HSISpectralAttributes(\n                hsi=attr.hsi,\n                attributes=new_attrs,\n                mask=band_mask,\n                band_names=band_names,\n                score=attr.score,\n            )\n            for attr in attributes\n        ]\n\n    out = visualize_spectral_attributes(\n        new_spectral_attributes,\n        ax=ax,\n        use_pyplot=False,\n        color_palette=color_palette,\n        show_not_included=show_not_included,\n    )  # type: ignore\n    if ax is not None:\n        return out\n\n    if use_pyplot:\n        plt.show()  # pragma: no cover\n        return None  # pragma: no cover\n\n    return out\n</code></pre>"},{"location":"reference/#src.meteors.visualize.attr_visualize.visualize_aggregated_attributes","title":"<code>visualize_aggregated_attributes(attributes, mask, band_names=None, ax=None, use_pyplot=False, color_palette=None, show_not_included=True, aggregate_func=torch.mean)</code>","text":"<p>Visualizes the aggregated attributes of an hsi object.</p> <p>Parameters:</p> Name Type Description Default <code>attributes</code> <code>HSIAttributes | list[HSIAttributes]</code> <p>The attributes of the hsi object to visualize.</p> required <code>mask</code> <code>Tensor | ndarray</code> <p>The mask used to aggregate the attributes.</p> required <code>band_names</code> <code>dict[str | tuple[str, ...], int] | None</code> <p>A dictionary mapping band names to their indices. If None, the visualization will be spatially aggregated. Defaults to None.</p> <code>None</code> <code>ax</code> <code>Axes | None</code> <p>The axes object to plot the visualization on. If None, a new axes will be created.</p> <code>None</code> <code>use_pyplot</code> <code>bool</code> <p>If True, displays the visualization using pyplot. If ax is not None, use_pyplot is ignored. If False, returns the figure and axes objects. Defaults to False.</p> <code>False</code> <code>color_palette</code> <code>list[str] | None</code> <p>The color palette to use for visualizing different spectral bands. If None, a default color palette is used. Defaults to None.</p> <code>None</code> <code>show_not_included</code> <code>bool</code> <p>If True, includes the spectral bands that are not included in the visualization. If False, only includes the spectral bands that are included in the visualization. Defaults to True.</p> <code>True</code> <code>aggregate_func</code> <code>Callable[[Tensor], Tensor]</code> <p>The aggregation function to be applied. The function should take a tensor as input and return a tensor as output. We recommend using torch functions. Defaults to torch.mean.</p> <code>mean</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the shape of the mask does not match the shape of the attributes.</p> <code>AssertionError</code> <p>If band_names is None and attributes is a list of HSIAttributes objects.</p> <p>Returns:</p> Type Description <code>tuple[Figure, Axes] | Axes | None</code> <p>tuple[Figure, Axes] | Axes | None: If ax is not None, returns the axes object. If use_pyplot is True, returns None. If use_pyplot is False, returns the figure and axes objects.</p> Source code in <code>src/meteors/visualize/attr_visualize.py</code> <pre><code>def visualize_aggregated_attributes(\n    attributes: HSIAttributes | list[HSIAttributes],\n    mask: torch.Tensor | np.ndarray,\n    band_names: dict[str | tuple[str, ...], int] | None = None,\n    ax: Axes | None = None,\n    use_pyplot: bool = False,\n    color_palette: list[str] | None = None,\n    show_not_included: bool = True,\n    aggregate_func: Callable[[torch.Tensor], torch.Tensor] = torch.mean,\n) -&gt; tuple[Figure, Axes] | Axes | None:\n    \"\"\"Visualizes the aggregated attributes of an hsi object.\n\n    Args:\n        attributes (HSIAttributes | list[HSIAttributes]): The attributes of the hsi object to visualize.\n        mask (torch.Tensor | np.ndarray): The mask used to aggregate the attributes.\n        band_names (dict[str | tuple[str, ...], int] | None, optional): A dictionary mapping band names to their indices.\n            If None, the visualization will be spatially aggregated. Defaults to None.\n        ax (Axes | None, optional): The axes object to plot the visualization on. If None, a new axes will be created.\n        use_pyplot (bool, optional): If True, displays the visualization using pyplot.\n            If ax is not None, use_pyplot is ignored. If False, returns the figure and axes objects. Defaults to False.\n        color_palette (list[str] | None, optional): The color palette to use for visualizing different spectral bands.\n            If None, a default color palette is used. Defaults to None.\n        show_not_included (bool, optional): If True, includes the spectral bands that are not included in the visualization.\n            If False, only includes the spectral bands that are included in the visualization. Defaults to True.\n        aggregate_func (Callable[[torch.Tensor], torch.Tensor], optional): The aggregation function to be applied.\n            The function should take a tensor as input and return a tensor as output.\n            We recommend using torch functions. Defaults to torch.mean.\n\n    Raises:\n        ValueError: If the shape of the mask does not match the shape of the attributes.\n        AssertionError: If band_names is None and attributes is a list of HSIAttributes objects.\n\n    Returns:\n        tuple[Figure, Axes] | Axes | None: If ax is not None, returns the axes object.\n            If use_pyplot is True, returns None. If use_pyplot is False, returns the figure and axes objects.\n    \"\"\"\n    agg = False if isinstance(attributes, HSIAttributes) else True\n    if band_names is None:\n        logger.info(\"Band names not provided. Using Spatial Analysis.\")\n        assert not agg, \"In Spatial Analysis, attributes must be a single HSIAttributes object.\"\n        return visualize_spatial_aggregated_attributes(attributes, mask, ax, use_pyplot, aggregate_func)  # type: ignore\n    else:\n        logger.info(\"Band names provided. Using Spectral Analysis.\")\n        return visualize_spectral_aggregated_attributes(\n            attributes, band_names, mask, ax, use_pyplot, color_palette, show_not_included, aggregate_func\n        )\n</code></pre>"},{"location":"reference/#src.meteors.visualize.attr_visualize.visualize_spectral_attributes_by_waveband","title":"<code>visualize_spectral_attributes_by_waveband(spectral_attributes, ax, color_palette=None, show_not_included=True, show_legend=True)</code>","text":"<p>Visualizes spectral attributes by waveband.</p> <p>Parameters:</p> Name Type Description Default <code>spectral_attributes</code> <code>HSISpectralAttributes | list[HSISpectralAttributes]</code> <p>The spectral attributes to visualize.</p> required <code>ax</code> <code>Axes | None</code> <p>The matplotlib axes to plot the visualization on. If None, a new axes will be created.</p> required <code>color_palette</code> <code>list[str] | None</code> <p>The color palette to use for plotting. If None, a default color palette will be used.</p> <code>None</code> <code>show_not_included</code> <code>bool</code> <p>Whether to show the \"not_included\" band in the visualization. Default is True.</p> <code>True</code> <code>show_legend</code> <code>bool</code> <p>Whether to show the legend in the visualization.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Axes</code> <code>Axes</code> <p>The matplotlib axes object containing the visualization.</p> <p>Raises:     TypeError: If the spectral attributes are not an HSISpectralAttributes object or a list of HSISpectralAttributes objects.</p> Source code in <code>src/meteors/visualize/attr_visualize.py</code> <pre><code>def visualize_spectral_attributes_by_waveband(\n    spectral_attributes: HSISpectralAttributes | list[HSISpectralAttributes],\n    ax: Axes | None,\n    color_palette: list[str] | None = None,\n    show_not_included: bool = True,\n    show_legend: bool = True,\n) -&gt; Axes:\n    \"\"\"Visualizes spectral attributes by waveband.\n\n    Args:\n        spectral_attributes (HSISpectralAttributes | list[HSISpectralAttributes]):\n            The spectral attributes to visualize.\n        ax (Axes | None): The matplotlib axes to plot the visualization on.\n            If None, a new axes will be created.\n        color_palette (list[str] | None): The color palette to use for plotting.\n            If None, a default color palette will be used.\n        show_not_included (bool): Whether to show the \"not_included\" band in the visualization.\n            Default is True.\n        show_legend (bool): Whether to show the legend in the visualization.\n\n    Returns:\n        Axes: The matplotlib axes object containing the visualization.\n    Raises:\n        TypeError: If the spectral attributes are not an HSISpectralAttributes object or a list of HSISpectralAttributes objects.\n    \"\"\"\n    if isinstance(spectral_attributes, HSISpectralAttributes):\n        spectral_attributes = [spectral_attributes]\n    if not (\n        isinstance(spectral_attributes, list)\n        and all(isinstance(attr, HSISpectralAttributes) for attr in spectral_attributes)\n    ):\n        raise TypeError(\n            \"spectral_attributes parameter must be an HSISpectralAttributes object or a list of HSISpectralAttributes objects.\"\n        )\n\n    aggregate_results = False if len(spectral_attributes) == 1 else True\n    band_names = dict(spectral_attributes[0].band_names)\n    wavelengths = spectral_attributes[0].hsi.wavelengths\n    validate_consistent_band_and_wavelengths(band_names, wavelengths, spectral_attributes)\n\n    ax = setup_visualization(ax, \"Attributions by Waveband\", \"Wavelength (nm)\", \"Correlation with Output\")\n\n    if not show_not_included and band_names.get(\"not_included\") is not None:\n        band_names.pop(\"not_included\")\n\n    band_names = _merge_band_names_segments(band_names)  # type: ignore\n\n    if color_palette is None:\n        color_palette = sns.color_palette(\"hsv\", len(band_names.keys()))\n\n    band_mask = spectral_attributes[0].band_mask.cpu()\n    attribution_map = torch.stack([attr.flattened_attributes.cpu() for attr in spectral_attributes])\n\n    for idx, (band_name, segment_id) in enumerate(band_names.items()):\n        current_wavelengths = wavelengths[band_mask == segment_id]\n        current_attribution_map = attribution_map[:, band_mask == segment_id]\n\n        current_mean = current_attribution_map.numpy().mean(axis=0)\n        if aggregate_results:\n            lolims = current_attribution_map.numpy().min(axis=0)\n            uplims = current_attribution_map.numpy().max(axis=0)\n\n            ax.errorbar(\n                current_wavelengths.numpy(),\n                current_mean,\n                yerr=[current_mean - lolims, uplims - current_mean],\n                label=band_name,\n                color=color_palette[idx],\n                linestyle=\"--\",\n                marker=\"o\",\n                markersize=5,\n            )\n        else:\n            ax.scatter(\n                current_wavelengths.numpy(),\n                current_mean,\n                label=band_name,\n                color=color_palette[idx],\n            )\n\n    if show_legend:\n        ax.legend(title=\"SuperBand\")\n\n    return ax\n</code></pre>"},{"location":"reference/#src.meteors.visualize.attr_visualize.visualize_spectral_attributes_by_magnitude","title":"<code>visualize_spectral_attributes_by_magnitude(spectral_attributes, ax, color_palette=None, annotate_bars=True, show_not_included=True)</code>","text":"<p>Visualizes the spectral attributes by magnitude.</p> <p>Parameters:</p> Name Type Description Default <code>spectral_attributes</code> <code>HSISpectralAttributes | list[HSISpectralAttributes]</code> <p>The spectral attributes to visualize.</p> required <code>ax</code> <code>Axes | None</code> <p>The matplotlib Axes object to plot the visualization on. If None, a new Axes object will be created.</p> required <code>color_palette</code> <code>list[str] | None</code> <p>The color palette to use for the visualization. If None, a default color palette will be used.</p> <code>None</code> <code>annotate_bars</code> <code>bool</code> <p>Whether to annotate the bars with their magnitudes. Defaults to True.</p> <code>True</code> <code>show_not_included</code> <code>bool</code> <p>Whether to show the 'not_included' band in the visualization. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Axes</code> <code>Axes</code> <p>The matplotlib Axes object containing the visualization.</p> <p>Raises:     TypeError: If the spectral attributes are not an HSISpectralAttributes object or a list of HSISpectralAttributes objects.</p> Source code in <code>src/meteors/visualize/attr_visualize.py</code> <pre><code>def visualize_spectral_attributes_by_magnitude(\n    spectral_attributes: HSISpectralAttributes | list[HSISpectralAttributes],\n    ax: Axes | None,\n    color_palette: list[str] | None = None,\n    annotate_bars: bool = True,\n    show_not_included: bool = True,\n) -&gt; Axes:\n    \"\"\"Visualizes the spectral attributes by magnitude.\n\n    Args:\n        spectral_attributes (HSISpectralAttributes | list[HSISpectralAttributes]):\n            The spectral attributes to visualize.\n        ax (Axes | None): The matplotlib Axes object to plot the visualization on.\n            If None, a new Axes object will be created.\n        color_palette (list[str] | None): The color palette to use for the visualization.\n            If None, a default color palette will be used.\n        annotate_bars (bool): Whether to annotate the bars with their magnitudes.\n            Defaults to True.\n        show_not_included (bool): Whether to show the 'not_included' band in the visualization.\n            Defaults to True.\n\n    Returns:\n        Axes: The matplotlib Axes object containing the visualization.\n    Raises:\n        TypeError: If the spectral attributes are not an HSISpectralAttributes object or a list of HSISpectralAttributes objects.\n    \"\"\"\n    if isinstance(spectral_attributes, HSISpectralAttributes):\n        spectral_attributes = [spectral_attributes]\n    if not (\n        isinstance(spectral_attributes, list)\n        and all(isinstance(attr, HSISpectralAttributes) for attr in spectral_attributes)\n    ):\n        raise TypeError(\n            \"spectral_attributes parameter must be an HSISpectralAttributes object or a list of HSISpectralAttributes objects.\"\n        )\n\n    aggregate_results = False if len(spectral_attributes) == 1 else True\n    band_names = dict(spectral_attributes[0].band_names)\n    wavelengths = spectral_attributes[0].hsi.wavelengths\n    validate_consistent_band_and_wavelengths(band_names, wavelengths, spectral_attributes)\n\n    ax = setup_visualization(ax, \"Attributions by Magnitude\", \"Group\", \"Average Attribution Magnitude\")\n    ax.tick_params(axis=\"x\", rotation=45)\n\n    band_names = _merge_band_names_segments(band_names)  # type: ignore\n    labels = list(band_names.keys())\n\n    if not show_not_included and band_names.get(\"not_included\") is not None:\n        band_names.pop(\"not_included\")\n        labels = list(band_names.keys())\n\n    if color_palette is None:\n        color_palette = sns.color_palette(\"hsv\", len(band_names.keys()))\n\n    band_mask = spectral_attributes[0].band_mask.cpu()\n    attribution_map = torch.stack([attr.flattened_attributes.cpu() for attr in spectral_attributes])\n    avg_magnitudes = calculate_average_magnitudes(band_names, band_mask, attribution_map)\n\n    if aggregate_results:\n        boxplot = ax.boxplot(avg_magnitudes, labels=labels, patch_artist=True)\n        for patch, color in zip(boxplot[\"boxes\"], color_palette):\n            patch.set_facecolor(color)\n\n    else:\n        bars = ax.bar(labels, avg_magnitudes, color=color_palette)\n        if annotate_bars:\n            for bar in bars:\n                height = bar.get_height()\n                ax.annotate(\n                    f\"{height:.2f}\",\n                    xy=(bar.get_x() + bar.get_width() / 2, height),\n                    xytext=(0, 3),  # 3 points vertical offset\n                    textcoords=\"offset points\",\n                    ha=\"center\",\n                    va=\"bottom\",\n                )\n    return ax\n</code></pre>"},{"location":"reference/#src.meteors.visualize.attr_visualize.visualize_spectral_attributes","title":"<code>visualize_spectral_attributes(spectral_attributes, ax=None, use_pyplot=False, color_palette=None, show_not_included=True)</code>","text":"<p>Visualizes the spectral attributes of an hsi object or a list of hsi objects.</p> <p>Parameters:</p> Name Type Description Default <code>spectral_attributes</code> <code>HSISpectralAttributes | list[HSISpectralAttributes]</code> <p>The spectral attributes of the image object to visualize.</p> required <code>ax</code> <code>Axes | None</code> <p>The axes object to plot the visualization on. If None, a new axes will be created.</p> <code>None</code> <code>use_pyplot</code> <code>bool</code> <p>If ax is not None, use_pyplot is ignored. If True, displays the visualization using pyplot. If False, returns the figure and axes objects. Defaults to False.</p> <code>False</code> <code>color_palette</code> <code>list[str] | None</code> <p>The color palette to use for visualizing different spectral bands. If None, a default color palette is used. Defaults to None.</p> <code>None</code> <code>show_not_included</code> <code>bool</code> <p>If True, includes the spectral bands that are not included in the visualization. If False, only includes the spectral bands that are included in the visualization. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple[Figure, Axes] | Axes | None</code> <p>tuple[matplotlib.figure.Figure, matplotlib.axes.Axes] | matplotlib.axes.Axes | None: If ax is not None, returns the axes object. If use_pyplot is True, returns None. If use_pyplot is False, returns the figure and axes objects.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If ax is provided as a single axes object and not a list of axes objects.</p> <code>ValueError</code> <p>If agg is True and the axes have less than 3 rows or 3 columns.</p> <code>ValueError</code> <p>If agg is False and the axes have less than 2 rows or 2 columns.</p> Source code in <code>src/meteors/visualize/attr_visualize.py</code> <pre><code>def visualize_spectral_attributes(\n    spectral_attributes: HSISpectralAttributes | list[HSISpectralAttributes],\n    ax: Axes | None = None,\n    use_pyplot: bool = False,\n    color_palette: list[str] | None = None,\n    show_not_included: bool = True,\n) -&gt; tuple[Figure, Axes] | Axes | None:\n    \"\"\"Visualizes the spectral attributes of an hsi object or a list of hsi objects.\n\n    Args:\n        spectral_attributes (HSISpectralAttributes | list[HSISpectralAttributes]):\n            The spectral attributes of the image object to visualize.\n        ax (Axes | None, optional):\n            The axes object to plot the visualization on. If None, a new axes will be created.\n        use_pyplot (bool, optional):\n            If ax is not None, use_pyplot is ignored.\n            If True, displays the visualization using pyplot.\n            If False, returns the figure and axes objects. Defaults to False.\n        color_palette (list[str] | None, optional):\n            The color palette to use for visualizing different spectral bands.\n            If None, a default color palette is used.\n            Defaults to None.\n        show_not_included (bool, optional):\n            If True, includes the spectral bands that are not included in the visualization.\n            If False, only includes the spectral bands that are included in the visualization.\n            Defaults to True.\n\n    Returns:\n        tuple[matplotlib.figure.Figure, matplotlib.axes.Axes] | matplotlib.axes.Axes | None:\n            If ax is not None, returns the axes object.\n            If use_pyplot is True, returns None.\n            If use_pyplot is False, returns the figure and axes objects.\n\n    Raises:\n        ValueError: If ax is provided as a single axes object and not a list of axes objects.\n        ValueError: If agg is True and the axes have less than 3 rows or 3 columns.\n        ValueError: If agg is False and the axes have less than 2 rows or 2 columns.\n    \"\"\"\n    agg = True if isinstance(spectral_attributes, list) else False\n    band_names = spectral_attributes[0].band_names if agg else spectral_attributes.band_names  # type: ignore\n\n    color_palette = color_palette or sns.color_palette(\"hsv\", len(band_names.keys()))\n\n    use_ax = True\n    if ax is None:\n        use_ax = False\n        fig, ax = plt.subplots(1, 3 if agg else 2, figsize=(15, 5))\n        fig.suptitle(\"Spectral Attributes Visualization\")\n\n    if not hasattr(ax, \"shape\"):\n        raise ValueError(\"Provided as is one axes object, but it should be a list of axes objects\")\n    if agg and (len(ax.shape) != 1 or ax.shape[0] &lt; 3):\n        raise ValueError(\"The axes should have at least 3 rows or 3 columns if agg is True\")\n    if not agg and (len(ax.shape) != 1 or ax.shape[0] &lt; 2):\n        raise ValueError(\"The axes should have at least 2 rows or 2 columns if agg is False\")\n\n    visualize_spectral_attributes_by_waveband(\n        spectral_attributes,\n        ax[0],\n        color_palette=color_palette,\n        show_not_included=show_not_included,\n        show_legend=False,\n    )\n\n    visualize_spectral_attributes_by_magnitude(\n        spectral_attributes,\n        ax[1],\n        color_palette=color_palette,\n        show_not_included=show_not_included,\n    )\n\n    if agg:\n        scores = [attr.score for attr in spectral_attributes]  # type: ignore\n        mean_score = sum(scores) / len(scores)  # type: ignore\n        ax[2].hist(scores, bins=50, color=\"steelblue\", alpha=0.7)\n        ax[2].axvline(mean_score, color=\"darkred\", linestyle=\"dashed\")\n\n        ax[2].set_title(\"Distribution of Score Values\")\n        ax[2].set_xlabel(\"Score\")\n        ax[2].set_ylabel(\"Frequency\")\n\n    if use_ax:\n        return ax\n\n    if use_pyplot:\n        plt.show()  # pragma: no cover\n        return None  # pragma: no cover\n\n    return fig, ax\n</code></pre>"},{"location":"reference/#src.meteors.visualize.attr_visualize.visualize_spatial_attributes","title":"<code>visualize_spatial_attributes(spatial_attributes, ax=None, use_pyplot=False)</code>","text":"<p>Visualizes the spatial attributes of an hsi using Lime attribution.</p> <p>Parameters:</p> Name Type Description Default <code>spatial_attributes</code> <code>HSISpatialAttributes</code> <p>The spatial attributes of the image object to visualize.</p> required <code>ax</code> <code>Axes | None</code> <p>The axes object to plot the visualization on. If None, a new axes will be created.</p> <code>None</code> <code>use_pyplot</code> <code>bool</code> <p>Whether to use pyplot for visualization. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[Figure, Axes] | Axes | None</code> <p>tuple[matplotlib.figure.Figure, matplotlib.axes.Axes] | matplotlib.axes.Axes | None: If ax is not None, returns the axes object. If use_pyplot is True, returns None. If use_pyplot is False, returns the figure and axes objects.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the axes have less 3 rows or 3 columns</p> <code>ValueError</code> <p>If the axes object is not a list of axes objects</p> Source code in <code>src/meteors/visualize/attr_visualize.py</code> <pre><code>def visualize_spatial_attributes(\n    spatial_attributes: HSISpatialAttributes, ax: Axes | None = None, use_pyplot: bool = False\n) -&gt; tuple[Figure, Axes] | Axes | None:\n    \"\"\"Visualizes the spatial attributes of an hsi using Lime attribution.\n\n    Args:\n        spatial_attributes (HSISpatialAttributes):\n            The spatial attributes of the image object to visualize.\n        ax (Axes | None, optional):\n            The axes object to plot the visualization on. If None, a new axes will be created.\n        use_pyplot (bool, optional):\n            Whether to use pyplot for visualization. Defaults to False.\n\n    Returns:\n        tuple[matplotlib.figure.Figure, matplotlib.axes.Axes] | matplotlib.axes.Axes | None:\n            If ax is not None, returns the axes object.\n            If use_pyplot is True, returns None.\n            If use_pyplot is False, returns the figure and axes objects.\n\n    Raises:\n        ValueError: If the axes have less 3 rows or 3 columns\n        ValueError: If the axes object is not a list of axes objects\n    \"\"\"\n    mask_enabled = spatial_attributes.segmentation_mask is not None\n    use_ax = True\n    if ax is None:\n        use_ax = False\n        fig, ax = plt.subplots(1, 3 if mask_enabled else 2, figsize=(15, 5))\n        fig.suptitle(\"Spatial Attributes Visualization\")\n\n    if not hasattr(ax, \"shape\"):\n        raise ValueError(\"Provided as is one axes object, but it should be a list of axes objects\")\n    elif len(ax.shape) != 1 or ax.shape[0] &lt; 3:\n        raise ValueError(\"The axes should have at least 3 rows or 3 columns\")\n    else:\n        fig = ax[0].get_figure()\n\n    spatial_attributes = spatial_attributes.change_orientation(\"HWC\", inplace=False)\n\n    if mask_enabled:\n        mask = spatial_attributes.segmentation_mask.cpu()\n\n        group_names = mask.unique().tolist()\n        colors = sns.color_palette(\"hsv\", len(group_names))\n        color_map = dict(zip(group_names, colors))\n\n        for unique in group_names:\n            segment_indices = torch.argwhere(mask == unique)\n\n            y_center, x_center = segment_indices.numpy().mean(axis=0).astype(int)\n            ax[1].text(x_center, y_center, str(unique), color=color_map[unique], fontsize=8, ha=\"center\", va=\"center\")\n            ax[2].text(x_center, y_center, str(unique), color=color_map[unique], fontsize=8, ha=\"center\", va=\"center\")\n\n        ax[2].imshow(mask.numpy() / mask.max(), cmap=\"gray\")\n        ax[2].set_title(\"Mask\")\n        ax[2].grid(False)\n        ax[2].axis(\"off\")\n\n    ax[0].imshow(spatial_attributes.hsi.get_rgb_image(output_channel_axis=2).cpu())\n    ax[0].set_title(\"Original image\")\n    ax[0].grid(False)\n    ax[0].axis(\"off\")\n\n    attrs = spatial_attributes.attributes.cpu().numpy()\n    if np.all(attrs == 0):\n        logger.warning(\"All spatial attributes are zero.\")\n        cmap = LinearSegmentedColormap.from_list(\"RdWhGn\", [\"red\", \"white\", \"green\"])\n        heat_map = ax[1].imshow(attrs.sum(axis=-1), cmap=cmap, vmin=-1, vmax=1)\n\n        axis_separator = make_axes_locatable(ax[1])\n        colorbar_axis = axis_separator.append_axes(\"bottom\", size=\"5%\", pad=0.1)\n        fig.colorbar(heat_map, orientation=\"horizontal\", cax=colorbar_axis)\n    else:\n        viz.visualize_image_attr(\n            attrs,\n            method=\"heat_map\",\n            sign=\"all\",\n            plt_fig_axis=(fig, ax[1]),\n            show_colorbar=True,\n            use_pyplot=False,\n        )\n    ax[1].set_title(\"Attribution Map\")\n    ax[1].axis(\"off\")\n\n    if use_ax:\n        return ax\n\n    if use_pyplot:\n        plt.show()  # pragma: no cover\n        return None  # pragma: no cover\n    else:\n        return fig, ax\n</code></pre>"},{"location":"reference/#attribution-methods","title":"Attribution Methods","text":""},{"location":"reference/#src.meteors.attr.attributes.HSIAttributes","title":"<code>HSIAttributes</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents an object that contains Hyperspectral image attributes and explanations.</p> <p>Attributes:</p> Name Type Description <code>hsi</code> <code>HSI</code> <p>Hyperspectral image object for which the explanations were created.</p> <code>attributes</code> <code>Tensor</code> <p>Attributions (explanations) for the hsi.</p> <code>score</code> <code>float</code> <p>The score provided by the interpretable model. Can be None if method don't provide one.</p> <code>device</code> <code>device</code> <p>Device to be used for inference. If None, the device of the input hsi will be used. Defaults to None.</p> <code>attribution_method</code> <code>str | None</code> <p>The method used to generate the explanation. Defaults to None.</p> Source code in <code>src/meteors/attr/attributes.py</code> <pre><code>class HSIAttributes(BaseModel):\n    \"\"\"Represents an object that contains Hyperspectral image attributes and explanations.\n\n    Attributes:\n        hsi (HSI): Hyperspectral image object for which the explanations were created.\n        attributes (torch.Tensor): Attributions (explanations) for the hsi.\n        score (float): The score provided by the interpretable model. Can be None if method don't provide one.\n        device (torch.device): Device to be used for inference. If None, the device of the input hsi will be used.\n            Defaults to None.\n        attribution_method (str | None): The method used to generate the explanation. Defaults to None.\n    \"\"\"\n\n    hsi: Annotated[\n        HSI,\n        Field(\n            description=\"Hyperspectral image object for which the explanations were created.\",\n        ),\n    ]\n    attributes: Annotated[\n        torch.Tensor,\n        BeforeValidator(validate_and_convert_attributes),\n        Field(\n            description=\"Attributions (explanations) for the hsi.\",\n        ),\n    ]\n    attribution_method: Annotated[\n        str | None,\n        AfterValidator(validate_attribution_method),\n        Field(\n            description=\"The method used to generate the explanation.\",\n        ),\n    ] = None\n    score: Annotated[\n        float | None,\n        Field(\n            validate_default=True,\n            description=\"The score provided by the interpretable model. Can be None if method don't provide one.\",\n        ),\n    ] = None\n    mask: Annotated[\n        torch.Tensor | None,\n        BeforeValidator(validate_and_convert_mask),\n        Field(\n            description=\"`superpixel` or `superband` mask used for the explanation.\",\n        ),\n    ] = None\n    device: Annotated[\n        torch.device,\n        BeforeValidator(resolve_inference_device_attributes),\n        Field(\n            validate_default=True,\n            exclude=True,\n            description=(\n                \"Device to be used for inference. If None, the device of the input hsi will be used. \"\n                \"Defaults to None.\"\n            ),\n        ),\n    ] = None\n\n    @property\n    def flattened_attributes(self) -&gt; torch.Tensor:\n        \"\"\"Returns a flattened tensor of attributes.\n\n        This method should be implemented in the subclass.\n\n        Returns:\n            torch.Tensor: A flattened tensor of attributes.\n        \"\"\"\n        raise NotImplementedError(\"The `flattened_attributes` property must be implemented in the subclass\")\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @property\n    def orientation(self) -&gt; tuple[str, str, str]:\n        \"\"\"Returns the orientation of the hsi.\n\n        Returns:\n            tuple[str, str, str]: The orientation of the hsi corresponding to the attributes.\n        \"\"\"\n        return self.hsi.orientation\n\n    def _validate_hsi_attributions_and_mask(self) -&gt; None:\n        \"\"\"Validates the hsi attributions and performs necessary operations to ensure compatibility with the device.\n\n        Raises:\n            ValueError: If the shapes of the attributes and hsi tensors do not match.\n        \"\"\"\n        validate_shapes(self.attributes, self.hsi)\n\n        self.attributes = self.attributes.to(self.device)\n        if self.device != self.hsi.device:\n            self.hsi.to(self.device)\n\n        if self.mask is not None:\n            validate_shapes(self.mask, self.hsi)\n            self.mask = self.mask.to(self.device)\n\n    @model_validator(mode=\"after\")\n    def validate_hsi_attributions(self) -&gt; Self:\n        \"\"\"Validates the hsi attributions.\n\n        This method performs validation on the hsi attributions to ensure they are correct.\n\n        Returns:\n            Self: The current instance of the class.\n        \"\"\"\n        self._validate_hsi_attributions_and_mask()\n        return self\n\n    def to(self, device: str | torch.device) -&gt; Self:\n        \"\"\"Move the hsi and attributes tensors to the specified device.\n\n        Args:\n            device (str or torch.device): The device to move the tensors to.\n\n        Returns:\n            Self: The modified object with tensors moved to the specified device.\n\n        Examples:\n            &gt;&gt;&gt; attrs = HSIAttributes(hsi, attributes, score=0.5)\n            &gt;&gt;&gt; attrs.to(\"cpu\")\n            &gt;&gt;&gt; attrs.hsi.device\n            device(type='cpu')\n            &gt;&gt;&gt; attrs.attributes.device\n            device(type='cpu')\n            &gt;&gt;&gt; attrs.to(\"cuda\")\n            &gt;&gt;&gt; attrs.hsi.device\n            device(type='cuda')\n            &gt;&gt;&gt; attrs.attributes.device\n            device(type='cuda')\n        \"\"\"\n        self.hsi = self.hsi.to(device)\n        self.attributes = self.attributes.to(device)\n        self.device = self.hsi.device\n        return self\n\n    def change_orientation(self, target_orientation: tuple[str, str, str] | list[str] | str, inplace=False) -&gt; Self:\n        \"\"\"Changes the orientation of the image data along with the attributions to the target orientation.\n\n        Args:\n            target_orientation (tuple[str, str, str] | list[str] | str): The target orientation for the attribution data.\n                This should be a tuple of three one-letter strings in any order: \"C\", \"H\", \"W\".\n            inplace (bool, optional): Whether to modify the data in place or return a new object.\n\n        Returns:\n            Self: The updated Image object with the new orientation.\n\n        Raises:\n            OrientationError: If the target orientation is not a valid tuple of three one-letter strings.\n        \"\"\"\n        current_orientation = self.orientation\n        hsi = self.hsi.change_orientation(target_orientation, inplace=inplace)\n        if inplace:\n            attrs = self\n        else:\n            attrs = self.model_copy()\n            attrs.hsi = hsi\n\n        # now change the orientation of the attributes\n        if current_orientation == target_orientation:\n            return attrs\n\n        permute_dims = [current_orientation.index(dim) for dim in target_orientation]\n\n        attrs.attributes = attrs.attributes.permute(permute_dims)\n\n        if attrs.mask is not None:\n            attrs.mask = attrs.mask.permute(permute_dims)\n        return attrs\n</code></pre>"},{"location":"reference/#src.meteors.attr.attributes.HSIAttributes.flattened_attributes","title":"<code>flattened_attributes: torch.Tensor</code>  <code>property</code>","text":"<p>Returns a flattened tensor of attributes.</p> <p>This method should be implemented in the subclass.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A flattened tensor of attributes.</p>"},{"location":"reference/#src.meteors.attr.attributes.HSIAttributes.orientation","title":"<code>orientation: tuple[str, str, str]</code>  <code>property</code>","text":"<p>Returns the orientation of the hsi.</p> <p>Returns:</p> Type Description <code>tuple[str, str, str]</code> <p>tuple[str, str, str]: The orientation of the hsi corresponding to the attributes.</p>"},{"location":"reference/#src.meteors.attr.attributes.HSIAttributes.change_orientation","title":"<code>change_orientation(target_orientation, inplace=False)</code>","text":"<p>Changes the orientation of the image data along with the attributions to the target orientation.</p> <p>Parameters:</p> Name Type Description Default <code>target_orientation</code> <code>tuple[str, str, str] | list[str] | str</code> <p>The target orientation for the attribution data. This should be a tuple of three one-letter strings in any order: \"C\", \"H\", \"W\".</p> required <code>inplace</code> <code>bool</code> <p>Whether to modify the data in place or return a new object.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>The updated Image object with the new orientation.</p> <p>Raises:</p> Type Description <code>OrientationError</code> <p>If the target orientation is not a valid tuple of three one-letter strings.</p> Source code in <code>src/meteors/attr/attributes.py</code> <pre><code>def change_orientation(self, target_orientation: tuple[str, str, str] | list[str] | str, inplace=False) -&gt; Self:\n    \"\"\"Changes the orientation of the image data along with the attributions to the target orientation.\n\n    Args:\n        target_orientation (tuple[str, str, str] | list[str] | str): The target orientation for the attribution data.\n            This should be a tuple of three one-letter strings in any order: \"C\", \"H\", \"W\".\n        inplace (bool, optional): Whether to modify the data in place or return a new object.\n\n    Returns:\n        Self: The updated Image object with the new orientation.\n\n    Raises:\n        OrientationError: If the target orientation is not a valid tuple of three one-letter strings.\n    \"\"\"\n    current_orientation = self.orientation\n    hsi = self.hsi.change_orientation(target_orientation, inplace=inplace)\n    if inplace:\n        attrs = self\n    else:\n        attrs = self.model_copy()\n        attrs.hsi = hsi\n\n    # now change the orientation of the attributes\n    if current_orientation == target_orientation:\n        return attrs\n\n    permute_dims = [current_orientation.index(dim) for dim in target_orientation]\n\n    attrs.attributes = attrs.attributes.permute(permute_dims)\n\n    if attrs.mask is not None:\n        attrs.mask = attrs.mask.permute(permute_dims)\n    return attrs\n</code></pre>"},{"location":"reference/#src.meteors.attr.attributes.HSIAttributes.to","title":"<code>to(device)</code>","text":"<p>Move the hsi and attributes tensors to the specified device.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str or device</code> <p>The device to move the tensors to.</p> required <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>The modified object with tensors moved to the specified device.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; attrs = HSIAttributes(hsi, attributes, score=0.5)\n&gt;&gt;&gt; attrs.to(\"cpu\")\n&gt;&gt;&gt; attrs.hsi.device\ndevice(type='cpu')\n&gt;&gt;&gt; attrs.attributes.device\ndevice(type='cpu')\n&gt;&gt;&gt; attrs.to(\"cuda\")\n&gt;&gt;&gt; attrs.hsi.device\ndevice(type='cuda')\n&gt;&gt;&gt; attrs.attributes.device\ndevice(type='cuda')\n</code></pre> Source code in <code>src/meteors/attr/attributes.py</code> <pre><code>def to(self, device: str | torch.device) -&gt; Self:\n    \"\"\"Move the hsi and attributes tensors to the specified device.\n\n    Args:\n        device (str or torch.device): The device to move the tensors to.\n\n    Returns:\n        Self: The modified object with tensors moved to the specified device.\n\n    Examples:\n        &gt;&gt;&gt; attrs = HSIAttributes(hsi, attributes, score=0.5)\n        &gt;&gt;&gt; attrs.to(\"cpu\")\n        &gt;&gt;&gt; attrs.hsi.device\n        device(type='cpu')\n        &gt;&gt;&gt; attrs.attributes.device\n        device(type='cpu')\n        &gt;&gt;&gt; attrs.to(\"cuda\")\n        &gt;&gt;&gt; attrs.hsi.device\n        device(type='cuda')\n        &gt;&gt;&gt; attrs.attributes.device\n        device(type='cuda')\n    \"\"\"\n    self.hsi = self.hsi.to(device)\n    self.attributes = self.attributes.to(device)\n    self.device = self.hsi.device\n    return self\n</code></pre>"},{"location":"reference/#src.meteors.attr.attributes.HSISpatialAttributes","title":"<code>HSISpatialAttributes</code>","text":"<p>               Bases: <code>HSIAttributes</code></p> <p>Represents spatial attributes of an hsi used for explanation.</p> <p>Attributes:</p> Name Type Description <code>hsi</code> <code>HSI</code> <p>Hyperspectral image object for which the explanations were created.</p> <code>attributes</code> <code>Tensor</code> <p>Attributions (explanations) for the hsi.</p> <code>score</code> <code>float</code> <p>The score provided by the interpretable model. Can be None if method don't provide one.</p> <code>device</code> <code>device</code> <p>Device to be used for inference. If None, the device of the input hsi will be used. Defaults to None.</p> <code>attribution_method</code> <code>str | None</code> <p>The method used to generate the explanation. Defaults to None.</p> <code>segmentation_mask</code> <code>Tensor</code> <p>Spatial (Segmentation) mask used for the explanation.</p> <code>flattened_attributes</code> <code>Tensor</code> <p>Spatial 2D attribution map.</p> Source code in <code>src/meteors/attr/attributes.py</code> <pre><code>class HSISpatialAttributes(HSIAttributes):\n    \"\"\"Represents spatial attributes of an hsi used for explanation.\n\n    Attributes:\n        hsi (HSI): Hyperspectral image object for which the explanations were created.\n        attributes (torch.Tensor): Attributions (explanations) for the hsi.\n        score (float): The score provided by the interpretable model. Can be None if method don't provide one.\n        device (torch.device): Device to be used for inference. If None, the device of the input hsi will be used.\n            Defaults to None.\n        attribution_method (str | None): The method used to generate the explanation. Defaults to None.\n        segmentation_mask (torch.Tensor): Spatial (Segmentation) mask used for the explanation.\n        flattened_attributes (torch.Tensor): Spatial 2D attribution map.\n    \"\"\"\n\n    @property\n    def segmentation_mask(self) -&gt; torch.Tensor:\n        \"\"\"Returns the 2D spatial segmentation mask that has the same size as the hsi image.\n\n        Returns:\n            torch.Tensor: The segmentation mask tensor.\n\n        Raises:\n            HSIAttributesError: If the segmentation mask is not provided in the attributes object.\n        \"\"\"\n        if self.mask is None:\n            raise HSIAttributesError(\"Segmentation mask is not provided in the attributes object\")\n        return self.mask.select(dim=self.hsi.spectral_axis, index=0)\n\n    @property\n    def flattened_attributes(self) -&gt; torch.Tensor:\n        \"\"\"Returns a flattened tensor of attributes, with removed repeated dimensions.\n\n        In the case of spatial attributes, the flattened attributes are 2D spatial attributes of shape (rows, columns) and the spectral dimension is removed.\n\n        Examples:\n            &gt;&gt;&gt; segmentation_mask = torch.zeros((3, 2, 2))\n            &gt;&gt;&gt; attrs = HSISpatialAttributes(hsi, attributes, score=0.5, segmentation_mask=segmentation_mask)\n            &gt;&gt;&gt; attrs.flattened_attributes\n                tensor([[0., 0.],\n                        [0., 0.]])\n\n        Returns:\n            torch.Tensor: A flattened tensor of attributes.\n        \"\"\"\n        return self.attributes.select(dim=self.hsi.spectral_axis, index=0)\n\n    def _validate_hsi_attributions_and_mask(self) -&gt; None:\n        \"\"\"Validates the hsi attributions and performs necessary operations to ensure compatibility with the device.\n\n        Raises:\n            HSIAttributesError: If the segmentation mask is not provided in the attributes object.\n        \"\"\"\n        super()._validate_hsi_attributions_and_mask()\n        if self.mask is None:\n            raise HSIAttributesError(\"Segmentation mask is not provided in the attributes object\")\n</code></pre>"},{"location":"reference/#src.meteors.attr.attributes.HSISpatialAttributes.flattened_attributes","title":"<code>flattened_attributes: torch.Tensor</code>  <code>property</code>","text":"<p>Returns a flattened tensor of attributes, with removed repeated dimensions.</p> <p>In the case of spatial attributes, the flattened attributes are 2D spatial attributes of shape (rows, columns) and the spectral dimension is removed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; segmentation_mask = torch.zeros((3, 2, 2))\n&gt;&gt;&gt; attrs = HSISpatialAttributes(hsi, attributes, score=0.5, segmentation_mask=segmentation_mask)\n&gt;&gt;&gt; attrs.flattened_attributes\n    tensor([[0., 0.],\n            [0., 0.]])\n</code></pre> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A flattened tensor of attributes.</p>"},{"location":"reference/#src.meteors.attr.attributes.HSISpatialAttributes.segmentation_mask","title":"<code>segmentation_mask: torch.Tensor</code>  <code>property</code>","text":"<p>Returns the 2D spatial segmentation mask that has the same size as the hsi image.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The segmentation mask tensor.</p> <p>Raises:</p> Type Description <code>HSIAttributesError</code> <p>If the segmentation mask is not provided in the attributes object.</p>"},{"location":"reference/#src.meteors.attr.attributes.HSISpectralAttributes","title":"<code>HSISpectralAttributes</code>","text":"<p>               Bases: <code>HSIAttributes</code></p> <p>Represents an hsi with spectral attributes used for explanation.</p> <p>Attributes:</p> Name Type Description <code>hsi</code> <code>HSI</code> <p>Hyperspectral hsi object for which the explanations were created.</p> <code>attributes</code> <code>Tensor</code> <p>Attributions (explanations) for the hsi.</p> <code>score</code> <code>float</code> <p>R^2 score of interpretable model used for the explanation.</p> <code>device</code> <code>device</code> <p>Device to be used for inference. If None, the device of the input hsi will be used. Defaults to None.</p> <code>attribution_method</code> <code>str | None</code> <p>The method used to generate the explanation. Defaults to None.</p> <code>band_mask</code> <code>Tensor</code> <p>Band mask used for the explanation.</p> <code>band_names</code> <code>dict[str | tuple[str, ...], int]</code> <p>Dictionary that translates the band names into the band segment ids.</p> <code>flattened_attributes</code> <code>Tensor</code> <p>Spectral 1D attribution map.</p> Source code in <code>src/meteors/attr/attributes.py</code> <pre><code>class HSISpectralAttributes(HSIAttributes):\n    \"\"\"Represents an hsi with spectral attributes used for explanation.\n\n    Attributes:\n        hsi (HSI): Hyperspectral hsi object for which the explanations were created.\n        attributes (torch.Tensor): Attributions (explanations) for the hsi.\n        score (float): R^2 score of interpretable model used for the explanation.\n        device (torch.device): Device to be used for inference. If None, the device of the input hsi will be used.\n            Defaults to None.\n        attribution_method (str | None): The method used to generate the explanation. Defaults to None.\n        band_mask (torch.Tensor): Band mask used for the explanation.\n        band_names (dict[str | tuple[str, ...], int]): Dictionary that translates the band names into the band segment ids.\n        flattened_attributes (torch.Tensor): Spectral 1D attribution map.\n    \"\"\"\n\n    band_names: Annotated[\n        dict[str | tuple[str, ...], int],\n        Field(\n            description=\"Dictionary that translates the band names into the band segment ids.\",\n        ),\n    ]\n\n    @property\n    def band_mask(self) -&gt; torch.Tensor:\n        \"\"\"Returns a 1D band mask - a band mask with removed repeated dimensions (num_bands, ),\n        where num_bands is the number of bands in the hsi image.\n\n        The method selects the appropriate dimensions from the `band_mask` tensor\n        based on the `axis_to_select` and returns a flattened version of the selected\n        tensor.\n\n        Returns:\n            torch.Tensor: The flattened band mask tensor.\n\n        Examples:\n            &gt;&gt;&gt; band_names = {\"R\": 0, \"G\": 1, \"B\": 2}\n            &gt;&gt;&gt; attrs = HSISpectralAttributes(hsi, attributes, score=0.5, mask=band_mask)\n            &gt;&gt;&gt; attrs.flattened_band_mask\n            torch.tensor([0, 1, 2])\n        \"\"\"\n        if self.mask is None:\n            raise ValueError(\"Band mask is not provided\")\n        axis_to_select = [i for i in range(self.hsi.image.ndim) if i != self.hsi.spectral_axis]\n        return self.mask.select(dim=axis_to_select[0], index=0).select(dim=axis_to_select[1] - 1, index=0)\n\n    @property\n    def flattened_attributes(self) -&gt; torch.Tensor:\n        \"\"\"Returns a flattened tensor of attributes with removed repeated dimensions.\n\n        In the case of spectral attributes, the flattened attributes are 1D tensor of shape (num_bands, ), where num_bands is the number of bands in the hsi image.\n\n        Returns:\n            torch.Tensor: A flattened tensor of attributes.\n        \"\"\"\n        axis = [i for i in range(self.attributes.ndim) if i != self.hsi.spectral_axis]\n        return self.attributes.select(dim=axis[0], index=0).select(dim=axis[1] - 1, index=0)\n\n    def _validate_hsi_attributions_and_mask(self) -&gt; None:\n        \"\"\"Validates the hsi attributions and performs necessary operations to ensure compatibility with the device.\n\n        Raises:\n            HSIAttributesError: If the band mask is not provided in the attributes object\n        \"\"\"\n        super()._validate_hsi_attributions_and_mask()\n        if self.mask is None:\n            raise HSIAttributesError(\"Band mask is not provided in the attributes object\")\n\n        self.band_names = align_band_names_with_mask(self.band_names, self.mask)\n</code></pre>"},{"location":"reference/#src.meteors.attr.attributes.HSISpectralAttributes.band_mask","title":"<code>band_mask: torch.Tensor</code>  <code>property</code>","text":"<p>Returns a 1D band mask - a band mask with removed repeated dimensions (num_bands, ), where num_bands is the number of bands in the hsi image.</p> <p>The method selects the appropriate dimensions from the <code>band_mask</code> tensor based on the <code>axis_to_select</code> and returns a flattened version of the selected tensor.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The flattened band mask tensor.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; band_names = {\"R\": 0, \"G\": 1, \"B\": 2}\n&gt;&gt;&gt; attrs = HSISpectralAttributes(hsi, attributes, score=0.5, mask=band_mask)\n&gt;&gt;&gt; attrs.flattened_band_mask\ntorch.tensor([0, 1, 2])\n</code></pre>"},{"location":"reference/#src.meteors.attr.attributes.HSISpectralAttributes.flattened_attributes","title":"<code>flattened_attributes: torch.Tensor</code>  <code>property</code>","text":"<p>Returns a flattened tensor of attributes with removed repeated dimensions.</p> <p>In the case of spectral attributes, the flattened attributes are 1D tensor of shape (num_bands, ), where num_bands is the number of bands in the hsi image.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A flattened tensor of attributes.</p>"},{"location":"reference/#src.meteors.attr.lime.Lime","title":"<code>Lime</code>","text":"<p>               Bases: <code>Explainer</code></p> <p>Lime class is a subclass of Explainer and represents the Lime explainer. Lime is an interpretable model-agnostic explanation method that explains the predictions of a black-box model by approximating it with a simpler interpretable model. The Lime method is based on the <code>captum</code> implementation and is an implementation of an idea coming from the original paper on Lime, where more details about this method can be found.</p> <p>Parameters:</p> Name Type Description Default <code>explainable_model</code> <code>ExplainableModel</code> <p>The explainable model to be explained.</p> required <code>interpretable_model</code> <code>InterpretableModel</code> <p>The interpretable model used to approximate the black-box model. Defaults to <code>SkLearnLasso</code> with alpha parameter set to 0.08.</p> <code>SkLearnLasso(alpha=0.08)</code> <code>similarity_func</code> <code>Callable[[Tensor], Tensor] | None</code> <p>The similarity function used by Lime. Defaults to None.</p> <code>None</code> <code>perturb_func</code> <code>Callable[[Tensor], Tensor] | None</code> <p>The perturbation function used by Lime. Defaults to None.</p> <code>None</code> Source code in <code>src/meteors/attr/lime.py</code> <pre><code>class Lime(Explainer):\n    \"\"\"Lime class is a subclass of Explainer and represents the Lime explainer. Lime is an interpretable model-agnostic\n    explanation method that explains the predictions of a black-box model by approximating it with a simpler\n    interpretable model. The Lime method is based on the [`captum` implementation](https://captum.ai/api/lime.html)\n    and is an implementation of an idea coming from the [original paper on Lime](https://arxiv.org/abs/1602.04938),\n    where more details about this method can be found.\n\n    Args:\n        explainable_model (ExplainableModel): The explainable model to be explained.\n        interpretable_model (InterpretableModel): The interpretable model used to approximate the black-box model.\n            Defaults to `SkLearnLasso` with alpha parameter set to 0.08.\n        similarity_func (Callable[[torch.Tensor], torch.Tensor] | None, optional): The similarity function used by Lime.\n            Defaults to None.\n        perturb_func (Callable[[torch.Tensor], torch.Tensor] | None, optional): The perturbation function used by Lime.\n            Defaults to None.\n    \"\"\"\n\n    def __init__(\n        self,\n        explainable_model: ExplainableModel,\n        interpretable_model: InterpretableModel = SkLearnLasso(alpha=0.08),\n        similarity_func: Callable[[torch.Tensor], torch.Tensor] | None = None,\n        perturb_func: Callable[[torch.Tensor], torch.Tensor] | None = None,\n    ):\n        super().__init__(explainable_model)\n        self.interpretable_model = interpretable_model\n        self._attribution_method: LimeBase = self._construct_lime(\n            self.explainable_model.forward_func, interpretable_model, similarity_func, perturb_func\n        )\n\n    @staticmethod\n    def _construct_lime(\n        forward_func: Callable[[torch.Tensor], torch.Tensor],\n        interpretable_model: InterpretableModel,\n        similarity_func: Callable | None,\n        perturb_func: Callable[[torch.Tensor], torch.Tensor] | None,\n    ) -&gt; LimeBase:\n        \"\"\"Constructs the LimeBase object.\n\n        Args:\n            forward_func (Callable[[torch.Tensor], torch.Tensor]): The forward function of the explainable model.\n            interpretable_model (InterpretableModel): The interpretable model used to approximate the black-box model.\n            similarity_func (Callable | None): The similarity function used by Lime.\n            perturb_func (Callable[[torch.Tensor], torch.Tensor] | None): The perturbation function used by Lime.\n\n        Returns:\n            LimeBase: The constructed LimeBase object.\n        \"\"\"\n        return LimeBase(\n            forward_func=forward_func,\n            interpretable_model=interpretable_model,\n            similarity_func=similarity_func,\n            perturb_func=perturb_func,\n        )\n\n    @staticmethod\n    def get_segmentation_mask(\n        hsi: HSI,\n        segmentation_method: Literal[\"patch\", \"slic\"] = \"slic\",\n        **segmentation_method_params: Any,\n    ) -&gt; torch.Tensor:\n        \"\"\"Generates a segmentation mask for the given hsi using the specified segmentation method.\n\n        Args:\n            hsi (HSI): The input hyperspectral image for which the segmentation mask needs to be generated.\n            segmentation_method (Literal[\"patch\", \"slic\"], optional): The segmentation method to be used.\n                Defaults to \"slic\".\n            **segmentation_method_params (Any): Additional parameters specific to the chosen segmentation method.\n\n        Returns:\n            torch.Tensor: The segmentation mask as a tensor.\n\n        Raises:\n            TypeError: If the input hsi is not an instance of the HSI class.\n            ValueError: If an unsupported segmentation method is specified.\n\n        Examples:\n            &gt;&gt;&gt; hsi = meteors.HSI(image=torch.ones((3, 240, 240)), wavelengths=[462.08, 465.27, 468.47])\n            &gt;&gt;&gt; segmentation_mask = mt_lime.Lime.get_segmentation_mask(hsi, segmentation_method=\"slic\")\n            &gt;&gt;&gt; segmentation_mask.shape\n            torch.Size([1, 240, 240])\n            &gt;&gt;&gt; segmentation_mask = meteors.attr.Lime.get_segmentation_mask(hsi, segmentation_method=\"patch\", patch_size=2)\n            &gt;&gt;&gt; segmentation_mask.shape\n            torch.Size([1, 240, 240])\n            &gt;&gt;&gt; segmentation_mask[0, :2, :2]\n            torch.tensor([[1, 1],\n                          [1, 1]])\n            &gt;&gt;&gt; segmentation_mask[0, 2:4, :2]\n            torch.tensor([[2, 2],\n                          [2, 2]])\n        \"\"\"\n        if not isinstance(hsi, HSI):\n            raise TypeError(\"hsi should be an instance of HSI class\")\n\n        try:\n            if segmentation_method == \"slic\":\n                return Lime._get_slic_segmentation_mask(hsi, **segmentation_method_params)\n            elif segmentation_method == \"patch\":\n                return Lime._get_patch_segmentation_mask(hsi, **segmentation_method_params)\n            else:\n                raise ValueError(f\"Unsupported segmentation method: {segmentation_method}\")\n        except Exception as e:\n            raise MaskCreationError(f\"Error creating segmentation mask using method {segmentation_method}: {e}\")\n\n    @staticmethod\n    def get_band_mask(\n        hsi: HSI,\n        band_names: None | list[str | list[str]] | dict[tuple[str, ...] | str, int] = None,\n        band_indices: None | dict[str | tuple[str, ...], ListOfWavelengthsIndices] = None,\n        band_wavelengths: None | dict[str | tuple[str, ...], ListOfWavelengths] = None,\n        device: str | torch.device | None = None,\n        repeat_dimensions: bool = False,\n    ) -&gt; tuple[torch.Tensor, dict[tuple[str, ...] | str, int]]:\n        \"\"\"Generates a band mask based on the provided hsi and band information.\n\n        Remember you need to provide either band_names, band_indices, or band_wavelengths to create the band mask.\n        If you provide more than one, the band mask will be created using only one using the following priority:\n        band_names &gt; band_wavelengths &gt; band_indices.\n\n        Args:\n            hsi (HSI): The input hyperspectral image.\n            band_names (None | list[str | list[str]] | dict[tuple[str, ...] | str, int], optional):\n                The names of the spectral bands to include in the mask. Defaults to None.\n            band_indices (None | dict[str | tuple[str, ...], list[tuple[int, int]] | tuple[int, int] | list[int]], optional):\n                The indices or ranges of indices of the spectral bands to include in the mask. Defaults to None.\n            band_wavelengths (None | dict[str | tuple[str, ...], list[tuple[float, float]] | tuple[float, float], list[float], float], optional):\n                The wavelengths or ranges of wavelengths of the spectral bands to include in the mask. Defaults to None.\n            device (str | torch.device | None, optional):\n                The device to use for computation. Defaults to None.\n            repeat_dimensions (bool, optional):\n                Whether to repeat the dimensions of the mask to match the input hsi shape. Defaults to False.\n\n        Returns:\n            tuple[torch.Tensor, dict[tuple[str, ...] | str, int]]: A tuple containing the band mask tensor and a dictionary\n            mapping band names to segment IDs.\n\n        Raises:\n            TypeError: If the input hsi is not an instance of the HSI class.\n            ValueError: If no band names, indices, or wavelengths are provided.\n\n        Examples:\n            &gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((len(wavelengths), 10, 10)), wavelengths=wavelengths)\n            &gt;&gt;&gt; band_names = [\"R\", \"G\"]\n            &gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_names=band_names)\n            &gt;&gt;&gt; dict_labels_to_segment_ids\n            {\"R\": 1, \"G\": 2}\n            &gt;&gt;&gt; band_indices = {\"RGB\": [0, 1, 2]}\n            &gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_indices=band_indices)\n            &gt;&gt;&gt; dict_labels_to_segment_ids\n            {\"RGB\": 1}\n            &gt;&gt;&gt; band_wavelengths = {\"RGB\": [(462.08, 465.27), (465.27, 468.47), (468.47, 471.68)]}\n            &gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_wavelengths=band_wavelengths)\n            &gt;&gt;&gt; dict_labels_to_segment_ids\n            {\"RGB\": 1}\n        \"\"\"\n        if not isinstance(hsi, HSI):\n            raise TypeError(\"hsi should be an instance of HSI class\")\n\n        try:\n            if not (band_names is not None or band_indices is not None or band_wavelengths is not None):\n                raise ValueError(\"No band names, indices, or wavelengths are provided.\")\n\n            # validate types\n            dict_labels_to_segment_ids = None\n            if band_names is not None:\n                logger.debug(\"Getting band mask from band names of spectral bands\")\n                if band_wavelengths is not None or band_indices is not None:\n                    ignored_params = [\n                        param\n                        for param in [\"band_wavelengths\", \"band_indices\"]\n                        if param in locals() and locals()[param] is not None\n                    ]\n                    ignored_params_str = \" and \".join(ignored_params)\n                    logger.info(\n                        f\"Only the band names will be used to create the band mask. The additional parameters {ignored_params_str} will be ignored.\"\n                    )\n                try:\n                    validate_band_names(band_names)\n                    band_groups, dict_labels_to_segment_ids = Lime._get_band_wavelengths_indices_from_band_names(\n                        hsi.wavelengths, band_names\n                    )\n                except Exception as e:\n                    raise BandSelectionError(f\"Incorrect band names provided: {e}\") from e\n            elif band_wavelengths is not None:\n                logger.debug(\"Getting band mask from band groups given by ranges of wavelengths\")\n                if band_indices is not None:\n                    logger.info(\n                        \"Only the band wavelengths will be used to create the band mask. The band_indices will be ignored.\"\n                    )\n                validate_band_format(band_wavelengths, variable_name=\"band_wavelengths\")\n                try:\n                    band_groups = Lime._get_band_indices_from_band_wavelengths(\n                        hsi.wavelengths,\n                        band_wavelengths,\n                    )\n                except Exception as e:\n                    raise ValueError(\n                        f\"Incorrect band ranges wavelengths provided, please check if provided wavelengths are correct: {e}\"\n                    ) from e\n            elif band_indices is not None:\n                logger.debug(\"Getting band mask from band groups given by ranges of indices\")\n                validate_band_format(band_indices, variable_name=\"band_indices\")\n                try:\n                    band_groups = Lime._get_band_indices_from_input_band_indices(hsi.wavelengths, band_indices)\n                except Exception as e:\n                    raise ValueError(\n                        f\"Incorrect band ranges indices provided, please check if provided indices are correct: {e}\"\n                    ) from e\n\n            return Lime._create_tensor_band_mask(\n                hsi,\n                band_groups,\n                dict_labels_to_segment_ids=dict_labels_to_segment_ids,\n                device=device,\n                repeat_dimensions=repeat_dimensions,\n                return_dict_labels_to_segment_ids=True,\n            )\n        except Exception as e:\n            raise MaskCreationError(f\"Error creating band mask: {e}\") from e\n\n    @staticmethod\n    def _make_band_names_indexable(segment_name: list[str] | tuple[str, ...] | str) -&gt; tuple[str, ...] | str:\n        \"\"\"Converts a list of strings into a tuple of strings if necessary to make it indexable.\n\n        Args:\n            segment_name (list[str] | tuple[str, ...] | str): The segment name to be converted.\n\n        Returns:\n            tuple[str, ...] | str: The converted segment name.\n\n        Raises:\n            TypeError: If the segment_name is not of type list or string.\n        \"\"\"\n        if (\n            isinstance(segment_name, tuple) and all(isinstance(subitem, str) for subitem in segment_name)\n        ) or isinstance(segment_name, str):\n            return segment_name\n        elif isinstance(segment_name, list) and all(isinstance(subitem, str) for subitem in segment_name):\n            return tuple(segment_name)\n        raise TypeError(f\"Incorrect segment {segment_name} type. Should be either a list or string\")\n\n    @staticmethod\n    # @lru_cache(maxsize=32) Can't use with lists as they are not hashable\n    def _extract_bands_from_spyndex(segment_name: list[str] | tuple[str, ...] | str) -&gt; tuple[str, ...] | str:\n        \"\"\"Extracts bands from the given segment name.\n\n        Args:\n            segment_name (list[str] | tuple[str, ...] | str): The name of the segment.\n                Users may pass either band names or indices names, as in the spyndex library.\n\n        Returns:\n            tuple[str, ...] | str: A tuple of band names if multiple bands are extracted,\n                or a single band name if only one band is extracted.\n\n        Raises:\n            BandSelectionError: If the provided band name is invalid.\n                The band name must be either in `spyndex.indices` or `spyndex.bands`.\n        \"\"\"\n        if isinstance(segment_name, str):\n            segment_name = (segment_name,)\n        elif isinstance(segment_name, list):\n            segment_name = tuple(segment_name)\n\n        band_names_segment: list[str] = []\n        for band_name in segment_name:\n            if band_name in spyndex.indices:\n                band_names_segment += list(spyndex.indices[band_name].bands)\n            elif band_name in spyndex.bands:\n                band_names_segment.append(band_name)\n            else:\n                raise BandSelectionError(\n                    f\"Invalid band name {band_name}, band name must be either in `spyndex.indices` or `spyndex.bands`\"\n                )\n\n        return tuple(set(band_names_segment)) if len(band_names_segment) &gt; 1 else band_names_segment[0]\n\n    @staticmethod\n    def _get_indices_from_wavelength_indices_range(\n        wavelengths: torch.Tensor, ranges: list[tuple[int, int]] | tuple[int, int]\n    ) -&gt; list[int]:\n        \"\"\"Converts wavelength indices ranges to list indices.\n\n        Args:\n            wavelengths (torch.Tensor): The tensor containing the wavelengths.\n            ranges (list[tuple[int, int]] | tuple[int, int]): The wavelength indices ranges.\n\n        Returns:\n            list[int]: The indices of bands corresponding to the wavelength indices ranges.\n        \"\"\"\n        validated_ranges_list = validate_segment_format(ranges)\n        validated_ranges_list = adjust_and_validate_segment_ranges(wavelengths, validated_ranges_list)\n\n        return list(\n            set(\n                chain.from_iterable(\n                    [list(range(int(validated_range[0]), int(validated_range[1]))) for validated_range in ranges]  # type: ignore\n                )\n            )\n        )\n\n    @staticmethod\n    def _get_band_wavelengths_indices_from_band_names(\n        wavelengths: torch.Tensor,\n        band_names: list[str | list[str]] | dict[tuple[str, ...] | str, int],\n    ) -&gt; tuple[dict[tuple[str, ...] | str, list[int]], dict[tuple[str, ...] | str, int]]:\n        \"\"\"Extracts band wavelengths indices from the given band names.\n\n        This function takes a list or dictionary of band names or segments and extracts the list of wavelengths indices\n        associated with each segment. It returns a tuple containing a dictionary with mapping segment labels into\n        wavelength indices and a dictionary mapping segment labels into segment ids.\n\n        Args:\n            wavelengths (torch.Tensor): The tensor containing the wavelengths.\n            band_names (list[str | list[str]] | dict[tuple[str, ...] | str, int]):\n                A list or dictionary with band names or segments.\n\n        Returns:\n            tuple[dict[tuple[str, ...] | str, list[int]], dict[tuple[str, ...] | str, int]]:\n                A tuple containing the dictionary with mapping segment labels into wavelength indices and the mapping\n                from segment labels into segment ids.\n\n        Raises:\n            TypeError: If the band names are not in the correct format.\n        \"\"\"\n        if isinstance(band_names, str):\n            band_names = [band_names]\n        if isinstance(band_names, list):\n            logger.debug(\"band_names is a list of segments, creating a dictionary of segments\")\n            band_names_hashed = [Lime._make_band_names_indexable(segment) for segment in band_names]\n            dict_labels_to_segment_ids = {segment: idx + 1 for idx, segment in enumerate(band_names_hashed)}\n            segments_list = band_names_hashed\n        elif isinstance(band_names, dict):\n            dict_labels_to_segment_ids = band_names.copy()\n            segments_list = tuple(band_names.keys())  # type: ignore\n        else:\n            raise TypeError(\"Incorrect band_names type. It should be a dict or a list\")\n        segments_list_after_mapping = [Lime._extract_bands_from_spyndex(segment) for segment in segments_list]\n        band_indices: dict[tuple[str, ...] | str, list[int]] = {}\n        for original_segment, segment in zip(segments_list, segments_list_after_mapping):\n            segment_indices_ranges: list[tuple[int, int]] = []\n            if isinstance(segment, str):\n                segment = (segment,)\n            for band_name in segment:\n                min_wavelength = spyndex.bands[band_name].min_wavelength\n                max_wavelength = spyndex.bands[band_name].max_wavelength\n\n                if min_wavelength &gt; wavelengths.max() or max_wavelength &lt; wavelengths.min():\n                    logger.debug(\n                        f\"Band {band_name} is not present in the given wavelengths. \"\n                        f\"Band ranges from {min_wavelength} nm to {max_wavelength} nm and the HSI wavelengths \"\n                        f\"range from {wavelengths.min():.2f} nm to {wavelengths.max():.2f} nm. The given band will be skipped\"\n                    )\n                else:\n                    segment_indices_ranges += Lime._convert_wavelengths_to_indices(\n                        wavelengths,\n                        (spyndex.bands[band_name].min_wavelength, spyndex.bands[band_name].max_wavelength),\n                    )\n\n            segment_list = Lime._get_indices_from_wavelength_indices_range(wavelengths, segment_indices_ranges)\n            band_indices[original_segment] = segment_list\n        return band_indices, dict_labels_to_segment_ids\n\n    @staticmethod\n    def _convert_wavelengths_to_indices(\n        wavelengths: torch.Tensor, ranges: list[tuple[float, float]] | tuple[float, float]\n    ) -&gt; list[tuple[int, int]]:\n        \"\"\"Converts wavelength ranges to index ranges.\n\n        Args:\n            wavelengths (torch.Tensor): The tensor containing the wavelengths.\n            ranges (list[tuple[float, float]] | tuple[float, float]): The wavelength ranges.\n\n        Returns:\n            list[tuple[int, int]]: The index ranges corresponding to the wavelength ranges.\n        \"\"\"\n        indices = []\n        if isinstance(ranges, tuple):\n            ranges = [ranges]\n\n        for start, end in ranges:\n            start_idx = torch.searchsorted(wavelengths, start, side=\"left\")\n            end_idx = torch.searchsorted(wavelengths, end, side=\"right\")\n            indices.append((start_idx.item(), end_idx.item()))\n        return indices\n\n    @staticmethod\n    def _get_band_indices_from_band_wavelengths(\n        wavelengths: torch.Tensor,\n        band_wavelengths: dict[str | tuple[str, ...], ListOfWavelengths],\n    ) -&gt; dict[str | tuple[str, ...], list[int]]:\n        \"\"\"Converts the ranges or list of wavelengths into indices.\n\n        Args:\n            wavelengths (torch.Tensor): The tensor containing the wavelengths.\n            band_wavelengths (dict): A dictionary mapping segment labels to wavelength list or ranges.\n\n        Returns:\n            dict: A dictionary mapping segment labels to index ranges.\n\n        Raises:\n            TypeError: If band_wavelengths is not a dictionary.\n        \"\"\"\n        if not isinstance(band_wavelengths, dict):\n            raise TypeError(\"band_wavelengths should be a dictionary\")\n\n        band_indices: dict[str | tuple[str, ...], list[int]] = {}\n        for segment_label, segment in band_wavelengths.items():\n            try:\n                dtype = torch_dtype_to_python_dtype(wavelengths.dtype)\n                if isinstance(segment, (float, int)):\n                    segment = [dtype(segment)]  # type: ignore\n                if isinstance(segment, list) and all(isinstance(x, (float, int)) for x in segment):\n                    segment_dtype = change_dtype_of_list(segment, dtype)\n                    indices = Lime._convert_wavelengths_list_to_indices(wavelengths, segment_dtype)  # type: ignore\n                else:\n                    if isinstance(segment, list):\n                        segment_dtype = [\n                            tuple(change_dtype_of_list(list(ranges), dtype))  # type: ignore\n                            for ranges in segment\n                        ]\n                    else:\n                        segment_dtype = tuple(change_dtype_of_list(segment, dtype))\n\n                    valid_segment_range = validate_segment_format(segment_dtype, dtype)\n                    range_indices = Lime._convert_wavelengths_to_indices(wavelengths, valid_segment_range)  # type: ignore\n                    valid_indices_format = validate_segment_format(range_indices)\n                    valid_range_indices = adjust_and_validate_segment_ranges(wavelengths, valid_indices_format)\n                    indices = Lime._get_indices_from_wavelength_indices_range(wavelengths, valid_range_indices)\n            except Exception as e:\n                raise ValueError(f\"Problem with segment {segment_label}: {e}\") from e\n\n            band_indices[segment_label] = indices\n\n        return band_indices\n\n    @staticmethod\n    def _convert_wavelengths_list_to_indices(wavelengths: torch.Tensor, ranges: list[float]) -&gt; list[int]:\n        \"\"\"Converts a list of wavelengths into indices.\n\n        Args:\n            wavelengths (torch.Tensor): The tensor containing the wavelengths.\n            ranges (list[float]): The list of wavelengths.\n\n        Returns:\n            list[int]: The indices corresponding to the wavelengths.\n        \"\"\"\n        indices = []\n        for wavelength in ranges:\n            index = (wavelengths == wavelength).nonzero(as_tuple=False)\n            number_of_elements = torch.numel(index)\n            if number_of_elements == 1:\n                indices.append(index.item())\n            elif number_of_elements == 0:\n                raise ValueError(f\"Couldn't find wavelength of value {wavelength} in list of wavelength\")\n            else:\n                raise ValueError(f\"Wavelength of value {wavelength} was present more than once in list of wavelength\")\n        return indices\n\n    @staticmethod\n    def _get_band_indices_from_input_band_indices(\n        wavelengths: torch.Tensor,\n        input_band_indices: dict[str | tuple[str, ...], ListOfWavelengthsIndices],\n    ) -&gt; dict[str | tuple[str, ...], list[int]]:\n        \"\"\"Get band indices from band list or ranges indices.\n\n        Args:\n            wavelengths (torch.Tensor): The tensor containing the wavelengths.\n            band_indices (dict[str | tuple[str, ...], ListOfWavelengthsIndices]):\n                A dictionary mapping segment labels to a list of wavelength indices.\n\n        Returns:\n            dict[str | tuple[str, ...], list[int]]: A dictionary mapping segment labels to a list of band indices.\n\n        Raises:\n            TypeError: If `band_indices` is not a dictionary.\n        \"\"\"\n        if not isinstance(input_band_indices, dict):\n            raise TypeError(\"band_indices should be a dictionary\")\n\n        band_indices: dict[str | tuple[str, ...], list[int]] = {}\n        for segment_label, indices in input_band_indices.items():\n            try:\n                if isinstance(indices, int):\n                    indices = [indices]  # type: ignore\n                if isinstance(indices, list) and all(isinstance(x, int) for x in indices):\n                    indices: list[int] = indices  # type: ignore\n                else:\n                    valid_indices_format = validate_segment_format(indices)  # type: ignore\n                    valid_range_indices = adjust_and_validate_segment_ranges(wavelengths, valid_indices_format)\n                    indices = Lime._get_indices_from_wavelength_indices_range(wavelengths, valid_range_indices)  # type: ignore\n\n                band_indices[segment_label] = indices  # type: ignore\n            except Exception as e:\n                raise ValueError(f\"Problem with segment {segment_label}\") from e\n\n        return band_indices\n\n    @staticmethod\n    def _check_overlapping_segments(dict_labels_to_indices: dict[str | tuple[str, ...], list[int]]) -&gt; None:\n        \"\"\"Check for overlapping segments.\n\n        Args:\n            dict_labels_to_indices (dict[str | tuple[str, ...], list[int]]):\n                A dictionary mapping segment labels to indices.\n\n        Returns:\n            None\n        \"\"\"\n        overlapping_segments: list[tuple[str | tuple[str, ...], str | tuple[str, ...]]] = []\n        labels = list(dict_labels_to_indices.keys())\n\n        for i, segment_label in enumerate(labels):\n            for second_label in labels[i + 1 :]:\n                indices = dict_labels_to_indices[segment_label]\n                second_indices = dict_labels_to_indices[second_label]\n\n                if set(indices) &amp; set(second_indices):\n                    overlapping_segments.append((segment_label, second_label))\n\n        for label_first, label_second in overlapping_segments:\n            label_first_str = label_first if isinstance(label_first, str) else \"/\".join(label_first)\n            label_second_str = label_second if isinstance(label_second, str) else \"/\".join(label_second)\n\n            logger.warning(\n                f\"Segments {label_first_str} and {label_second_str} are overlapping,\"\n                \" overlapping wavelengths will be assigned to only one\"\n            )\n\n    @staticmethod\n    def _validate_and_create_dict_labels_to_segment_ids(\n        dict_labels_to_segment_ids: dict[str | tuple[str, ...], int] | None,\n        segment_labels: list[str | tuple[str, ...]],\n    ) -&gt; dict[str | tuple[str, ...], int]:\n        \"\"\"Validates and creates a dictionary mapping segment labels to segment IDs.\n\n        Args:\n            dict_labels_to_segment_ids (dict[str | tuple[str, ...], int] | None):\n                The existing mapping from segment labels to segment IDs, or None if it doesn't exist.\n            segment_labels (list[str | tuple[str, ...]]): The list of segment labels.\n\n        Returns:\n            dict[str | tuple[str, ...], int]: A tuple containing the validated dictionary mapping segment\n            labels to segment IDs and a boolean flag indicating whether the segment labels are hashed.\n\n        Raises:\n            ValueError: If the length of `dict_labels_to_segment_ids` doesn't match the length of `segment_labels`.\n            ValueError: If a segment label is not present in `dict_labels_to_segment_ids`.\n            ValueError: If there are non-unique segment IDs in `dict_labels_to_segment_ids`.\n        \"\"\"\n        if dict_labels_to_segment_ids is None:\n            logger.debug(\"Creating mapping from segment labels into ids\")\n            return {segment: idx + 1 for idx, segment in enumerate(segment_labels)}\n\n        logger.debug(\"Using existing mapping from segment labels into segment ids\")\n\n        if len(dict_labels_to_segment_ids) != len(segment_labels):\n            raise ValueError(\n                (\n                    f\"Incorrect dict_labels_to_segment_ids - length mismatch. Expected: \"\n                    f\"{len(segment_labels)}, Actual: {len(dict_labels_to_segment_ids)}\"\n                )\n            )\n\n        unique_segment_ids = set(dict_labels_to_segment_ids.values())\n        if len(unique_segment_ids) != len(segment_labels):\n            raise ValueError(\"Non unique segment ids in the dict_labels_to_segment_ids\")\n\n        logger.debug(\"Passed mapping is correct\")\n        return dict_labels_to_segment_ids\n\n    @staticmethod\n    def _create_single_dim_band_mask(\n        hsi: HSI,\n        dict_labels_to_indices: dict[str | tuple[str, ...], list[int]],\n        dict_labels_to_segment_ids: dict[str | tuple[str, ...], int],\n        device: torch.device,\n    ) -&gt; torch.Tensor:\n        \"\"\"Create a one-dimensional band mask based on the given image, labels, and segment IDs.\n\n        Args:\n            hsi (HSI): The input hsi.\n            dict_labels_to_indices (dict[str | tuple[str, ...], list[int]]):\n                A dictionary mapping labels or label tuples to lists of indices.\n            dict_labels_to_segment_ids (dict[str | tuple[str, ...], int]):\n                A dictionary mapping labels or label tuples to segment IDs.\n            device (torch.device): The device to use for the tensor.\n\n        Returns:\n            torch.Tensor: The one-dimensional band mask tensor.\n\n        Raises:\n            ValueError: If the indices for a segment are out of bounds for the one-dimensional band mask.\n        \"\"\"\n        band_mask_single_dim = torch.zeros(len(hsi.wavelengths), dtype=torch.int64, device=device)\n\n        segment_labels = list(dict_labels_to_segment_ids.keys())\n\n        for segment_label in segment_labels[::-1]:\n            segment_indices = dict_labels_to_indices[segment_label]\n            segment_id = dict_labels_to_segment_ids[segment_label]\n            are_indices_valid = all(0 &lt;= idx &lt; band_mask_single_dim.shape[0] for idx in segment_indices)\n            if not are_indices_valid:\n                raise ValueError(\n                    (\n                        f\"Indices for segment {segment_label} are out of bounds for the one-dimensional band mask\"\n                        f\"of shape {band_mask_single_dim.shape}\"\n                    )\n                )\n            band_mask_single_dim[segment_indices] = segment_id\n\n        return band_mask_single_dim\n\n    @staticmethod\n    def _create_tensor_band_mask(\n        hsi: HSI,\n        dict_labels_to_indices: dict[str | tuple[str, ...], list[int]],\n        dict_labels_to_segment_ids: dict[str | tuple[str, ...], int] | None = None,\n        device: str | torch.device | None = None,\n        repeat_dimensions: bool = False,\n        return_dict_labels_to_segment_ids: bool = True,\n    ) -&gt; torch.Tensor | tuple[torch.Tensor, dict[tuple[str, ...] | str, int]]:\n        \"\"\"Create a tensor band mask from dictionaries. The band mask is created based on the given hsi, labels, and\n        segment IDs. The band mask is a tensor with the same shape as the input hsi and contains segment IDs, where each\n        segment is represented by a unique ID. The band mask will be used to attribute the hsi using the LIME method.\n\n        Args:\n            hsi (HSI): The input hsi.\n            dict_labels_to_indices (dict[str | tuple[str, ...], list[int]]): A dictionary mapping labels to indices.\n            dict_labels_to_segment_ids (dict[str | tuple[str, ...], int] | None, optional):\n                A dictionary mapping labels to segment IDs. Defaults to None.\n            device (str | torch.device | None, optional): The device to use. Defaults to None.\n            repeat_dimensions (bool, optional): Whether to repeat dimensions. Defaults to False.\n            return_dict_labels_to_segment_ids (bool, optional):\n                Whether to return the dictionary mapping labels to segment IDs. Defaults to True.\n\n        Returns:\n            torch.Tensor | tuple[torch.Tensor, dict[tuple[str, ...] | str, int]]:\n                The tensor band mask or a tuple containing the tensor band mask\n                and the dictionary mapping labels to segment IDs.\n        \"\"\"\n        if device is None:\n            device = hsi.device\n        segment_labels = list(dict_labels_to_indices.keys())\n\n        logger.debug(f\"Creating a band mask on the device {device} using {len(segment_labels)} segments\")\n\n        # Check for overlapping segments\n        Lime._check_overlapping_segments(dict_labels_to_indices)\n\n        # Create or validate dict_labels_to_segment_ids\n        dict_labels_to_segment_ids = Lime._validate_and_create_dict_labels_to_segment_ids(\n            dict_labels_to_segment_ids, segment_labels\n        )\n\n        # Create single-dimensional band mask\n        band_mask_single_dim = Lime._create_single_dim_band_mask(\n            hsi, dict_labels_to_indices, dict_labels_to_segment_ids, device\n        )\n\n        # Expand band mask to match image dimensions\n        band_mask = expand_spectral_mask(hsi, band_mask_single_dim, repeat_dimensions)\n\n        if return_dict_labels_to_segment_ids:\n            return band_mask, dict_labels_to_segment_ids\n        return band_mask\n\n    def attribute(  # type: ignore\n        self,\n        hsi: list[HSI] | HSI,\n        target: list[int] | int | None = None,\n        attribution_type: Literal[\"spatial\", \"spectral\"] | None = None,\n        additional_forward_args: Any = None,\n        **kwargs: Any,\n    ) -&gt; HSISpatialAttributes | HSISpectralAttributes | list[HSISpatialAttributes] | list[HSISpectralAttributes]:\n        \"\"\"A wrapper function to attribute the image using the LIME method. It executes either the\n        `get_spatial_attributes` or `get_spectral_attributes` method based on the provided `attribution_type`. For more\n        detailed description of the methods, please refer to the respective method documentation.\n\n        Args:\n            hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n                If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n                The output will be a list of HSISpatialAttributes or HSISpectralAttributes objects.\n            target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n                methods assume that the output has only one class. If the output has multiple classes, the target index\n                must be provided. For multiple input images, a list of target indices can be provided, one for each\n                image or single target value will be used for all images. Defaults to None.\n            attribution_type (Literal[\"spatial\", \"spectral\"] | None, optional): The type of attribution to be computed.\n                User can compute spatial or spectral attributions with the LIME method. If None, the method will\n                throw an error. Defaults to None.\n            additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n                the inputs for which attributions should not be computed, this argument can be provided.\n                It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n                containing multiple additional arguments including tensors or any arbitrary python types.\n                These arguments are provided to forward_func in order following the arguments in inputs.\n                Note that attributions are not computed with respect to these arguments. Default: None\n            kwargs (Any): Additional keyword arguments for the LIME method.\n\n        Returns:\n            HSISpectralAttributes | HSISpatialAttributes | list[HSISpectralAttributes | HSISpatialAttributes]:\n                The computed attributions Spectral or Spatial for the input hyperspectral image(s).\n                if a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n\n        Raises:\n            RuntimeError: If the Lime object is not initialized or is not an instance of LimeBase.\n            ValueError: If number of HSI images is not equal to the number of masks provided.\n\n        Examples:\n            &gt;&gt;&gt; simple_model = lambda x: torch.rand((x.shape[0], 2))\n            &gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n            &gt;&gt;&gt; segmentation_mask = torch.randint(1, 4, (1, 240, 240))\n            &gt;&gt;&gt; lime = meteors.attr.Lime(\n                    explainable_model=ExplainableModel(simple_model, \"regression\"), interpretable_model=SkLearnLasso(alpha=0.1)\n                )\n            &gt;&gt;&gt; spatial_attribution = lime.attribute(hsi, segmentation_mask=segmentation_mask, target=0, attribution_type=\"spatial\")\n            &gt;&gt;&gt; spatial_attribution.hsi\n            HSI(shape=(4, 240, 240), dtype=torch.float32)\n            &gt;&gt;&gt; band_mask = torch.randint(1, 4, (4, 1, 1)).repeat(1, 240, 240)\n            &gt;&gt;&gt; band_names = [\"R\", \"G\", \"B\"]\n            &gt;&gt;&gt; spectral_attribution = lime.attribute(\n            ...     hsi, band_mask=band_mask, band_names=band_names, target=0, attribution_type=\"spectral\"\n            ... )\n            &gt;&gt;&gt; spectral_attribution.hsi\n            HSI(shape=(4, 240, 240), dtype=torch.float32)\n        \"\"\"\n        if attribution_type == \"spatial\":\n            return self.get_spatial_attributes(\n                hsi, target=target, additional_forward_args=additional_forward_args, **kwargs\n            )\n        elif attribution_type == \"spectral\":\n            return self.get_spectral_attributes(\n                hsi, target=target, additional_forward_args=additional_forward_args, **kwargs\n            )\n        raise ValueError(f\"Unsupported attribution type: {attribution_type}. Use 'spatial' or 'spectral'\")\n\n    def get_spatial_attributes(\n        self,\n        hsi: list[HSI] | HSI,\n        segmentation_mask: np.ndarray | torch.Tensor | list[np.ndarray | torch.Tensor] | None = None,\n        target: list[int] | int | None = None,\n        n_samples: int = 10,\n        perturbations_per_eval: int = 4,\n        verbose: bool = False,\n        segmentation_method: Literal[\"slic\", \"patch\"] = \"slic\",\n        additional_forward_args: Any = None,\n        **segmentation_method_params: Any,\n    ) -&gt; list[HSISpatialAttributes] | HSISpatialAttributes:\n        \"\"\"\n        Get spatial attributes of an hsi image using the LIME method. Based on the provided hsi and segmentation mask\n        LIME method attributes the `superpixels` provided by the segmentation mask. Please refer to the original paper\n        `https://arxiv.org/abs/1602.04938` for more details or to Christoph Molnar's book\n        `https://christophm.github.io/interpretable-ml-book/lime.html`.\n\n        This function attributes the hyperspectral image using the LIME (Local Interpretable Model-Agnostic Explanations)\n        method for spatial data. It returns an `HSISpatialAttributes` object that contains the hyperspectral image,,\n        the attributions, the segmentation mask, and the score of the interpretable model used for the explanation.\n\n        Args:\n            hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n                If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n                The output will be a list of HSISpatialAttributes objects.\n            segmentation_mask (np.ndarray | torch.Tensor | list[np.ndarray | torch.Tensor] | None, optional):\n                A segmentation mask according to which the attribution should be performed.\n                The segmentation mask should have a 2D or 3D shape, which can be broadcastable to the shape of the\n                input image. The only dimension on which the image and the mask shapes can differ is the spectral\n                dimension, marked with letter `C` in the `image.orientation` parameter. If None, a new segmentation mask\n                is created using the `segmentation_method`. Additional parameters for the segmentation method may be\n                passed as kwargs. If multiple HSI images are provided, a list of segmentation masks can be provided,\n                one for each image. If list is not provided method will assume that the same segmentation mask is used\n                    for all images. Defaults to None.\n            target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n                methods assume that the output has only one class. If the output has multiple classes, the target index\n                must be provided. For multiple input images, a list of target indices can be provided, one for each\n                image or single target value will be used for all images. Defaults to None.\n            n_samples (int, optional): The number of samples to generate/analyze in LIME. The more the better but slower.\n                Defaults to 10.\n            perturbations_per_eval (int, optional): The number of perturbations to evaluate at once\n                (Simply the inner batch size). Defaults to 4.\n            verbose (bool, optional): Whether to show the progress bar. Defaults to False.\n            segmentation_method (Literal[\"slic\", \"patch\"], optional):\n                Segmentation method used only if `segmentation_mask` is None. Defaults to \"slic\".\n            additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n                the inputs for which attributions should not be computed, this argument can be provided.\n                It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n                containing multiple additional arguments including tensors or any arbitrary python types.\n                These arguments are provided to forward_func in order following the arguments in inputs.\n                Note that attributions are not computed with respect to these arguments. Default: None\n            **segmentation_method_params (Any): Additional parameters for the segmentation method.\n\n        Returns:\n            HSISpatialAttributes | list[HSISpatialAttributes]: An object containing the image, the attributions,\n                the segmentation mask, and the score of the interpretable model used for the explanation.\n\n        Raises:\n            RuntimeError: If the Lime object is not initialized or is not an instance of LimeBase.\n            MaskCreationError: If there is an error creating the segmentation mask.\n            ValueError: If the number of segmentation masks is not equal to the number of HSI images provided.\n            HSIAttributesError: If there is an error during creating spatial attribution.\n\n        Examples:\n            &gt;&gt;&gt; simple_model = lambda x: torch.rand((x.shape[0], 2))\n            &gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n            &gt;&gt;&gt; segmentation_mask = torch.randint(1, 4, (1, 240, 240))\n            &gt;&gt;&gt; lime = meteors.attr.Lime(\n                    explainable_model=ExplainableModel(simple_model, \"regression\"), interpretable_model=SkLearnLasso(alpha=0.1)\n                )\n            &gt;&gt;&gt; spatial_attribution = lime.get_spatial_attributes(hsi, segmentation_mask=segmentation_mask, target=0)\n            &gt;&gt;&gt; spatial_attribution.hsi\n            HSI(shape=(4, 240, 240), dtype=torch.float32)\n            &gt;&gt;&gt; spatial_attribution.attributes.shape\n            torch.Size([4, 240, 240])\n            &gt;&gt;&gt; spatial_attribution.segmentation_mask.shape\n            torch.Size([1, 240, 240])\n            &gt;&gt;&gt; spatial_attribution.score\n            1.0\n        \"\"\"\n        if self._attribution_method is None or not isinstance(self._attribution_method, LimeBase):\n            raise RuntimeError(\"Lime object not initialized\")  # pragma: no cover\n\n        if isinstance(hsi, HSI):\n            hsi = [hsi]\n\n        if not all(isinstance(hsi_image, HSI) for hsi_image in hsi):\n            raise TypeError(\"All of the input hyperspectral images must be of type HSI\")\n\n        if segmentation_mask is None:\n            segmentation_mask = self.get_segmentation_mask(hsi[0], segmentation_method, **segmentation_method_params)\n\n            logger.warning(\n                \"Segmentation mask is created based on the first HSI image provided, this approach may not be optimal as \"\n                \"the same segmentation mask may not be the best suitable for all images\",\n            )\n\n        if isinstance(segmentation_mask, tuple):\n            segmentation_mask = tuple(segmentation_mask)\n        elif not isinstance(segmentation_mask, list):\n            segmentation_mask = [segmentation_mask] * len(hsi)\n\n        if len(hsi) != len(segmentation_mask):\n            raise ValueError(\n                f\"Number of segmentation masks should be equal to the number of HSI images provided, provided {len(segmentation_mask)}\"\n            )\n\n        segmentation_mask = [\n            ensure_torch_tensor(mask, f\"Segmentation mask number {idx+1} should be None, numpy array, or torch tensor\")\n            for idx, mask in enumerate(segmentation_mask)\n        ]\n        segmentation_mask = [\n            mask.unsqueeze(0).moveaxis(0, hsi_img.spectral_axis) if mask.ndim != hsi_img.image.ndim else mask\n            for hsi_img, mask in zip(hsi, segmentation_mask)\n        ]\n        segmentation_mask = [\n            validate_mask_shape(\"segmentation\", hsi_img, mask) for hsi_img, mask in zip(hsi, segmentation_mask)\n        ]\n\n        hsi_input = torch.stack([hsi_img.get_image() for hsi_img in hsi], dim=0)\n        segmentation_mask = torch.stack(segmentation_mask, dim=0)\n\n        assert segmentation_mask.shape == hsi_input.shape\n\n        segmentation_mask = segmentation_mask.to(self.device)\n        hsi_input = hsi_input.to(self.device)\n\n        lime_attributes, score = self._attribution_method.attribute(\n            inputs=hsi_input,\n            target=target,\n            feature_mask=segmentation_mask,\n            n_samples=n_samples,\n            perturbations_per_eval=perturbations_per_eval,\n            additional_forward_args=additional_forward_args,\n            show_progress=verbose,\n            return_input_shape=True,\n        )\n\n        try:\n            spatial_attribution = [\n                HSISpatialAttributes(\n                    hsi=hsi_img,\n                    attributes=lime_attr,\n                    mask=segmentation_mask[idx].expand_as(hsi_img.image),\n                    score=score.item(),\n                    attribution_method=\"Lime\",\n                )\n                for idx, (hsi_img, lime_attr) in enumerate(zip(hsi, lime_attributes))\n            ]\n        except Exception as e:\n            raise HSIAttributesError(f\"Error during creating spatial attribution {e}\") from e\n\n        return spatial_attribution[0] if len(spatial_attribution) == 1 else spatial_attribution\n\n    def get_spectral_attributes(\n        self,\n        hsi: list[HSI] | HSI,\n        band_mask: np.ndarray | torch.Tensor | list[np.ndarray | torch.Tensor] | None = None,\n        target: list[int] | int | None = None,\n        n_samples: int = 10,\n        perturbations_per_eval: int = 4,\n        verbose: bool = False,\n        additional_forward_args: Any = None,\n        band_names: list[str | list[str]] | dict[tuple[str, ...] | str, int] | None = None,\n    ) -&gt; HSISpectralAttributes | list[HSISpectralAttributes]:\n        \"\"\"\n        Attributes the hsi image using LIME method for spectral data. Based on the provided hsi and band mask, the LIME\n        method attributes the hsi based on `superbands` (clustered bands) provided by the band mask.\n        Please refer to the original paper `https://arxiv.org/abs/1602.04938` for more details or to\n        Christoph Molnar's book `https://christophm.github.io/interpretable-ml-book/lime.html`.\n\n        The function returns a HSISpectralAttributes object that contains the image, the attributions, the band mask,\n        the band names, and the score of the interpretable model used for the explanation.\n\n        Args:\n            hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n                If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n                The output will be a list of HSISpatialAttributes objects.\n            band_mask (np.ndarray | torch.Tensor | list[np.ndarray | torch.Tensor] | None, optional): Band mask that\n                is used for the spectral attribution. The band mask should have a 1D or 3D shape, which can be\n                broadcastable to the shape of the input image. The only dimensions on which the image and the mask shapes\n                can differ is the height and width dimensions, marked with letters `H` and `W` in the `image.orientation`\n                parameter. If equals to None, the band mask is created within the function. If multiple HSI images are\n                provided, a list of band masks can be provided, one for each image. If list is not provided method will\n                assume that the same band mask is used for all images. Defaults to None.\n            target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n                methods assume that the output has only one class. If the output has multiple classes, the target index\n                must be provided. For multiple input images, a list of target indices can be provided, one for each\n                image or single target value will be used for all images. Defaults to None.\n            n_samples (int, optional): The number of samples to generate/analyze in LIME. The more the better but slower.\n                Defaults to 10.\n            perturbations_per_eval (int, optional): The number of perturbations to evaluate at once\n                (Simply the inner batch size). Defaults to 4.\n            verbose (bool, optional): Whether to show the progress bar. Defaults to False.\n            segmentation_method (Literal[\"slic\", \"patch\"], optional):\n                Segmentation method used only if `segmentation_mask` is None. Defaults to \"slic\".\n            additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n                the inputs for which attributions should not be computed, this argument can be provided.\n                It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n                containing multiple additional arguments including tensors or any arbitrary python types.\n                These arguments are provided to forward_func in order following the arguments in inputs.\n                Note that attributions are not computed with respect to these arguments. Default: None\n            band_names (list[str] | dict[str | tuple[str, ...], int] | None, optional): Band names. Defaults to None.\n\n        Returns:\n            HSISpectralAttributes | list[HSISpectralAttributes]: An object containing the image, the attributions,\n                the band mask, the band names, and the score of the interpretable model used for the explanation.\n\n        Raises:\n            RuntimeError: If the Lime object is not initialized or is not an instance of LimeBase.\n            MaskCreationError: If there is an error creating the band mask.\n            ValueError: If the number of band masks is not equal to the number of HSI images provided.\n            HSIAttributesError: If there is an error during creating spectral attribution.\n\n        Examples:\n            &gt;&gt;&gt; simple_model = lambda x: torch.rand((x.shape[0], 2))\n            &gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n            &gt;&gt;&gt; band_mask = torch.randint(1, 4, (4, 1, 1)).repeat(1, 240, 240)\n            &gt;&gt;&gt; band_names = [\"R\", \"G\", \"B\"]\n            &gt;&gt;&gt; lime = meteors.attr.Lime(\n                    explainable_model=ExplainableModel(simple_model, \"regression\"), interpretable_model=SkLearnLasso(alpha=0.1)\n                )\n            &gt;&gt;&gt; spectral_attribution = lime.get_spectral_attributes(hsi, band_mask=band_mask, band_names=band_names, target=0)\n            &gt;&gt;&gt; spectral_attribution.hsi\n            HSI(shape=(4, 240, 240), dtype=torch.float32)\n            &gt;&gt;&gt; spectral_attribution.attributes.shape\n            torch.Size([4, 240, 240])\n            &gt;&gt;&gt; spectral_attribution.band_mask.shape\n            torch.Size([4, 240, 240])\n            &gt;&gt;&gt; spectral_attribution.band_names\n            [\"R\", \"G\", \"B\"]\n            &gt;&gt;&gt; spectral_attribution.score\n            1.0\n        \"\"\"\n\n        if self._attribution_method is None or not isinstance(self._attribution_method, LimeBase):\n            raise RuntimeError(\"Lime object not initialized\")  # pragma: no cover\n\n        if isinstance(hsi, HSI):\n            hsi = [hsi]\n\n        if not all(isinstance(hsi_image, HSI) for hsi_image in hsi):\n            raise TypeError(\"All of the input hyperspectral images must be of type HSI\")\n\n        if band_mask is None:\n            created_bands = [self.get_band_mask(hsi_img, band_names) for hsi_img in hsi]\n            band_mask, band_name_list = zip(*created_bands)\n            band_names = band_name_list[0]\n\n        if isinstance(band_mask, tuple):\n            band_mask = list(band_mask)\n        elif not isinstance(band_mask, list):\n            band_mask = [band_mask]\n\n        if len(hsi) != len(band_mask):\n            if len(band_mask) == 1:\n                band_mask = band_mask * len(hsi)\n                logger.debug(\"Reusing the same band mask for all images\")\n            else:\n                raise ValueError(\n                    f\"Number of band masks should be equal to the number of HSI images provided, provided {len(band_mask)}\"\n                )\n\n        band_mask = [\n            ensure_torch_tensor(mask, f\"Band mask number {idx+1} should be None, numpy array, or torch tensor\")\n            for idx, mask in enumerate(band_mask)\n        ]\n        band_mask = [\n            mask.unsqueeze(-1).unsqueeze(-1).moveaxis(0, hsi_img.spectral_axis)\n            if mask.ndim != hsi_img.image.ndim\n            else mask\n            for hsi_img, mask in zip(hsi, band_mask)\n        ]\n        band_mask = [validate_mask_shape(\"band\", hsi_img, mask) for hsi_img, mask in zip(hsi, band_mask)]\n\n        hsi_input = torch.stack([hsi_img.get_image() for hsi_img in hsi], dim=0)\n        band_mask = torch.stack(band_mask, dim=0)\n\n        if band_names is None:\n            band_names = {str(segment): idx for idx, segment in enumerate(torch.unique(band_mask))}\n        else:\n            logger.debug(\n                \"Band names are provided and will be used. In the future, there should be an option to validate them.\"\n            )\n\n        assert hsi_input.shape == band_mask.shape\n\n        hsi_input = hsi_input.to(self.device)\n        band_mask = band_mask.to(self.device)\n\n        lime_attributes, score = self._attribution_method.attribute(\n            inputs=hsi_input,\n            target=target,\n            feature_mask=band_mask,\n            n_samples=n_samples,\n            perturbations_per_eval=perturbations_per_eval,\n            additional_forward_args=additional_forward_args,\n            show_progress=verbose,\n            return_input_shape=True,\n        )\n\n        try:\n            spectral_attribution = [\n                HSISpectralAttributes(\n                    hsi=hsi_img,\n                    attributes=lime_attr,\n                    mask=band_mask[idx].expand_as(hsi_img.image),\n                    band_names=band_names,\n                    score=score.item(),\n                    attribution_method=\"Lime\",\n                )\n                for idx, (hsi_img, lime_attr) in enumerate(zip(hsi, lime_attributes))\n            ]\n        except Exception as e:\n            raise HSIAttributesError(f\"Error during creating spectral attribution {e}\") from e\n\n        return spectral_attribution[0] if len(spectral_attribution) == 1 else spectral_attribution\n\n    @staticmethod\n    def _get_slic_segmentation_mask(\n        hsi: HSI, num_interpret_features: int = 10, *args: Any, **kwargs: Any\n    ) -&gt; torch.Tensor:\n        \"\"\"Creates a segmentation mask using the SLIC method.\n\n        Args:\n            hsi (HSI): An HSI object for which the segmentation mask is created.\n            num_interpret_features (int, optional): Number of segments. Defaults to 10.\n            *args: Additional positional arguments to be passed to the SLIC method.\n            **kwargs: Additional keyword arguments to be passed to the SLIC method.\n\n        Returns:\n            torch.Tensor: An output segmentation mask.\n        \"\"\"\n        segmentation_mask = slic(\n            hsi.get_image().cpu().detach().numpy(),\n            n_segments=num_interpret_features,\n            mask=hsi.spatial_binary_mask.cpu().detach().numpy(),\n            channel_axis=hsi.spectral_axis,\n            *args,\n            **kwargs,\n        )\n\n        if segmentation_mask.min() == 1:\n            segmentation_mask -= 1\n\n        segmentation_mask = torch.from_numpy(segmentation_mask)\n        segmentation_mask = segmentation_mask.unsqueeze(dim=hsi.spectral_axis)\n\n        return segmentation_mask\n\n    @staticmethod\n    def _get_patch_segmentation_mask(hsi: HSI, patch_size: int | float = 10, *args: Any, **kwargs: Any) -&gt; torch.Tensor:\n        \"\"\"\n        Creates a segmentation mask using the patch method - creates small squares of the same size\n            and assigns a unique value to each square.\n\n        Args:\n            hsi (HSI): An HSI object for which the segmentation mask is created.\n            patch_size (int, optional): Size of the patch, the hsi size should be divisible by this value.\n                Defaults to 10.\n\n        Returns:\n            torch.Tensor: An output segmentation mask.\n        \"\"\"\n        if patch_size &lt; 1 or not isinstance(patch_size, (int, float)):\n            raise ValueError(\"Invalid patch_size. patch_size must be a positive integer\")\n\n        if hsi.image.shape[1] % patch_size != 0 or hsi.image.shape[2] % patch_size != 0:\n            raise ValueError(\"Invalid patch_size. patch_size must be a factor of both width and height of the hsi\")\n\n        height, width = hsi.image.shape[1], hsi.image.shape[2]\n\n        idx_mask = torch.arange(height // patch_size * width // patch_size, device=hsi.device).reshape(\n            height // patch_size, width // patch_size\n        )\n        idx_mask += 1\n        segmentation_mask = torch.repeat_interleave(idx_mask, patch_size, dim=0)\n        segmentation_mask = torch.repeat_interleave(segmentation_mask, patch_size, dim=1)\n        segmentation_mask = segmentation_mask * hsi.spatial_binary_mask\n        # segmentation_mask = torch.repeat_interleave(\n        # torch.unsqueeze(segmentation_mask, dim=hsi.spectral_axis),\n        # repeats=hsi.image.shape[hsi.spectral_axis], dim=hsi.spectral_axis)\n        segmentation_mask = segmentation_mask.unsqueeze(dim=hsi.spectral_axis)\n\n        mask_idx = np.unique(segmentation_mask).tolist()\n        for idx, mask_val in enumerate(mask_idx):\n            segmentation_mask[segmentation_mask == mask_val] = idx\n\n        return segmentation_mask\n</code></pre>"},{"location":"reference/#src.meteors.attr.lime.Lime.attribute","title":"<code>attribute(hsi, target=None, attribution_type=None, additional_forward_args=None, **kwargs)</code>","text":"<p>A wrapper function to attribute the image using the LIME method. It executes either the <code>get_spatial_attributes</code> or <code>get_spectral_attributes</code> method based on the provided <code>attribution_type</code>. For more detailed description of the methods, please refer to the respective method documentation.</p> <p>Parameters:</p> Name Type Description Default <code>hsi</code> <code>list[HSI] | HSI</code> <p>Input hyperspectral image(s) for which the attributions are to be computed. If a list of HSI objects is provided, the attributions are computed for each HSI object in the list. The output will be a list of HSISpatialAttributes or HSISpectralAttributes objects.</p> required <code>target</code> <code>list[int] | int | None</code> <p>target class index for computing the attributions. If None, methods assume that the output has only one class. If the output has multiple classes, the target index must be provided. For multiple input images, a list of target indices can be provided, one for each image or single target value will be used for all images. Defaults to None.</p> <code>None</code> <code>attribution_type</code> <code>Literal['spatial', 'spectral'] | None</code> <p>The type of attribution to be computed. User can compute spatial or spectral attributions with the LIME method. If None, the method will throw an error. Defaults to None.</p> <code>None</code> <code>additional_forward_args</code> <code>Any</code> <p>If the forward function requires additional arguments other than the inputs for which attributions should not be computed, this argument can be provided. It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple containing multiple additional arguments including tensors or any arbitrary python types. These arguments are provided to forward_func in order following the arguments in inputs. Note that attributions are not computed with respect to these arguments. Default: None</p> <code>None</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments for the LIME method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>HSISpatialAttributes | HSISpectralAttributes | list[HSISpatialAttributes] | list[HSISpectralAttributes]</code> <p>HSISpectralAttributes | HSISpatialAttributes | list[HSISpectralAttributes | HSISpatialAttributes]: The computed attributions Spectral or Spatial for the input hyperspectral image(s). if a list of HSI objects is provided, the attributions are computed for each HSI object in the list.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the Lime object is not initialized or is not an instance of LimeBase.</p> <code>ValueError</code> <p>If number of HSI images is not equal to the number of masks provided.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; simple_model = lambda x: torch.rand((x.shape[0], 2))\n&gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n&gt;&gt;&gt; segmentation_mask = torch.randint(1, 4, (1, 240, 240))\n&gt;&gt;&gt; lime = meteors.attr.Lime(\n        explainable_model=ExplainableModel(simple_model, \"regression\"), interpretable_model=SkLearnLasso(alpha=0.1)\n    )\n&gt;&gt;&gt; spatial_attribution = lime.attribute(hsi, segmentation_mask=segmentation_mask, target=0, attribution_type=\"spatial\")\n&gt;&gt;&gt; spatial_attribution.hsi\nHSI(shape=(4, 240, 240), dtype=torch.float32)\n&gt;&gt;&gt; band_mask = torch.randint(1, 4, (4, 1, 1)).repeat(1, 240, 240)\n&gt;&gt;&gt; band_names = [\"R\", \"G\", \"B\"]\n&gt;&gt;&gt; spectral_attribution = lime.attribute(\n...     hsi, band_mask=band_mask, band_names=band_names, target=0, attribution_type=\"spectral\"\n... )\n&gt;&gt;&gt; spectral_attribution.hsi\nHSI(shape=(4, 240, 240), dtype=torch.float32)\n</code></pre> Source code in <code>src/meteors/attr/lime.py</code> <pre><code>def attribute(  # type: ignore\n    self,\n    hsi: list[HSI] | HSI,\n    target: list[int] | int | None = None,\n    attribution_type: Literal[\"spatial\", \"spectral\"] | None = None,\n    additional_forward_args: Any = None,\n    **kwargs: Any,\n) -&gt; HSISpatialAttributes | HSISpectralAttributes | list[HSISpatialAttributes] | list[HSISpectralAttributes]:\n    \"\"\"A wrapper function to attribute the image using the LIME method. It executes either the\n    `get_spatial_attributes` or `get_spectral_attributes` method based on the provided `attribution_type`. For more\n    detailed description of the methods, please refer to the respective method documentation.\n\n    Args:\n        hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n            If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n            The output will be a list of HSISpatialAttributes or HSISpectralAttributes objects.\n        target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n            methods assume that the output has only one class. If the output has multiple classes, the target index\n            must be provided. For multiple input images, a list of target indices can be provided, one for each\n            image or single target value will be used for all images. Defaults to None.\n        attribution_type (Literal[\"spatial\", \"spectral\"] | None, optional): The type of attribution to be computed.\n            User can compute spatial or spectral attributions with the LIME method. If None, the method will\n            throw an error. Defaults to None.\n        additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n            the inputs for which attributions should not be computed, this argument can be provided.\n            It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n            containing multiple additional arguments including tensors or any arbitrary python types.\n            These arguments are provided to forward_func in order following the arguments in inputs.\n            Note that attributions are not computed with respect to these arguments. Default: None\n        kwargs (Any): Additional keyword arguments for the LIME method.\n\n    Returns:\n        HSISpectralAttributes | HSISpatialAttributes | list[HSISpectralAttributes | HSISpatialAttributes]:\n            The computed attributions Spectral or Spatial for the input hyperspectral image(s).\n            if a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n\n    Raises:\n        RuntimeError: If the Lime object is not initialized or is not an instance of LimeBase.\n        ValueError: If number of HSI images is not equal to the number of masks provided.\n\n    Examples:\n        &gt;&gt;&gt; simple_model = lambda x: torch.rand((x.shape[0], 2))\n        &gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n        &gt;&gt;&gt; segmentation_mask = torch.randint(1, 4, (1, 240, 240))\n        &gt;&gt;&gt; lime = meteors.attr.Lime(\n                explainable_model=ExplainableModel(simple_model, \"regression\"), interpretable_model=SkLearnLasso(alpha=0.1)\n            )\n        &gt;&gt;&gt; spatial_attribution = lime.attribute(hsi, segmentation_mask=segmentation_mask, target=0, attribution_type=\"spatial\")\n        &gt;&gt;&gt; spatial_attribution.hsi\n        HSI(shape=(4, 240, 240), dtype=torch.float32)\n        &gt;&gt;&gt; band_mask = torch.randint(1, 4, (4, 1, 1)).repeat(1, 240, 240)\n        &gt;&gt;&gt; band_names = [\"R\", \"G\", \"B\"]\n        &gt;&gt;&gt; spectral_attribution = lime.attribute(\n        ...     hsi, band_mask=band_mask, band_names=band_names, target=0, attribution_type=\"spectral\"\n        ... )\n        &gt;&gt;&gt; spectral_attribution.hsi\n        HSI(shape=(4, 240, 240), dtype=torch.float32)\n    \"\"\"\n    if attribution_type == \"spatial\":\n        return self.get_spatial_attributes(\n            hsi, target=target, additional_forward_args=additional_forward_args, **kwargs\n        )\n    elif attribution_type == \"spectral\":\n        return self.get_spectral_attributes(\n            hsi, target=target, additional_forward_args=additional_forward_args, **kwargs\n        )\n    raise ValueError(f\"Unsupported attribution type: {attribution_type}. Use 'spatial' or 'spectral'\")\n</code></pre>"},{"location":"reference/#src.meteors.attr.lime.Lime.get_band_mask","title":"<code>get_band_mask(hsi, band_names=None, band_indices=None, band_wavelengths=None, device=None, repeat_dimensions=False)</code>  <code>staticmethod</code>","text":"<p>Generates a band mask based on the provided hsi and band information.</p> <p>Remember you need to provide either band_names, band_indices, or band_wavelengths to create the band mask. If you provide more than one, the band mask will be created using only one using the following priority: band_names &gt; band_wavelengths &gt; band_indices.</p> <p>Parameters:</p> Name Type Description Default <code>hsi</code> <code>HSI</code> <p>The input hyperspectral image.</p> required <code>band_names</code> <code>None | list[str | list[str]] | dict[tuple[str, ...] | str, int]</code> <p>The names of the spectral bands to include in the mask. Defaults to None.</p> <code>None</code> <code>band_indices</code> <code>None | dict[str | tuple[str, ...], list[tuple[int, int]] | tuple[int, int] | list[int]]</code> <p>The indices or ranges of indices of the spectral bands to include in the mask. Defaults to None.</p> <code>None</code> <code>band_wavelengths</code> <code>None | dict[str | tuple[str, ...], list[tuple[float, float]] | tuple[float, float], list[float], float]</code> <p>The wavelengths or ranges of wavelengths of the spectral bands to include in the mask. Defaults to None.</p> <code>None</code> <code>device</code> <code>str | device | None</code> <p>The device to use for computation. Defaults to None.</p> <code>None</code> <code>repeat_dimensions</code> <code>bool</code> <p>Whether to repeat the dimensions of the mask to match the input hsi shape. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>tuple[torch.Tensor, dict[tuple[str, ...] | str, int]]: A tuple containing the band mask tensor and a dictionary</p> <code>dict[tuple[str, ...] | str, int]</code> <p>mapping band names to segment IDs.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input hsi is not an instance of the HSI class.</p> <code>ValueError</code> <p>If no band names, indices, or wavelengths are provided.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((len(wavelengths), 10, 10)), wavelengths=wavelengths)\n&gt;&gt;&gt; band_names = [\"R\", \"G\"]\n&gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_names=band_names)\n&gt;&gt;&gt; dict_labels_to_segment_ids\n{\"R\": 1, \"G\": 2}\n&gt;&gt;&gt; band_indices = {\"RGB\": [0, 1, 2]}\n&gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_indices=band_indices)\n&gt;&gt;&gt; dict_labels_to_segment_ids\n{\"RGB\": 1}\n&gt;&gt;&gt; band_wavelengths = {\"RGB\": [(462.08, 465.27), (465.27, 468.47), (468.47, 471.68)]}\n&gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_wavelengths=band_wavelengths)\n&gt;&gt;&gt; dict_labels_to_segment_ids\n{\"RGB\": 1}\n</code></pre> Source code in <code>src/meteors/attr/lime.py</code> <pre><code>@staticmethod\ndef get_band_mask(\n    hsi: HSI,\n    band_names: None | list[str | list[str]] | dict[tuple[str, ...] | str, int] = None,\n    band_indices: None | dict[str | tuple[str, ...], ListOfWavelengthsIndices] = None,\n    band_wavelengths: None | dict[str | tuple[str, ...], ListOfWavelengths] = None,\n    device: str | torch.device | None = None,\n    repeat_dimensions: bool = False,\n) -&gt; tuple[torch.Tensor, dict[tuple[str, ...] | str, int]]:\n    \"\"\"Generates a band mask based on the provided hsi and band information.\n\n    Remember you need to provide either band_names, band_indices, or band_wavelengths to create the band mask.\n    If you provide more than one, the band mask will be created using only one using the following priority:\n    band_names &gt; band_wavelengths &gt; band_indices.\n\n    Args:\n        hsi (HSI): The input hyperspectral image.\n        band_names (None | list[str | list[str]] | dict[tuple[str, ...] | str, int], optional):\n            The names of the spectral bands to include in the mask. Defaults to None.\n        band_indices (None | dict[str | tuple[str, ...], list[tuple[int, int]] | tuple[int, int] | list[int]], optional):\n            The indices or ranges of indices of the spectral bands to include in the mask. Defaults to None.\n        band_wavelengths (None | dict[str | tuple[str, ...], list[tuple[float, float]] | tuple[float, float], list[float], float], optional):\n            The wavelengths or ranges of wavelengths of the spectral bands to include in the mask. Defaults to None.\n        device (str | torch.device | None, optional):\n            The device to use for computation. Defaults to None.\n        repeat_dimensions (bool, optional):\n            Whether to repeat the dimensions of the mask to match the input hsi shape. Defaults to False.\n\n    Returns:\n        tuple[torch.Tensor, dict[tuple[str, ...] | str, int]]: A tuple containing the band mask tensor and a dictionary\n        mapping band names to segment IDs.\n\n    Raises:\n        TypeError: If the input hsi is not an instance of the HSI class.\n        ValueError: If no band names, indices, or wavelengths are provided.\n\n    Examples:\n        &gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((len(wavelengths), 10, 10)), wavelengths=wavelengths)\n        &gt;&gt;&gt; band_names = [\"R\", \"G\"]\n        &gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_names=band_names)\n        &gt;&gt;&gt; dict_labels_to_segment_ids\n        {\"R\": 1, \"G\": 2}\n        &gt;&gt;&gt; band_indices = {\"RGB\": [0, 1, 2]}\n        &gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_indices=band_indices)\n        &gt;&gt;&gt; dict_labels_to_segment_ids\n        {\"RGB\": 1}\n        &gt;&gt;&gt; band_wavelengths = {\"RGB\": [(462.08, 465.27), (465.27, 468.47), (468.47, 471.68)]}\n        &gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_wavelengths=band_wavelengths)\n        &gt;&gt;&gt; dict_labels_to_segment_ids\n        {\"RGB\": 1}\n    \"\"\"\n    if not isinstance(hsi, HSI):\n        raise TypeError(\"hsi should be an instance of HSI class\")\n\n    try:\n        if not (band_names is not None or band_indices is not None or band_wavelengths is not None):\n            raise ValueError(\"No band names, indices, or wavelengths are provided.\")\n\n        # validate types\n        dict_labels_to_segment_ids = None\n        if band_names is not None:\n            logger.debug(\"Getting band mask from band names of spectral bands\")\n            if band_wavelengths is not None or band_indices is not None:\n                ignored_params = [\n                    param\n                    for param in [\"band_wavelengths\", \"band_indices\"]\n                    if param in locals() and locals()[param] is not None\n                ]\n                ignored_params_str = \" and \".join(ignored_params)\n                logger.info(\n                    f\"Only the band names will be used to create the band mask. The additional parameters {ignored_params_str} will be ignored.\"\n                )\n            try:\n                validate_band_names(band_names)\n                band_groups, dict_labels_to_segment_ids = Lime._get_band_wavelengths_indices_from_band_names(\n                    hsi.wavelengths, band_names\n                )\n            except Exception as e:\n                raise BandSelectionError(f\"Incorrect band names provided: {e}\") from e\n        elif band_wavelengths is not None:\n            logger.debug(\"Getting band mask from band groups given by ranges of wavelengths\")\n            if band_indices is not None:\n                logger.info(\n                    \"Only the band wavelengths will be used to create the band mask. The band_indices will be ignored.\"\n                )\n            validate_band_format(band_wavelengths, variable_name=\"band_wavelengths\")\n            try:\n                band_groups = Lime._get_band_indices_from_band_wavelengths(\n                    hsi.wavelengths,\n                    band_wavelengths,\n                )\n            except Exception as e:\n                raise ValueError(\n                    f\"Incorrect band ranges wavelengths provided, please check if provided wavelengths are correct: {e}\"\n                ) from e\n        elif band_indices is not None:\n            logger.debug(\"Getting band mask from band groups given by ranges of indices\")\n            validate_band_format(band_indices, variable_name=\"band_indices\")\n            try:\n                band_groups = Lime._get_band_indices_from_input_band_indices(hsi.wavelengths, band_indices)\n            except Exception as e:\n                raise ValueError(\n                    f\"Incorrect band ranges indices provided, please check if provided indices are correct: {e}\"\n                ) from e\n\n        return Lime._create_tensor_band_mask(\n            hsi,\n            band_groups,\n            dict_labels_to_segment_ids=dict_labels_to_segment_ids,\n            device=device,\n            repeat_dimensions=repeat_dimensions,\n            return_dict_labels_to_segment_ids=True,\n        )\n    except Exception as e:\n        raise MaskCreationError(f\"Error creating band mask: {e}\") from e\n</code></pre>"},{"location":"reference/#src.meteors.attr.lime.Lime.get_segmentation_mask","title":"<code>get_segmentation_mask(hsi, segmentation_method='slic', **segmentation_method_params)</code>  <code>staticmethod</code>","text":"<p>Generates a segmentation mask for the given hsi using the specified segmentation method.</p> <p>Parameters:</p> Name Type Description Default <code>hsi</code> <code>HSI</code> <p>The input hyperspectral image for which the segmentation mask needs to be generated.</p> required <code>segmentation_method</code> <code>Literal['patch', 'slic']</code> <p>The segmentation method to be used. Defaults to \"slic\".</p> <code>'slic'</code> <code>**segmentation_method_params</code> <code>Any</code> <p>Additional parameters specific to the chosen segmentation method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The segmentation mask as a tensor.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input hsi is not an instance of the HSI class.</p> <code>ValueError</code> <p>If an unsupported segmentation method is specified.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hsi = meteors.HSI(image=torch.ones((3, 240, 240)), wavelengths=[462.08, 465.27, 468.47])\n&gt;&gt;&gt; segmentation_mask = mt_lime.Lime.get_segmentation_mask(hsi, segmentation_method=\"slic\")\n&gt;&gt;&gt; segmentation_mask.shape\ntorch.Size([1, 240, 240])\n&gt;&gt;&gt; segmentation_mask = meteors.attr.Lime.get_segmentation_mask(hsi, segmentation_method=\"patch\", patch_size=2)\n&gt;&gt;&gt; segmentation_mask.shape\ntorch.Size([1, 240, 240])\n&gt;&gt;&gt; segmentation_mask[0, :2, :2]\ntorch.tensor([[1, 1],\n              [1, 1]])\n&gt;&gt;&gt; segmentation_mask[0, 2:4, :2]\ntorch.tensor([[2, 2],\n              [2, 2]])\n</code></pre> Source code in <code>src/meteors/attr/lime.py</code> <pre><code>@staticmethod\ndef get_segmentation_mask(\n    hsi: HSI,\n    segmentation_method: Literal[\"patch\", \"slic\"] = \"slic\",\n    **segmentation_method_params: Any,\n) -&gt; torch.Tensor:\n    \"\"\"Generates a segmentation mask for the given hsi using the specified segmentation method.\n\n    Args:\n        hsi (HSI): The input hyperspectral image for which the segmentation mask needs to be generated.\n        segmentation_method (Literal[\"patch\", \"slic\"], optional): The segmentation method to be used.\n            Defaults to \"slic\".\n        **segmentation_method_params (Any): Additional parameters specific to the chosen segmentation method.\n\n    Returns:\n        torch.Tensor: The segmentation mask as a tensor.\n\n    Raises:\n        TypeError: If the input hsi is not an instance of the HSI class.\n        ValueError: If an unsupported segmentation method is specified.\n\n    Examples:\n        &gt;&gt;&gt; hsi = meteors.HSI(image=torch.ones((3, 240, 240)), wavelengths=[462.08, 465.27, 468.47])\n        &gt;&gt;&gt; segmentation_mask = mt_lime.Lime.get_segmentation_mask(hsi, segmentation_method=\"slic\")\n        &gt;&gt;&gt; segmentation_mask.shape\n        torch.Size([1, 240, 240])\n        &gt;&gt;&gt; segmentation_mask = meteors.attr.Lime.get_segmentation_mask(hsi, segmentation_method=\"patch\", patch_size=2)\n        &gt;&gt;&gt; segmentation_mask.shape\n        torch.Size([1, 240, 240])\n        &gt;&gt;&gt; segmentation_mask[0, :2, :2]\n        torch.tensor([[1, 1],\n                      [1, 1]])\n        &gt;&gt;&gt; segmentation_mask[0, 2:4, :2]\n        torch.tensor([[2, 2],\n                      [2, 2]])\n    \"\"\"\n    if not isinstance(hsi, HSI):\n        raise TypeError(\"hsi should be an instance of HSI class\")\n\n    try:\n        if segmentation_method == \"slic\":\n            return Lime._get_slic_segmentation_mask(hsi, **segmentation_method_params)\n        elif segmentation_method == \"patch\":\n            return Lime._get_patch_segmentation_mask(hsi, **segmentation_method_params)\n        else:\n            raise ValueError(f\"Unsupported segmentation method: {segmentation_method}\")\n    except Exception as e:\n        raise MaskCreationError(f\"Error creating segmentation mask using method {segmentation_method}: {e}\")\n</code></pre>"},{"location":"reference/#src.meteors.attr.lime.Lime.get_spatial_attributes","title":"<code>get_spatial_attributes(hsi, segmentation_mask=None, target=None, n_samples=10, perturbations_per_eval=4, verbose=False, segmentation_method='slic', additional_forward_args=None, **segmentation_method_params)</code>","text":"<p>Get spatial attributes of an hsi image using the LIME method. Based on the provided hsi and segmentation mask LIME method attributes the <code>superpixels</code> provided by the segmentation mask. Please refer to the original paper <code>https://arxiv.org/abs/1602.04938</code> for more details or to Christoph Molnar's book <code>https://christophm.github.io/interpretable-ml-book/lime.html</code>.</p> <p>This function attributes the hyperspectral image using the LIME (Local Interpretable Model-Agnostic Explanations) method for spatial data. It returns an <code>HSISpatialAttributes</code> object that contains the hyperspectral image,, the attributions, the segmentation mask, and the score of the interpretable model used for the explanation.</p> <p>Parameters:</p> Name Type Description Default <code>hsi</code> <code>list[HSI] | HSI</code> <p>Input hyperspectral image(s) for which the attributions are to be computed. If a list of HSI objects is provided, the attributions are computed for each HSI object in the list. The output will be a list of HSISpatialAttributes objects.</p> required <code>segmentation_mask</code> <code>ndarray | Tensor | list[ndarray | Tensor] | None</code> <p>A segmentation mask according to which the attribution should be performed. The segmentation mask should have a 2D or 3D shape, which can be broadcastable to the shape of the input image. The only dimension on which the image and the mask shapes can differ is the spectral dimension, marked with letter <code>C</code> in the <code>image.orientation</code> parameter. If None, a new segmentation mask is created using the <code>segmentation_method</code>. Additional parameters for the segmentation method may be passed as kwargs. If multiple HSI images are provided, a list of segmentation masks can be provided, one for each image. If list is not provided method will assume that the same segmentation mask is used     for all images. Defaults to None.</p> <code>None</code> <code>target</code> <code>list[int] | int | None</code> <p>target class index for computing the attributions. If None, methods assume that the output has only one class. If the output has multiple classes, the target index must be provided. For multiple input images, a list of target indices can be provided, one for each image or single target value will be used for all images. Defaults to None.</p> <code>None</code> <code>n_samples</code> <code>int</code> <p>The number of samples to generate/analyze in LIME. The more the better but slower. Defaults to 10.</p> <code>10</code> <code>perturbations_per_eval</code> <code>int</code> <p>The number of perturbations to evaluate at once (Simply the inner batch size). Defaults to 4.</p> <code>4</code> <code>verbose</code> <code>bool</code> <p>Whether to show the progress bar. Defaults to False.</p> <code>False</code> <code>segmentation_method</code> <code>Literal['slic', 'patch']</code> <p>Segmentation method used only if <code>segmentation_mask</code> is None. Defaults to \"slic\".</p> <code>'slic'</code> <code>additional_forward_args</code> <code>Any</code> <p>If the forward function requires additional arguments other than the inputs for which attributions should not be computed, this argument can be provided. It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple containing multiple additional arguments including tensors or any arbitrary python types. These arguments are provided to forward_func in order following the arguments in inputs. Note that attributions are not computed with respect to these arguments. Default: None</p> <code>None</code> <code>**segmentation_method_params</code> <code>Any</code> <p>Additional parameters for the segmentation method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[HSISpatialAttributes] | HSISpatialAttributes</code> <p>HSISpatialAttributes | list[HSISpatialAttributes]: An object containing the image, the attributions, the segmentation mask, and the score of the interpretable model used for the explanation.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the Lime object is not initialized or is not an instance of LimeBase.</p> <code>MaskCreationError</code> <p>If there is an error creating the segmentation mask.</p> <code>ValueError</code> <p>If the number of segmentation masks is not equal to the number of HSI images provided.</p> <code>HSIAttributesError</code> <p>If there is an error during creating spatial attribution.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; simple_model = lambda x: torch.rand((x.shape[0], 2))\n&gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n&gt;&gt;&gt; segmentation_mask = torch.randint(1, 4, (1, 240, 240))\n&gt;&gt;&gt; lime = meteors.attr.Lime(\n        explainable_model=ExplainableModel(simple_model, \"regression\"), interpretable_model=SkLearnLasso(alpha=0.1)\n    )\n&gt;&gt;&gt; spatial_attribution = lime.get_spatial_attributes(hsi, segmentation_mask=segmentation_mask, target=0)\n&gt;&gt;&gt; spatial_attribution.hsi\nHSI(shape=(4, 240, 240), dtype=torch.float32)\n&gt;&gt;&gt; spatial_attribution.attributes.shape\ntorch.Size([4, 240, 240])\n&gt;&gt;&gt; spatial_attribution.segmentation_mask.shape\ntorch.Size([1, 240, 240])\n&gt;&gt;&gt; spatial_attribution.score\n1.0\n</code></pre> Source code in <code>src/meteors/attr/lime.py</code> <pre><code>def get_spatial_attributes(\n    self,\n    hsi: list[HSI] | HSI,\n    segmentation_mask: np.ndarray | torch.Tensor | list[np.ndarray | torch.Tensor] | None = None,\n    target: list[int] | int | None = None,\n    n_samples: int = 10,\n    perturbations_per_eval: int = 4,\n    verbose: bool = False,\n    segmentation_method: Literal[\"slic\", \"patch\"] = \"slic\",\n    additional_forward_args: Any = None,\n    **segmentation_method_params: Any,\n) -&gt; list[HSISpatialAttributes] | HSISpatialAttributes:\n    \"\"\"\n    Get spatial attributes of an hsi image using the LIME method. Based on the provided hsi and segmentation mask\n    LIME method attributes the `superpixels` provided by the segmentation mask. Please refer to the original paper\n    `https://arxiv.org/abs/1602.04938` for more details or to Christoph Molnar's book\n    `https://christophm.github.io/interpretable-ml-book/lime.html`.\n\n    This function attributes the hyperspectral image using the LIME (Local Interpretable Model-Agnostic Explanations)\n    method for spatial data. It returns an `HSISpatialAttributes` object that contains the hyperspectral image,,\n    the attributions, the segmentation mask, and the score of the interpretable model used for the explanation.\n\n    Args:\n        hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n            If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n            The output will be a list of HSISpatialAttributes objects.\n        segmentation_mask (np.ndarray | torch.Tensor | list[np.ndarray | torch.Tensor] | None, optional):\n            A segmentation mask according to which the attribution should be performed.\n            The segmentation mask should have a 2D or 3D shape, which can be broadcastable to the shape of the\n            input image. The only dimension on which the image and the mask shapes can differ is the spectral\n            dimension, marked with letter `C` in the `image.orientation` parameter. If None, a new segmentation mask\n            is created using the `segmentation_method`. Additional parameters for the segmentation method may be\n            passed as kwargs. If multiple HSI images are provided, a list of segmentation masks can be provided,\n            one for each image. If list is not provided method will assume that the same segmentation mask is used\n                for all images. Defaults to None.\n        target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n            methods assume that the output has only one class. If the output has multiple classes, the target index\n            must be provided. For multiple input images, a list of target indices can be provided, one for each\n            image or single target value will be used for all images. Defaults to None.\n        n_samples (int, optional): The number of samples to generate/analyze in LIME. The more the better but slower.\n            Defaults to 10.\n        perturbations_per_eval (int, optional): The number of perturbations to evaluate at once\n            (Simply the inner batch size). Defaults to 4.\n        verbose (bool, optional): Whether to show the progress bar. Defaults to False.\n        segmentation_method (Literal[\"slic\", \"patch\"], optional):\n            Segmentation method used only if `segmentation_mask` is None. Defaults to \"slic\".\n        additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n            the inputs for which attributions should not be computed, this argument can be provided.\n            It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n            containing multiple additional arguments including tensors or any arbitrary python types.\n            These arguments are provided to forward_func in order following the arguments in inputs.\n            Note that attributions are not computed with respect to these arguments. Default: None\n        **segmentation_method_params (Any): Additional parameters for the segmentation method.\n\n    Returns:\n        HSISpatialAttributes | list[HSISpatialAttributes]: An object containing the image, the attributions,\n            the segmentation mask, and the score of the interpretable model used for the explanation.\n\n    Raises:\n        RuntimeError: If the Lime object is not initialized or is not an instance of LimeBase.\n        MaskCreationError: If there is an error creating the segmentation mask.\n        ValueError: If the number of segmentation masks is not equal to the number of HSI images provided.\n        HSIAttributesError: If there is an error during creating spatial attribution.\n\n    Examples:\n        &gt;&gt;&gt; simple_model = lambda x: torch.rand((x.shape[0], 2))\n        &gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n        &gt;&gt;&gt; segmentation_mask = torch.randint(1, 4, (1, 240, 240))\n        &gt;&gt;&gt; lime = meteors.attr.Lime(\n                explainable_model=ExplainableModel(simple_model, \"regression\"), interpretable_model=SkLearnLasso(alpha=0.1)\n            )\n        &gt;&gt;&gt; spatial_attribution = lime.get_spatial_attributes(hsi, segmentation_mask=segmentation_mask, target=0)\n        &gt;&gt;&gt; spatial_attribution.hsi\n        HSI(shape=(4, 240, 240), dtype=torch.float32)\n        &gt;&gt;&gt; spatial_attribution.attributes.shape\n        torch.Size([4, 240, 240])\n        &gt;&gt;&gt; spatial_attribution.segmentation_mask.shape\n        torch.Size([1, 240, 240])\n        &gt;&gt;&gt; spatial_attribution.score\n        1.0\n    \"\"\"\n    if self._attribution_method is None or not isinstance(self._attribution_method, LimeBase):\n        raise RuntimeError(\"Lime object not initialized\")  # pragma: no cover\n\n    if isinstance(hsi, HSI):\n        hsi = [hsi]\n\n    if not all(isinstance(hsi_image, HSI) for hsi_image in hsi):\n        raise TypeError(\"All of the input hyperspectral images must be of type HSI\")\n\n    if segmentation_mask is None:\n        segmentation_mask = self.get_segmentation_mask(hsi[0], segmentation_method, **segmentation_method_params)\n\n        logger.warning(\n            \"Segmentation mask is created based on the first HSI image provided, this approach may not be optimal as \"\n            \"the same segmentation mask may not be the best suitable for all images\",\n        )\n\n    if isinstance(segmentation_mask, tuple):\n        segmentation_mask = tuple(segmentation_mask)\n    elif not isinstance(segmentation_mask, list):\n        segmentation_mask = [segmentation_mask] * len(hsi)\n\n    if len(hsi) != len(segmentation_mask):\n        raise ValueError(\n            f\"Number of segmentation masks should be equal to the number of HSI images provided, provided {len(segmentation_mask)}\"\n        )\n\n    segmentation_mask = [\n        ensure_torch_tensor(mask, f\"Segmentation mask number {idx+1} should be None, numpy array, or torch tensor\")\n        for idx, mask in enumerate(segmentation_mask)\n    ]\n    segmentation_mask = [\n        mask.unsqueeze(0).moveaxis(0, hsi_img.spectral_axis) if mask.ndim != hsi_img.image.ndim else mask\n        for hsi_img, mask in zip(hsi, segmentation_mask)\n    ]\n    segmentation_mask = [\n        validate_mask_shape(\"segmentation\", hsi_img, mask) for hsi_img, mask in zip(hsi, segmentation_mask)\n    ]\n\n    hsi_input = torch.stack([hsi_img.get_image() for hsi_img in hsi], dim=0)\n    segmentation_mask = torch.stack(segmentation_mask, dim=0)\n\n    assert segmentation_mask.shape == hsi_input.shape\n\n    segmentation_mask = segmentation_mask.to(self.device)\n    hsi_input = hsi_input.to(self.device)\n\n    lime_attributes, score = self._attribution_method.attribute(\n        inputs=hsi_input,\n        target=target,\n        feature_mask=segmentation_mask,\n        n_samples=n_samples,\n        perturbations_per_eval=perturbations_per_eval,\n        additional_forward_args=additional_forward_args,\n        show_progress=verbose,\n        return_input_shape=True,\n    )\n\n    try:\n        spatial_attribution = [\n            HSISpatialAttributes(\n                hsi=hsi_img,\n                attributes=lime_attr,\n                mask=segmentation_mask[idx].expand_as(hsi_img.image),\n                score=score.item(),\n                attribution_method=\"Lime\",\n            )\n            for idx, (hsi_img, lime_attr) in enumerate(zip(hsi, lime_attributes))\n        ]\n    except Exception as e:\n        raise HSIAttributesError(f\"Error during creating spatial attribution {e}\") from e\n\n    return spatial_attribution[0] if len(spatial_attribution) == 1 else spatial_attribution\n</code></pre>"},{"location":"reference/#src.meteors.attr.lime.Lime.get_spectral_attributes","title":"<code>get_spectral_attributes(hsi, band_mask=None, target=None, n_samples=10, perturbations_per_eval=4, verbose=False, additional_forward_args=None, band_names=None)</code>","text":"<p>Attributes the hsi image using LIME method for spectral data. Based on the provided hsi and band mask, the LIME method attributes the hsi based on <code>superbands</code> (clustered bands) provided by the band mask. Please refer to the original paper <code>https://arxiv.org/abs/1602.04938</code> for more details or to Christoph Molnar's book <code>https://christophm.github.io/interpretable-ml-book/lime.html</code>.</p> <p>The function returns a HSISpectralAttributes object that contains the image, the attributions, the band mask, the band names, and the score of the interpretable model used for the explanation.</p> <p>Parameters:</p> Name Type Description Default <code>hsi</code> <code>list[HSI] | HSI</code> <p>Input hyperspectral image(s) for which the attributions are to be computed. If a list of HSI objects is provided, the attributions are computed for each HSI object in the list. The output will be a list of HSISpatialAttributes objects.</p> required <code>band_mask</code> <code>ndarray | Tensor | list[ndarray | Tensor] | None</code> <p>Band mask that is used for the spectral attribution. The band mask should have a 1D or 3D shape, which can be broadcastable to the shape of the input image. The only dimensions on which the image and the mask shapes can differ is the height and width dimensions, marked with letters <code>H</code> and <code>W</code> in the <code>image.orientation</code> parameter. If equals to None, the band mask is created within the function. If multiple HSI images are provided, a list of band masks can be provided, one for each image. If list is not provided method will assume that the same band mask is used for all images. Defaults to None.</p> <code>None</code> <code>target</code> <code>list[int] | int | None</code> <p>target class index for computing the attributions. If None, methods assume that the output has only one class. If the output has multiple classes, the target index must be provided. For multiple input images, a list of target indices can be provided, one for each image or single target value will be used for all images. Defaults to None.</p> <code>None</code> <code>n_samples</code> <code>int</code> <p>The number of samples to generate/analyze in LIME. The more the better but slower. Defaults to 10.</p> <code>10</code> <code>perturbations_per_eval</code> <code>int</code> <p>The number of perturbations to evaluate at once (Simply the inner batch size). Defaults to 4.</p> <code>4</code> <code>verbose</code> <code>bool</code> <p>Whether to show the progress bar. Defaults to False.</p> <code>False</code> <code>segmentation_method</code> <code>Literal['slic', 'patch']</code> <p>Segmentation method used only if <code>segmentation_mask</code> is None. Defaults to \"slic\".</p> required <code>additional_forward_args</code> <code>Any</code> <p>If the forward function requires additional arguments other than the inputs for which attributions should not be computed, this argument can be provided. It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple containing multiple additional arguments including tensors or any arbitrary python types. These arguments are provided to forward_func in order following the arguments in inputs. Note that attributions are not computed with respect to these arguments. Default: None</p> <code>None</code> <code>band_names</code> <code>list[str] | dict[str | tuple[str, ...], int] | None</code> <p>Band names. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>HSISpectralAttributes | list[HSISpectralAttributes]</code> <p>HSISpectralAttributes | list[HSISpectralAttributes]: An object containing the image, the attributions, the band mask, the band names, and the score of the interpretable model used for the explanation.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the Lime object is not initialized or is not an instance of LimeBase.</p> <code>MaskCreationError</code> <p>If there is an error creating the band mask.</p> <code>ValueError</code> <p>If the number of band masks is not equal to the number of HSI images provided.</p> <code>HSIAttributesError</code> <p>If there is an error during creating spectral attribution.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; simple_model = lambda x: torch.rand((x.shape[0], 2))\n&gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n&gt;&gt;&gt; band_mask = torch.randint(1, 4, (4, 1, 1)).repeat(1, 240, 240)\n&gt;&gt;&gt; band_names = [\"R\", \"G\", \"B\"]\n&gt;&gt;&gt; lime = meteors.attr.Lime(\n        explainable_model=ExplainableModel(simple_model, \"regression\"), interpretable_model=SkLearnLasso(alpha=0.1)\n    )\n&gt;&gt;&gt; spectral_attribution = lime.get_spectral_attributes(hsi, band_mask=band_mask, band_names=band_names, target=0)\n&gt;&gt;&gt; spectral_attribution.hsi\nHSI(shape=(4, 240, 240), dtype=torch.float32)\n&gt;&gt;&gt; spectral_attribution.attributes.shape\ntorch.Size([4, 240, 240])\n&gt;&gt;&gt; spectral_attribution.band_mask.shape\ntorch.Size([4, 240, 240])\n&gt;&gt;&gt; spectral_attribution.band_names\n[\"R\", \"G\", \"B\"]\n&gt;&gt;&gt; spectral_attribution.score\n1.0\n</code></pre> Source code in <code>src/meteors/attr/lime.py</code> <pre><code>def get_spectral_attributes(\n    self,\n    hsi: list[HSI] | HSI,\n    band_mask: np.ndarray | torch.Tensor | list[np.ndarray | torch.Tensor] | None = None,\n    target: list[int] | int | None = None,\n    n_samples: int = 10,\n    perturbations_per_eval: int = 4,\n    verbose: bool = False,\n    additional_forward_args: Any = None,\n    band_names: list[str | list[str]] | dict[tuple[str, ...] | str, int] | None = None,\n) -&gt; HSISpectralAttributes | list[HSISpectralAttributes]:\n    \"\"\"\n    Attributes the hsi image using LIME method for spectral data. Based on the provided hsi and band mask, the LIME\n    method attributes the hsi based on `superbands` (clustered bands) provided by the band mask.\n    Please refer to the original paper `https://arxiv.org/abs/1602.04938` for more details or to\n    Christoph Molnar's book `https://christophm.github.io/interpretable-ml-book/lime.html`.\n\n    The function returns a HSISpectralAttributes object that contains the image, the attributions, the band mask,\n    the band names, and the score of the interpretable model used for the explanation.\n\n    Args:\n        hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n            If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n            The output will be a list of HSISpatialAttributes objects.\n        band_mask (np.ndarray | torch.Tensor | list[np.ndarray | torch.Tensor] | None, optional): Band mask that\n            is used for the spectral attribution. The band mask should have a 1D or 3D shape, which can be\n            broadcastable to the shape of the input image. The only dimensions on which the image and the mask shapes\n            can differ is the height and width dimensions, marked with letters `H` and `W` in the `image.orientation`\n            parameter. If equals to None, the band mask is created within the function. If multiple HSI images are\n            provided, a list of band masks can be provided, one for each image. If list is not provided method will\n            assume that the same band mask is used for all images. Defaults to None.\n        target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n            methods assume that the output has only one class. If the output has multiple classes, the target index\n            must be provided. For multiple input images, a list of target indices can be provided, one for each\n            image or single target value will be used for all images. Defaults to None.\n        n_samples (int, optional): The number of samples to generate/analyze in LIME. The more the better but slower.\n            Defaults to 10.\n        perturbations_per_eval (int, optional): The number of perturbations to evaluate at once\n            (Simply the inner batch size). Defaults to 4.\n        verbose (bool, optional): Whether to show the progress bar. Defaults to False.\n        segmentation_method (Literal[\"slic\", \"patch\"], optional):\n            Segmentation method used only if `segmentation_mask` is None. Defaults to \"slic\".\n        additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n            the inputs for which attributions should not be computed, this argument can be provided.\n            It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n            containing multiple additional arguments including tensors or any arbitrary python types.\n            These arguments are provided to forward_func in order following the arguments in inputs.\n            Note that attributions are not computed with respect to these arguments. Default: None\n        band_names (list[str] | dict[str | tuple[str, ...], int] | None, optional): Band names. Defaults to None.\n\n    Returns:\n        HSISpectralAttributes | list[HSISpectralAttributes]: An object containing the image, the attributions,\n            the band mask, the band names, and the score of the interpretable model used for the explanation.\n\n    Raises:\n        RuntimeError: If the Lime object is not initialized or is not an instance of LimeBase.\n        MaskCreationError: If there is an error creating the band mask.\n        ValueError: If the number of band masks is not equal to the number of HSI images provided.\n        HSIAttributesError: If there is an error during creating spectral attribution.\n\n    Examples:\n        &gt;&gt;&gt; simple_model = lambda x: torch.rand((x.shape[0], 2))\n        &gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n        &gt;&gt;&gt; band_mask = torch.randint(1, 4, (4, 1, 1)).repeat(1, 240, 240)\n        &gt;&gt;&gt; band_names = [\"R\", \"G\", \"B\"]\n        &gt;&gt;&gt; lime = meteors.attr.Lime(\n                explainable_model=ExplainableModel(simple_model, \"regression\"), interpretable_model=SkLearnLasso(alpha=0.1)\n            )\n        &gt;&gt;&gt; spectral_attribution = lime.get_spectral_attributes(hsi, band_mask=band_mask, band_names=band_names, target=0)\n        &gt;&gt;&gt; spectral_attribution.hsi\n        HSI(shape=(4, 240, 240), dtype=torch.float32)\n        &gt;&gt;&gt; spectral_attribution.attributes.shape\n        torch.Size([4, 240, 240])\n        &gt;&gt;&gt; spectral_attribution.band_mask.shape\n        torch.Size([4, 240, 240])\n        &gt;&gt;&gt; spectral_attribution.band_names\n        [\"R\", \"G\", \"B\"]\n        &gt;&gt;&gt; spectral_attribution.score\n        1.0\n    \"\"\"\n\n    if self._attribution_method is None or not isinstance(self._attribution_method, LimeBase):\n        raise RuntimeError(\"Lime object not initialized\")  # pragma: no cover\n\n    if isinstance(hsi, HSI):\n        hsi = [hsi]\n\n    if not all(isinstance(hsi_image, HSI) for hsi_image in hsi):\n        raise TypeError(\"All of the input hyperspectral images must be of type HSI\")\n\n    if band_mask is None:\n        created_bands = [self.get_band_mask(hsi_img, band_names) for hsi_img in hsi]\n        band_mask, band_name_list = zip(*created_bands)\n        band_names = band_name_list[0]\n\n    if isinstance(band_mask, tuple):\n        band_mask = list(band_mask)\n    elif not isinstance(band_mask, list):\n        band_mask = [band_mask]\n\n    if len(hsi) != len(band_mask):\n        if len(band_mask) == 1:\n            band_mask = band_mask * len(hsi)\n            logger.debug(\"Reusing the same band mask for all images\")\n        else:\n            raise ValueError(\n                f\"Number of band masks should be equal to the number of HSI images provided, provided {len(band_mask)}\"\n            )\n\n    band_mask = [\n        ensure_torch_tensor(mask, f\"Band mask number {idx+1} should be None, numpy array, or torch tensor\")\n        for idx, mask in enumerate(band_mask)\n    ]\n    band_mask = [\n        mask.unsqueeze(-1).unsqueeze(-1).moveaxis(0, hsi_img.spectral_axis)\n        if mask.ndim != hsi_img.image.ndim\n        else mask\n        for hsi_img, mask in zip(hsi, band_mask)\n    ]\n    band_mask = [validate_mask_shape(\"band\", hsi_img, mask) for hsi_img, mask in zip(hsi, band_mask)]\n\n    hsi_input = torch.stack([hsi_img.get_image() for hsi_img in hsi], dim=0)\n    band_mask = torch.stack(band_mask, dim=0)\n\n    if band_names is None:\n        band_names = {str(segment): idx for idx, segment in enumerate(torch.unique(band_mask))}\n    else:\n        logger.debug(\n            \"Band names are provided and will be used. In the future, there should be an option to validate them.\"\n        )\n\n    assert hsi_input.shape == band_mask.shape\n\n    hsi_input = hsi_input.to(self.device)\n    band_mask = band_mask.to(self.device)\n\n    lime_attributes, score = self._attribution_method.attribute(\n        inputs=hsi_input,\n        target=target,\n        feature_mask=band_mask,\n        n_samples=n_samples,\n        perturbations_per_eval=perturbations_per_eval,\n        additional_forward_args=additional_forward_args,\n        show_progress=verbose,\n        return_input_shape=True,\n    )\n\n    try:\n        spectral_attribution = [\n            HSISpectralAttributes(\n                hsi=hsi_img,\n                attributes=lime_attr,\n                mask=band_mask[idx].expand_as(hsi_img.image),\n                band_names=band_names,\n                score=score.item(),\n                attribution_method=\"Lime\",\n            )\n            for idx, (hsi_img, lime_attr) in enumerate(zip(hsi, lime_attributes))\n        ]\n    except Exception as e:\n        raise HSIAttributesError(f\"Error during creating spectral attribution {e}\") from e\n\n    return spectral_attribution[0] if len(spectral_attribution) == 1 else spectral_attribution\n</code></pre>"},{"location":"reference/#lime-base","title":"Lime Base","text":"<p>The Lime Base class was adapted from the Captum Lime implementation. This adaptation builds upon the original work, extending and customizing it for specific use cases within this project. To see the original implementation, please refer to the Captum repository.</p>"},{"location":"reference/#src.meteors.attr.integrated_gradients.IntegratedGradients","title":"<code>IntegratedGradients</code>","text":"<p>               Bases: <code>Explainer</code></p> <p>IntegratedGradients explainer class for generating attributions using the Integrated Gradients method. The Integrated Gradients method is based on the <code>captum</code> implementation and is an implementation of an idea coming from the original paper on Integrated Gradients, where more details about this method can be found.</p> <p>Attributes:</p> Name Type Description <code>_attribution_method</code> <code>IntegratedGradients</code> <p>The Integrated Gradients method from the <code>captum</code> library.</p> <code>multiply_by_inputs</code> <p>Indicates whether to factor model inputs\u2019 multiplier in the final attribution scores. In the literature this is also known as local vs global attribution. If inputs\u2019 multiplier isn\u2019t factored in, then that type of attribution method is also called local attribution. If it is, then that type of attribution method is called global. More detailed can be found in this paper. In case of integrated gradients, if multiply_by_inputs is set to True, final sensitivity scores are being multiplied by (inputs - baselines).</p> <p>Parameters:</p> Name Type Description Default <code>explainable_model</code> <code>ExplainableModel | Explainer</code> <p>The explainable model to be explained.</p> required Source code in <code>src/meteors/attr/integrated_gradients.py</code> <pre><code>class IntegratedGradients(Explainer):\n    \"\"\"\n    IntegratedGradients explainer class for generating attributions using the Integrated Gradients method.\n    The Integrated Gradients method is based on the [`captum` implementation](https://captum.ai/api/integrated_gradients.html)\n    and is an implementation of an idea coming from the [original paper on Integrated Gradients](https://arxiv.org/pdf/1703.01365),\n    where more details about this method can be found.\n\n    Attributes:\n        _attribution_method (CaptumIntegratedGradients): The Integrated Gradients method from the `captum` library.\n        multiply_by_inputs: Indicates whether to factor model inputs\u2019 multiplier in the final attribution scores.\n            In the literature this is also known as local vs global attribution. If inputs\u2019 multiplier isn\u2019t factored\n            in, then that type of attribution method is also called local attribution. If it is, then that type of\n            attribution method is called global. More detailed can be found in this [paper](https://arxiv.org/abs/1711.06104).\n            In case of integrated gradients, if multiply_by_inputs is set to True,\n            final sensitivity scores are being multiplied by (inputs - baselines).\n\n    Args:\n        explainable_model (ExplainableModel | Explainer): The explainable model to be explained.\n    \"\"\"\n\n    def __init__(self, explainable_model: ExplainableModel, multiply_by_inputs: bool = True):\n        super().__init__(explainable_model)\n        self.multiply_by_inputs = multiply_by_inputs\n\n        self._attribution_method = CaptumIntegratedGradients(\n            explainable_model.forward_func, multiply_by_inputs=self.multiply_by_inputs\n        )\n\n    def attribute(\n        self,\n        hsi: list[HSI] | HSI,\n        baseline: int | float | torch.Tensor | list[int | float | torch.Tensor] = None,\n        target: list[int] | int | None = None,\n        additional_forward_args: Any = None,\n        n_steps: int = 50,\n        method: Literal[\n            \"riemann_right\", \"riemann_left\", \"riemann_middle\", \"riemann_trapezoid\", \"gausslegendre\"\n        ] = \"gausslegendre\",\n        return_convergence_delta: bool = False,\n    ) -&gt; HSIAttributes | list[HSIAttributes]:\n        \"\"\"\n        Method for generating attributions using the Integrated Gradients method.\n\n        Args:\n            hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n                If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n                The output will be a list of HSIAttributes objects.\n            baseline (int | float | torch.Tensor | list[int | float | torch.Tensor, optional): Baselines define the\n                starting point from which integral is computed and can be provided as:\n                    - integer or float representing a constant value used as the baseline for all input pixels.\n                    - tensor with the same shape as the input tensor, providing a baseline for each input pixel.\n                        if the input is a list of HSI objects, the baseline can be a tensor with the same shape as\n                        the input tensor for each HSI object.\n                    - list of integers, floats or tensors with the same shape as the input tensor, providing a baseline\n                        for each input pixel. If the input is a list of HSI objects, the baseline can be a list of\n                        tensors with the same shape as the input tensor for each HSI object. Defaults to None.\n            target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n                methods assume that the output has only one class. If the output has multiple classes, the target index\n                must be provided. For multiple input images, a list of target indices can be provided, one for each\n                image or single target value will be used for all images. Defaults to None.\n            additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n                the inputs for which attributions should not be computed, this argument can be provided.\n                It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n                containing multiple additional arguments including tensors or any arbitrary python types.\n                These arguments are provided to forward_func in order following the arguments in inputs.\n                Note that attributions are not computed with respect to these arguments. Default: None\n            n_steps (int, optional): The number of steps to approximate the integral. Default: 50.\n            method (Literal[\"riemann_right\", \"riemann_left\", \"riemann_middle\", \"riemann_trapezoid\", \"gausslegendre\"],\n                optional): Method for approximating the integral, one of riemann_right, riemann_left, riemann_middle,\n                riemann_trapezoid or gausslegendre. Default: gausslegendre if no method is provided.\n            return_convergence_delta (bool, optional): Indicates whether to return convergence delta or not.\n                If return_convergence_delta is set to True convergence delta will be returned in a tuple following\n                attributions. Default: False\n\n        Returns:\n            HSIAttributes | list[HSIAttributes]: The computed attributions for the input hyperspectral image(s).\n                if a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n\n        Raises:\n            RuntimeError: If the explainer is not initialized.\n            HSIAttributesError: If an error occurs during the generation of the attributions.\n\n\n        Examples:\n            &gt;&gt;&gt; integrated_gradients = IntegratedGradients(explainable_model)\n            &gt;&gt;&gt; hsi = HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n            &gt;&gt;&gt; attributions = integrated_gradients.attribute(hsi, method=\"riemann_right\", baseline=0.0)\n            &gt;&gt;&gt; attributions, approximation_error = integrated_gradients.attribute(hsi, return_convergence_delta=True)\n            &gt;&gt;&gt; approximation_error\n            0.5\n            &gt;&gt;&gt; attributions = integrated_gradients.attribute([hsi, hsi])\n            &gt;&gt;&gt; len(attributions)\n            2\n        \"\"\"\n        if self._attribution_method is None:\n            raise RuntimeError(\"IntegratedGradients explainer is not initialized, INITIALIZATION ERROR\")\n\n        if not isinstance(hsi, list):\n            hsi = [hsi]\n\n        if not all(isinstance(hsi_image, HSI) for hsi_image in hsi):\n            raise TypeError(\"All of the input hyperspectral images must be of type HSI\")\n\n        if not isinstance(baseline, list):\n            baseline = [baseline] * len(hsi)\n\n        baseline = torch.stack(\n            [\n                validate_and_transform_baseline(base, hsi_image).to(hsi_image.device)\n                for hsi_image, base in zip(hsi, baseline)\n            ],\n            dim=0,\n        )\n        input_tensor = torch.stack(\n            [hsi_image.get_image().requires_grad_(True).to(hsi_image.device) for hsi_image in hsi], dim=0\n        )\n\n        ig_attributions = self._attribution_method.attribute(\n            input_tensor,\n            baselines=baseline,\n            target=target,\n            n_steps=n_steps,\n            additional_forward_args=additional_forward_args,\n            method=method,\n            return_convergence_delta=return_convergence_delta,\n        )\n\n        if return_convergence_delta:\n            attributions, approximation_error = ig_attributions\n        else:\n            attributions, approximation_error = ig_attributions, [None] * len(hsi)\n\n        try:\n            attributes = [\n                HSIAttributes(hsi=hsi_image, attributes=attribution, score=error, attribution_method=self.get_name())\n                for hsi_image, attribution, error in zip(hsi, attributions, approximation_error)\n            ]\n        except Exception as e:\n            raise HSIAttributesError(f\"Error while creating HSIAttributes: {e}\") from e\n\n        return attributes[0] if len(attributes) == 1 else attributes\n</code></pre>"},{"location":"reference/#src.meteors.attr.integrated_gradients.IntegratedGradients.attribute","title":"<code>attribute(hsi, baseline=None, target=None, additional_forward_args=None, n_steps=50, method='gausslegendre', return_convergence_delta=False)</code>","text":"<p>Method for generating attributions using the Integrated Gradients method.</p> <p>Parameters:</p> Name Type Description Default <code>hsi</code> <code>list[HSI] | HSI</code> <p>Input hyperspectral image(s) for which the attributions are to be computed. If a list of HSI objects is provided, the attributions are computed for each HSI object in the list. The output will be a list of HSIAttributes objects.</p> required <code>baseline</code> <code>int | float | torch.Tensor | list[int | float | torch.Tensor</code> <p>Baselines define the starting point from which integral is computed and can be provided as:     - integer or float representing a constant value used as the baseline for all input pixels.     - tensor with the same shape as the input tensor, providing a baseline for each input pixel.         if the input is a list of HSI objects, the baseline can be a tensor with the same shape as         the input tensor for each HSI object.     - list of integers, floats or tensors with the same shape as the input tensor, providing a baseline         for each input pixel. If the input is a list of HSI objects, the baseline can be a list of         tensors with the same shape as the input tensor for each HSI object. Defaults to None.</p> <code>None</code> <code>target</code> <code>list[int] | int | None</code> <p>target class index for computing the attributions. If None, methods assume that the output has only one class. If the output has multiple classes, the target index must be provided. For multiple input images, a list of target indices can be provided, one for each image or single target value will be used for all images. Defaults to None.</p> <code>None</code> <code>additional_forward_args</code> <code>Any</code> <p>If the forward function requires additional arguments other than the inputs for which attributions should not be computed, this argument can be provided. It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple containing multiple additional arguments including tensors or any arbitrary python types. These arguments are provided to forward_func in order following the arguments in inputs. Note that attributions are not computed with respect to these arguments. Default: None</p> <code>None</code> <code>n_steps</code> <code>int</code> <p>The number of steps to approximate the integral. Default: 50.</p> <code>50</code> <code>return_convergence_delta</code> <code>bool</code> <p>Indicates whether to return convergence delta or not. If return_convergence_delta is set to True convergence delta will be returned in a tuple following attributions. Default: False</p> <code>False</code> <p>Returns:</p> Type Description <code>HSIAttributes | list[HSIAttributes]</code> <p>HSIAttributes | list[HSIAttributes]: The computed attributions for the input hyperspectral image(s). if a list of HSI objects is provided, the attributions are computed for each HSI object in the list.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the explainer is not initialized.</p> <code>HSIAttributesError</code> <p>If an error occurs during the generation of the attributions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; integrated_gradients = IntegratedGradients(explainable_model)\n&gt;&gt;&gt; hsi = HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n&gt;&gt;&gt; attributions = integrated_gradients.attribute(hsi, method=\"riemann_right\", baseline=0.0)\n&gt;&gt;&gt; attributions, approximation_error = integrated_gradients.attribute(hsi, return_convergence_delta=True)\n&gt;&gt;&gt; approximation_error\n0.5\n&gt;&gt;&gt; attributions = integrated_gradients.attribute([hsi, hsi])\n&gt;&gt;&gt; len(attributions)\n2\n</code></pre> Source code in <code>src/meteors/attr/integrated_gradients.py</code> <pre><code>def attribute(\n    self,\n    hsi: list[HSI] | HSI,\n    baseline: int | float | torch.Tensor | list[int | float | torch.Tensor] = None,\n    target: list[int] | int | None = None,\n    additional_forward_args: Any = None,\n    n_steps: int = 50,\n    method: Literal[\n        \"riemann_right\", \"riemann_left\", \"riemann_middle\", \"riemann_trapezoid\", \"gausslegendre\"\n    ] = \"gausslegendre\",\n    return_convergence_delta: bool = False,\n) -&gt; HSIAttributes | list[HSIAttributes]:\n    \"\"\"\n    Method for generating attributions using the Integrated Gradients method.\n\n    Args:\n        hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n            If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n            The output will be a list of HSIAttributes objects.\n        baseline (int | float | torch.Tensor | list[int | float | torch.Tensor, optional): Baselines define the\n            starting point from which integral is computed and can be provided as:\n                - integer or float representing a constant value used as the baseline for all input pixels.\n                - tensor with the same shape as the input tensor, providing a baseline for each input pixel.\n                    if the input is a list of HSI objects, the baseline can be a tensor with the same shape as\n                    the input tensor for each HSI object.\n                - list of integers, floats or tensors with the same shape as the input tensor, providing a baseline\n                    for each input pixel. If the input is a list of HSI objects, the baseline can be a list of\n                    tensors with the same shape as the input tensor for each HSI object. Defaults to None.\n        target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n            methods assume that the output has only one class. If the output has multiple classes, the target index\n            must be provided. For multiple input images, a list of target indices can be provided, one for each\n            image or single target value will be used for all images. Defaults to None.\n        additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n            the inputs for which attributions should not be computed, this argument can be provided.\n            It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n            containing multiple additional arguments including tensors or any arbitrary python types.\n            These arguments are provided to forward_func in order following the arguments in inputs.\n            Note that attributions are not computed with respect to these arguments. Default: None\n        n_steps (int, optional): The number of steps to approximate the integral. Default: 50.\n        method (Literal[\"riemann_right\", \"riemann_left\", \"riemann_middle\", \"riemann_trapezoid\", \"gausslegendre\"],\n            optional): Method for approximating the integral, one of riemann_right, riemann_left, riemann_middle,\n            riemann_trapezoid or gausslegendre. Default: gausslegendre if no method is provided.\n        return_convergence_delta (bool, optional): Indicates whether to return convergence delta or not.\n            If return_convergence_delta is set to True convergence delta will be returned in a tuple following\n            attributions. Default: False\n\n    Returns:\n        HSIAttributes | list[HSIAttributes]: The computed attributions for the input hyperspectral image(s).\n            if a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n\n    Raises:\n        RuntimeError: If the explainer is not initialized.\n        HSIAttributesError: If an error occurs during the generation of the attributions.\n\n\n    Examples:\n        &gt;&gt;&gt; integrated_gradients = IntegratedGradients(explainable_model)\n        &gt;&gt;&gt; hsi = HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n        &gt;&gt;&gt; attributions = integrated_gradients.attribute(hsi, method=\"riemann_right\", baseline=0.0)\n        &gt;&gt;&gt; attributions, approximation_error = integrated_gradients.attribute(hsi, return_convergence_delta=True)\n        &gt;&gt;&gt; approximation_error\n        0.5\n        &gt;&gt;&gt; attributions = integrated_gradients.attribute([hsi, hsi])\n        &gt;&gt;&gt; len(attributions)\n        2\n    \"\"\"\n    if self._attribution_method is None:\n        raise RuntimeError(\"IntegratedGradients explainer is not initialized, INITIALIZATION ERROR\")\n\n    if not isinstance(hsi, list):\n        hsi = [hsi]\n\n    if not all(isinstance(hsi_image, HSI) for hsi_image in hsi):\n        raise TypeError(\"All of the input hyperspectral images must be of type HSI\")\n\n    if not isinstance(baseline, list):\n        baseline = [baseline] * len(hsi)\n\n    baseline = torch.stack(\n        [\n            validate_and_transform_baseline(base, hsi_image).to(hsi_image.device)\n            for hsi_image, base in zip(hsi, baseline)\n        ],\n        dim=0,\n    )\n    input_tensor = torch.stack(\n        [hsi_image.get_image().requires_grad_(True).to(hsi_image.device) for hsi_image in hsi], dim=0\n    )\n\n    ig_attributions = self._attribution_method.attribute(\n        input_tensor,\n        baselines=baseline,\n        target=target,\n        n_steps=n_steps,\n        additional_forward_args=additional_forward_args,\n        method=method,\n        return_convergence_delta=return_convergence_delta,\n    )\n\n    if return_convergence_delta:\n        attributions, approximation_error = ig_attributions\n    else:\n        attributions, approximation_error = ig_attributions, [None] * len(hsi)\n\n    try:\n        attributes = [\n            HSIAttributes(hsi=hsi_image, attributes=attribution, score=error, attribution_method=self.get_name())\n            for hsi_image, attribution, error in zip(hsi, attributions, approximation_error)\n        ]\n    except Exception as e:\n        raise HSIAttributesError(f\"Error while creating HSIAttributes: {e}\") from e\n\n    return attributes[0] if len(attributes) == 1 else attributes\n</code></pre>"},{"location":"reference/#src.meteors.attr.input_x_gradients.InputXGradient","title":"<code>InputXGradient</code>","text":"<p>               Bases: <code>Explainer</code></p> <p>Initializes the InputXGradient explainer. The InputXGradients method is a straightforward approach to computing attribution. It simply multiplies the input image with the gradient with respect to the input. This method is based on the <code>captum</code> implementation</p> <p>Attributes:</p> Name Type Description <code>_attribution_method</code> <code>CaptumIntegratedGradients</code> <p>The InputXGradient method from the <code>captum</code> library.</p> <p>Parameters:</p> Name Type Description Default <code>explainable_model</code> <code>ExplainableModel | Explainer</code> <p>The explainable model to be explained.</p> required Source code in <code>src/meteors/attr/input_x_gradients.py</code> <pre><code>class InputXGradient(Explainer):\n    \"\"\"\n    Initializes the InputXGradient explainer. The InputXGradients method is a straightforward approach to\n    computing attribution. It simply multiplies the input image with the gradient with respect to the input.\n    This method is based on the [`captum` implementation](https://captum.ai/api/input_x_gradient.html)\n\n    Attributes:\n        _attribution_method (CaptumIntegratedGradients): The InputXGradient method from the `captum` library.\n\n    Args:\n        explainable_model (ExplainableModel | Explainer): The explainable model to be explained.\n    \"\"\"\n\n    def __init__(self, explainable_model: ExplainableModel):\n        super().__init__(explainable_model)\n\n        self._attribution_method = CaptumInputXGradient(explainable_model.forward_func)\n\n    def attribute(\n        self,\n        hsi: list[HSI] | HSI,\n        target: list[int] | int | None = None,\n        additional_forward_args: Any = None,\n    ) -&gt; HSIAttributes | list[HSIAttributes]:\n        \"\"\"\n        Method for generating attributions using the InputXGradient method.\n\n        Args:\n            hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n                If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n                The output will be a list of HSIAttributes objects.\n            target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n                methods assume that the output has only one class. If the output has multiple classes, the target index\n                must be provided. For multiple input images, a list of target indices can be provided, one for each\n                image or single target value will be used for all images. Defaults to None.\n            additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n                the inputs for which attributions should not be computed, this argument can be provided.\n                It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n                containing multiple additional arguments including tensors or any arbitrary python types.\n                These arguments are provided to forward_func in order following the arguments in inputs.\n                Note that attributions are not computed with respect to these arguments. Default: None\n\n        Returns:\n            HSIAttributes | list[HSIAttributes]: The computed attributions for the input hyperspectral image(s).\n                if a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n\n        Raises:\n            RuntimeError: If the explainer is not initialized.\n            HSIAttributesError: If an error occurs during the generation of the attributions.\n\n        Examples:\n            &gt;&gt;&gt; input_x_gradient = InputXGradient(explainable_model)\n            &gt;&gt;&gt; hsi = HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n            &gt;&gt;&gt; attributions = input_x_gradient.attribute(hsi)\n            &gt;&gt;&gt; attributions = input_x_gradient.attribute([hsi, hsi])\n            &gt;&gt;&gt; len(attributions)\n            2\n        \"\"\"\n        if self._attribution_method is None:\n            raise RuntimeError(\"InputXGradient explainer is not initialized, INITIALIZATION ERROR\")\n\n        if not isinstance(hsi, list):\n            hsi = [hsi]\n\n        if not all(isinstance(hsi_image, HSI) for hsi_image in hsi):\n            raise TypeError(\"All of the input hyperspectral images must be of type HSI\")\n\n        input_tensor = torch.stack(\n            [hsi_image.get_image().requires_grad_(True).to(hsi_image.device) for hsi_image in hsi], dim=0\n        )\n\n        gradient_attribution = self._attribution_method.attribute(\n            input_tensor, target=target, additional_forward_args=additional_forward_args\n        )\n\n        try:\n            attributes = [\n                HSIAttributes(hsi=hsi_image, attributes=attribution, attribution_method=self.get_name())\n                for hsi_image, attribution in zip(hsi, gradient_attribution)\n            ]\n        except Exception as e:\n            raise HSIAttributesError(f\"Error in generating InputXGradient attributions: {e}\") from e\n\n        return attributes[0] if len(attributes) == 1 else attributes\n</code></pre>"},{"location":"reference/#src.meteors.attr.input_x_gradients.InputXGradient.attribute","title":"<code>attribute(hsi, target=None, additional_forward_args=None)</code>","text":"<p>Method for generating attributions using the InputXGradient method.</p> <p>Parameters:</p> Name Type Description Default <code>hsi</code> <code>list[HSI] | HSI</code> <p>Input hyperspectral image(s) for which the attributions are to be computed. If a list of HSI objects is provided, the attributions are computed for each HSI object in the list. The output will be a list of HSIAttributes objects.</p> required <code>target</code> <code>list[int] | int | None</code> <p>target class index for computing the attributions. If None, methods assume that the output has only one class. If the output has multiple classes, the target index must be provided. For multiple input images, a list of target indices can be provided, one for each image or single target value will be used for all images. Defaults to None.</p> <code>None</code> <code>additional_forward_args</code> <code>Any</code> <p>If the forward function requires additional arguments other than the inputs for which attributions should not be computed, this argument can be provided. It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple containing multiple additional arguments including tensors or any arbitrary python types. These arguments are provided to forward_func in order following the arguments in inputs. Note that attributions are not computed with respect to these arguments. Default: None</p> <code>None</code> <p>Returns:</p> Type Description <code>HSIAttributes | list[HSIAttributes]</code> <p>HSIAttributes | list[HSIAttributes]: The computed attributions for the input hyperspectral image(s). if a list of HSI objects is provided, the attributions are computed for each HSI object in the list.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the explainer is not initialized.</p> <code>HSIAttributesError</code> <p>If an error occurs during the generation of the attributions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; input_x_gradient = InputXGradient(explainable_model)\n&gt;&gt;&gt; hsi = HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n&gt;&gt;&gt; attributions = input_x_gradient.attribute(hsi)\n&gt;&gt;&gt; attributions = input_x_gradient.attribute([hsi, hsi])\n&gt;&gt;&gt; len(attributions)\n2\n</code></pre> Source code in <code>src/meteors/attr/input_x_gradients.py</code> <pre><code>def attribute(\n    self,\n    hsi: list[HSI] | HSI,\n    target: list[int] | int | None = None,\n    additional_forward_args: Any = None,\n) -&gt; HSIAttributes | list[HSIAttributes]:\n    \"\"\"\n    Method for generating attributions using the InputXGradient method.\n\n    Args:\n        hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n            If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n            The output will be a list of HSIAttributes objects.\n        target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n            methods assume that the output has only one class. If the output has multiple classes, the target index\n            must be provided. For multiple input images, a list of target indices can be provided, one for each\n            image or single target value will be used for all images. Defaults to None.\n        additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n            the inputs for which attributions should not be computed, this argument can be provided.\n            It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n            containing multiple additional arguments including tensors or any arbitrary python types.\n            These arguments are provided to forward_func in order following the arguments in inputs.\n            Note that attributions are not computed with respect to these arguments. Default: None\n\n    Returns:\n        HSIAttributes | list[HSIAttributes]: The computed attributions for the input hyperspectral image(s).\n            if a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n\n    Raises:\n        RuntimeError: If the explainer is not initialized.\n        HSIAttributesError: If an error occurs during the generation of the attributions.\n\n    Examples:\n        &gt;&gt;&gt; input_x_gradient = InputXGradient(explainable_model)\n        &gt;&gt;&gt; hsi = HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n        &gt;&gt;&gt; attributions = input_x_gradient.attribute(hsi)\n        &gt;&gt;&gt; attributions = input_x_gradient.attribute([hsi, hsi])\n        &gt;&gt;&gt; len(attributions)\n        2\n    \"\"\"\n    if self._attribution_method is None:\n        raise RuntimeError(\"InputXGradient explainer is not initialized, INITIALIZATION ERROR\")\n\n    if not isinstance(hsi, list):\n        hsi = [hsi]\n\n    if not all(isinstance(hsi_image, HSI) for hsi_image in hsi):\n        raise TypeError(\"All of the input hyperspectral images must be of type HSI\")\n\n    input_tensor = torch.stack(\n        [hsi_image.get_image().requires_grad_(True).to(hsi_image.device) for hsi_image in hsi], dim=0\n    )\n\n    gradient_attribution = self._attribution_method.attribute(\n        input_tensor, target=target, additional_forward_args=additional_forward_args\n    )\n\n    try:\n        attributes = [\n            HSIAttributes(hsi=hsi_image, attributes=attribution, attribution_method=self.get_name())\n            for hsi_image, attribution in zip(hsi, gradient_attribution)\n        ]\n    except Exception as e:\n        raise HSIAttributesError(f\"Error in generating InputXGradient attributions: {e}\") from e\n\n    return attributes[0] if len(attributes) == 1 else attributes\n</code></pre>"},{"location":"reference/#src.meteors.attr.occlusion.Occlusion","title":"<code>Occlusion</code>","text":"<p>               Bases: <code>Explainer</code></p> <p>Occlusion explainer class for generating attributions using the Occlusion method. This attribution method perturbs the input by replacing the contiguous rectangular region with a given baseline and computing the difference in output. In our case, features are located in multiple regions, and attribution from different hyper-rectangles is averaged. The implementation of this method is also based on the <code>captum</code> repository. More details about this approach can be found in the original paper</p> <p>Attributes:</p> Name Type Description <code>_attribution_method</code> <code>Occlusion</code> <p>The Occlusion method from the <code>captum</code> library.</p> <p>Parameters:</p> Name Type Description Default <code>explainable_model</code> <code>ExplainableModel | Explainer</code> <p>The explainable model to be explained.</p> required <code>postprocessing_segmentation_output</code> <code>Callable[[Tensor], Tensor] | None</code> <p>A segmentation postprocessing function for segmentation problem type. This is required for segmentation problem type as attribution methods needs to have 1d output. Defaults to None, which means that the attribution method is not used.</p> required Source code in <code>src/meteors/attr/occlusion.py</code> <pre><code>class Occlusion(Explainer):\n    \"\"\"\n    Occlusion explainer class for generating attributions using the Occlusion method.\n    This attribution method perturbs the input by replacing the contiguous rectangular region\n    with a given baseline and computing the difference in output.\n    In our case, features are located in multiple regions, and attribution from different hyper-rectangles is averaged.\n    The implementation of this method is also based on the [`captum` repository](https://captum.ai/api/occlusion.html).\n    More details about this approach can be found in the [original paper](https://arxiv.org/abs/1311.2901)\n\n    Attributes:\n        _attribution_method (CaptumOcclusion): The Occlusion method from the `captum` library.\n\n    Args:\n        explainable_model (ExplainableModel | Explainer): The explainable model to be explained.\n        postprocessing_segmentation_output (Callable[[torch.Tensor], torch.Tensor] | None):\n            A segmentation postprocessing function for segmentation problem type. This is required for segmentation\n            problem type as attribution methods needs to have 1d output. Defaults to None, which means that the\n            attribution method is not used.\n    \"\"\"\n\n    def __init__(self, explainable_model: ExplainableModel):\n        super().__init__(explainable_model)\n\n        self._attribution_method = CaptumOcclusion(explainable_model.forward_func)\n\n    @staticmethod\n    def _create_segmentation_mask(\n        input_shape: tuple[int, int, int], sliding_window_shapes: tuple[int, int, int], strides: tuple[int, int, int]\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Create a binary segmentation mask based on sliding windows.\n\n        Args:\n            input_shape (Tuple[int, int, int]): Shape of the input tensor (e.g., (H, W, C))\n            sliding_window_shapes (Tuple[int, int, int]): Shape of the sliding window (e.g., (h, w, c))\n            strides (Tuple[int, int, int]): Strides for the sliding window (e.g., (s_h, s_w, s_c))\n\n        Returns:\n            torch.Tensor: Binary mask tensor with ones where windows are placed\n        \"\"\"\n        # Initialize empty mask\n        mask = torch.zeros(input_shape, dtype=torch.int32)\n\n        # Calculate number of windows in each dimension\n        windows = []\n        for dim_size, window_size, stride in zip(input_shape, sliding_window_shapes, strides):\n            if stride == 0:\n                raise ValueError(\"Stride cannot be zero.\")\n            n_windows = dim_size // stride if (dim_size - window_size) % stride == 0 else dim_size // stride + 1\n            # 1 + (dim_size - window_size) // stride\n            windows.append(n_windows)\n\n        # Generate all possible indices using itertools.product\n        for i, indices in enumerate(itertools.product(*[range(w) for w in windows])):\n            # Calculate start position for each dimension\n            starts = [idx * stride for idx, stride in zip(indices, strides)]\n\n            # Calculate end position for each dimension\n            ends = [start + window for start, window in zip(starts, sliding_window_shapes)]\n\n            # Create slice objects for each dimension\n            slices = tuple(slice(start, end) for start, end in zip(starts, ends))\n\n            # Mark window position in mask\n            mask[slices] = i + 1\n\n        return mask\n\n    def attribute(\n        self,\n        hsi: list[HSI] | HSI,\n        target: list[int] | int | None = None,\n        sliding_window_shapes: int | tuple[int, int, int] = (1, 1, 1),\n        strides: int | tuple[int, int, int] = (1, 1, 1),\n        baseline: int | float | torch.Tensor | list[int | float | torch.Tensor] = None,\n        additional_forward_args: Any = None,\n        perturbations_per_eval: int = 1,\n        show_progress: bool = False,\n    ) -&gt; HSIAttributes | list[HSIAttributes]:\n        \"\"\"\n        Method for generating attributions using the Occlusion method.\n\n        Args:\n            hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n                If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n                The output will be a list of HSIAttributes objects.\n            target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n                methods assume that the output has only one class. If the output has multiple classes, the target index\n                must be provided. For multiple input images, a list of target indices can be provided, one for each\n                image or single target value will be used for all images. Defaults to None.\n            sliding_window_shapes (int | tuple[int, int, int]):\n                The shape of the sliding window. If an integer is provided, it will be used for all dimensions.\n                Defaults to (1, 1, 1).\n            strides (int | tuple[int, int, int], optional): The stride of the sliding window. Defaults to (1, 1, 1).\n                Simply put, the stride is the number of pixels by which the sliding window is moved in each dimension.\n            baseline (int | float | torch.Tensor | list[int | float | torch.Tensor], optional): Baselines define\n                reference value which replaces each feature when occluded is computed and can be provided as:\n                    - integer or float representing a constant value used as the baseline for all input pixels.\n                    - tensor with the same shape as the input tensor, providing a baseline for each input pixel.\n                        if the input is a list of HSI objects, the baseline can be a tensor with the same shape as\n                        the input tensor for each HSI object.\n                    - list of integers, floats or tensors with the same shape as the input tensor, providing a baseline\n                        for each input pixel. If the input is a list of HSI objects, the baseline can be a list of\n                        tensors with the same shape as the input tensor for each HSI object. Defaults to None.\n            additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n                the inputs for which attributions should not be computed, this argument can be provided.\n                It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n                containing multiple additional arguments including tensors or any arbitrary python types.\n                These arguments are provided to forward_func in order following the arguments in inputs.\n                Note that attributions are not computed with respect to these arguments. Default: None\n            perturbations_per_eval (int, optional): Allows multiple occlusions to be included in one batch\n                (one call to forward_fn). By default, perturbations_per_eval is 1, so each occlusion is processed\n                individually. Each forward pass will contain a maximum of perturbations_per_eval * #examples samples.\n                For DataParallel models, each batch is split among the available devices, so evaluations on each\n                available device contain at most (perturbations_per_eval * #examples) / num_devices samples. When\n                working with multiple examples, the number of perturbations per evaluation should be set to at least\n                the number of examples. Defaults to 1.\n            show_progress (bool, optional): If True, displays a progress bar. Defaults to False.\n\n        Returns:\n            HSIAttributes: The computed attributions for the input hyperspectral image(s). if a list of HSI objects\n                is provided, the attributions are computed for each HSI object in the list.\n\n        Raises:\n            RuntimeError: If the explainer is not initialized.\n            ValueError: If the sliding window shapes or strides are not a tuple of three integers.\n            HSIAttributesError: If an error occurs during the generation of the attributions.\n\n        Example:\n            &gt;&gt;&gt; occlusion = Occlusion(explainable_model)\n            &gt;&gt;&gt; hsi = HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n            &gt;&gt;&gt; attributions = occlusion.attribute(hsi, baseline=0, sliding_window_shapes=(4, 3, 3), strides=(1, 1, 1))\n            &gt;&gt;&gt; attributions = occlusion.attribute([hsi, hsi], baseline=0, sliding_window_shapes=(4, 3, 3), strides=(1, 2, 2))\n            &gt;&gt;&gt; len(attributions)\n            2\n        \"\"\"\n        if self._attribution_method is None:\n            raise RuntimeError(\"Occlusion explainer is not initialized, INITIALIZATION ERROR\")\n\n        if not isinstance(hsi, list):\n            hsi = [hsi]\n\n        if not all(isinstance(hsi_image, HSI) for hsi_image in hsi):\n            raise TypeError(\"All of the input hyperspectral images must be of type HSI\")\n\n        if not isinstance(baseline, list):\n            baseline = [baseline] * len(hsi)\n\n        baseline = torch.stack(\n            [\n                validate_and_transform_baseline(base, hsi_image).to(hsi_image.device)\n                for hsi_image, base in zip(hsi, baseline)\n            ],\n            dim=0,\n        )\n        input_tensor = torch.stack(\n            [hsi_image.get_image().requires_grad_(True).to(hsi_image.device) for hsi_image in hsi], dim=0\n        )\n\n        if isinstance(sliding_window_shapes, int):\n            sliding_window_shapes = (sliding_window_shapes, sliding_window_shapes, sliding_window_shapes)\n        if isinstance(strides, int):\n            strides = (strides, strides, strides)\n\n        if len(strides) != 3:\n            raise ValueError(\"Strides must be a tuple of three integers\")\n        if len(sliding_window_shapes) != 3:\n            raise ValueError(\"Sliding window shapes must be a tuple of three integers\")\n\n        assert len(sliding_window_shapes) == len(strides) == 3\n        occlusion_attributions = self._attribution_method.attribute(\n            input_tensor,\n            sliding_window_shapes=sliding_window_shapes,\n            strides=strides,\n            target=target,\n            baselines=baseline,\n            additional_forward_args=additional_forward_args,\n            perturbations_per_eval=min(perturbations_per_eval, len(hsi)),\n            show_progress=show_progress,\n        )\n\n        try:\n            attributes = [\n                HSIAttributes(hsi=hsi_image, attributes=attribution, attribution_method=self.get_name())\n                for hsi_image, attribution in zip(hsi, occlusion_attributions)\n            ]\n        except Exception as e:\n            raise HSIAttributesError(f\"Error in generating Occlusion attributions: {e}\") from e\n\n        return attributes[0] if len(attributes) == 1 else attributes\n\n    def get_spatial_attributes(\n        self,\n        hsi: list[HSI] | HSI,\n        target: list[int] | int | None = None,\n        sliding_window_shapes: int | tuple[int, int] = (1, 1),\n        strides: int | tuple[int, int] = 1,\n        baseline: int | float | torch.Tensor | list[int | float | torch.Tensor] = None,\n        additional_forward_args: Any = None,\n        perturbations_per_eval: int = 1,\n        show_progress: bool = False,\n    ) -&gt; HSISpatialAttributes | list[HSISpatialAttributes]:\n        \"\"\"Compute spatial attributions for the input HSI using the Occlusion method. In this case, the sliding window\n        is applied to the spatial dimensions only.\n\n        Args:\n            hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n                If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n                The output will be a list of HSIAttributes objects.\n            target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n                methods assume that the output has only one class. If the output has multiple classes, the target index\n                must be provided. For multiple input images, a list of target indices can be provided, one for each\n                image or single target value will be used for all images. Defaults to None.\n            sliding_window_shapes (int | tuple[int, int]): The shape of the sliding window for spatial dimensions.\n                If an integer is provided, it will be used for both spatial dimensions. Defaults to (1, 1).\n            strides (int | tuple[int, int], optional): The stride of the sliding window for spatial dimensions.\n                Defaults to 1. Simply put, the stride is the number of pixels by which the sliding window is moved\n                in each spatial dimension.\n            baseline (int | float | torch.Tensor | list[int | float | torch.Tensor], optional): Baselines define\n                reference value which replaces each feature when occluded is computed and can be provided as:\n                    - integer or float representing a constant value used as the baseline for all input pixels.\n                    - tensor with the same shape as the input tensor, providing a baseline for each input pixel.\n                        if the input is a list of HSI objects, the baseline can be a tensor with the same shape as\n                        the input tensor for each HSI object.\n                    - list of integers, floats or tensors with the same shape as the input tensor, providing a baseline\n                      for each input pixel. If the input is a list of HSI objects, the baseline can be a list of\n                      tensors with the same shape as the input tensor for each HSI object. Defaults to None.\n            additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n                the inputs for which attributions should not be computed, this argument can be provided.\n                It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n                containing multiple additional arguments including tensors or any arbitrary python types.\n                These arguments are provided to forward_func in order following the arguments in inputs.\n                Note that attributions are not computed with respect to these arguments. Default: None\n            perturbations_per_eval (int, optional): Allows multiple occlusions to be included in one batch\n                (one call to forward_fn). By default, perturbations_per_eval is 1, so each occlusion is processed\n                individually. Each forward pass will contain a maximum of perturbations_per_eval * #examples samples.\n                For DataParallel models, each batch is split among the available devices, so evaluations on each\n                available device contain at most (perturbations_per_eval * #examples) / num_devices samples. When\n                working with multiple examples, the number of perturbations per evaluation should be set to at least\n                the number of examples. Defaults to 1.\n            show_progress (bool, optional): If True, displays a progress bar. Defaults to False.\n\n        Returns:\n            HSISpatialAttributes | list[HSISpatialAttributes]: The computed attributions for the input hyperspectral image(s).\n                if a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n\n        Raises:\n            RuntimeError: If the explainer is not initialized.\n            ValueError: If the sliding window shapes or strides are not a tuple of two integers.\n            HSIAttributesError: If an error occurs during the generation of the attributions\n\n        Example:\n            &gt;&gt;&gt; occlusion = Occlusion(explainable_model)\n            &gt;&gt;&gt; hsi = HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n            &gt;&gt;&gt; attributions = occlusion.get_spatial_attributes(hsi, baseline=0, sliding_window_shapes=(3, 3), strides=(1, 1))\n            &gt;&gt;&gt; attributions = occlusion.get_spatial_attributes([hsi, hsi], baseline=0, sliding_window_shapes=(3, 3), strides=(2, 2))\n            &gt;&gt;&gt; len(attributions)\n            2\n        \"\"\"\n        if self._attribution_method is None:\n            raise RuntimeError(\"Occlusion explainer is not initialized, INITIALIZATION ERROR\")\n\n        if not isinstance(hsi, list):\n            hsi = [hsi]\n\n        if not all(isinstance(hsi_image, HSI) for hsi_image in hsi):\n            raise TypeError(\"All of the input hyperspectral images must be of type HSI\")\n\n        if not isinstance(baseline, list):\n            baseline = [baseline] * len(hsi)\n\n        baseline = torch.stack(\n            [\n                validate_and_transform_baseline(base, hsi_image).to(hsi_image.device)\n                for hsi_image, base in zip(hsi, baseline)\n            ],\n            dim=0,\n        )\n        input_tensor = torch.stack(\n            [hsi_image.get_image().requires_grad_(True).to(hsi_image.device) for hsi_image in hsi], dim=0\n        )\n\n        if isinstance(sliding_window_shapes, int):\n            sliding_window_shapes = (sliding_window_shapes, sliding_window_shapes)\n        if isinstance(strides, int):\n            strides = (strides, strides)\n\n        if len(strides) != 2:\n            raise ValueError(\"Strides must be a tuple of two integers\")\n        if len(sliding_window_shapes) != 2:\n            raise ValueError(\"Sliding window shapes must be a tuple of two integers\")\n\n        list_sliding_window_shapes = list(sliding_window_shapes)\n        list_strides = list(strides)\n        if isinstance(hsi, list):\n            list_sliding_window_shapes.insert(hsi[0].spectral_axis, hsi[0].image.shape[hsi[0].spectral_axis])\n            list_strides.insert(hsi[0].spectral_axis, hsi[0].image.shape[hsi[0].spectral_axis])\n        else:\n            list_sliding_window_shapes.insert(hsi.spectral_axis, hsi.image.shape[hsi.spectral_axis])\n            list_strides.insert(hsi.spectral_axis, hsi.image.shape[hsi.spectral_axis])\n        sliding_window_shapes = tuple(list_sliding_window_shapes)  # type: ignore\n        strides = tuple(list_strides)  # type: ignore\n\n        assert len(sliding_window_shapes) == len(strides) == 3\n        segment_mask = [\n            self._create_segmentation_mask(hsi_image.image.shape, sliding_window_shapes, strides) for hsi_image in hsi\n        ]\n\n        occlusion_attributions = self._attribution_method.attribute(\n            input_tensor,\n            sliding_window_shapes=sliding_window_shapes,\n            strides=strides,\n            target=target,\n            baselines=baseline,\n            additional_forward_args=additional_forward_args,\n            perturbations_per_eval=min(perturbations_per_eval, len(hsi)),\n            show_progress=show_progress,\n        )\n\n        try:\n            spatial_attributes = [\n                HSISpatialAttributes(\n                    hsi=hsi_image, attributes=attribution, attribution_method=self.get_name(), mask=mask\n                )\n                for hsi_image, attribution, mask in zip(hsi, occlusion_attributions, segment_mask)\n            ]\n        except Exception as e:\n            raise HSIAttributesError(f\"Error in generating Occlusion attributions: {e}\") from e\n\n        return spatial_attributes[0] if len(spatial_attributes) == 1 else spatial_attributes\n\n    def get_spectral_attributes(\n        self,\n        hsi: list[HSI] | HSI,\n        target: list[int] | int | None = None,\n        sliding_window_shapes: int | tuple[int] = 1,\n        strides: int | tuple[int] = 1,\n        baseline: int | float | torch.Tensor | list[int | float | torch.Tensor] = None,\n        additional_forward_args: Any = None,\n        perturbations_per_eval: int = 1,\n        show_progress: bool = False,\n    ) -&gt; HSISpectralAttributes | list[HSISpectralAttributes]:\n        \"\"\"Compute spectral attributions for the input HSI using the Occlusion method. In this case, the sliding window\n        is applied to the spectral dimension only.\n\n        Args:\n            hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n                If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n                The output will be a list of HSIAttributes objects.\n            target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n                methods assume that the output has only one class. If the output has multiple classes, the target index\n                must be provided. For multiple input images, a list of target indices can be provided, one for each\n                image or single target value will be used for all images. Defaults to None.\n            sliding_window_shapes (int | tuple[int]): The size of the sliding window for the spectral dimension.\n                Defaults to 1.\n            strides (int | tuple[int], optional): The stride of the sliding window for the spectral dimension.\n                Defaults to 1. Simply put, the stride is the number of pixels by which the sliding window is moved\n                in spectral dimension.\n            baseline (int | float | torch.Tensor | list[int | float | torch.Tensor], optional): Baselines define\n                reference value which replaces each feature when occluded is computed and can be provided as:\n                    - integer or float representing a constant value used as the baseline for all input pixels.\n                    - tensor with the same shape as the input tensor, providing a baseline for each input pixel.\n                        if the input is a list of HSI objects, the baseline can be a tensor with the same shape as\n                        the input tensor for each HSI object.\n                    - list of integers, floats or tensors with the same shape as the input tensor, providing a baseline\n                      for each input pixel. If the input is a list of HSI objects, the baseline can be a list of\n                      tensors with the same shape as the input tensor for each HSI object. Defaults to None.\n            additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n                the inputs for which attributions should not be computed, this argument can be provided.\n                It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n                containing multiple additional arguments including tensors or any arbitrary python types.\n                These arguments are provided to forward_func in order following the arguments in inputs.\n                Note that attributions are not computed with respect to these arguments. Default: None\n            perturbations_per_eval (int, optional): Allows multiple occlusions to be included in one batch\n                (one call to forward_fn). By default, perturbations_per_eval is 1, so each occlusion is processed\n                individually. Each forward pass will contain a maximum of perturbations_per_eval * #examples samples.\n                For DataParallel models, each batch is split among the available devices, so evaluations on each\n                available device contain at most (perturbations_per_eval * #examples) / num_devices samples. When\n                working with multiple examples, the number of perturbations per evaluation should be set to at least\n                the number of examples. Defaults to 1.\n            show_progress (bool, optional): If True, displays a progress bar. Defaults to False.\n\n        Returns:\n            HSISpectralAttributes | list[HSISpectralAttributes]: The computed attributions for the input hyperspectral\n                image(s). if a list of HSI objects is provided, the attributions are computed for each HSI object in\n                the list.\n\n        Raises:\n            RuntimeError: If the explainer is not initialized.\n            ValueError: If the sliding window shapes or strides are not a tuple of a single integer.\n            TypeError: If the sliding window shapes or strides are not a single integer.\n            HSIAttributesError: If an error occurs during the generation of the attributions\n\n        Example:\n            &gt;&gt;&gt; occlusion = Occlusion(explainable_model)\n            &gt;&gt;&gt; hsi = HSI(image=torch.ones((10, 240, 240)), wavelengths=torch.arange(10))\n            &gt;&gt;&gt; attributions = occlusion.get_spectral_attributes(hsi, baseline=0, sliding_window_shapes=3, strides=1)\n            &gt;&gt;&gt; attributions = occlusion.get_spectral_attributes([hsi, hsi], baseline=0, sliding_window_shapes=3, strides=2)\n            &gt;&gt;&gt; len(attributions)\n            2\n        \"\"\"\n        if self._attribution_method is None:\n            raise RuntimeError(\"Occlusion explainer is not initialized, INITIALIZATION ERROR\")\n\n        if not isinstance(hsi, list):\n            hsi = [hsi]\n\n        if not all(isinstance(hsi_image, HSI) for hsi_image in hsi):\n            raise TypeError(\"All of the input hyperspectral images must be of type HSI\")\n\n        if not isinstance(baseline, list):\n            baseline = [baseline] * len(hsi)\n\n        baseline = torch.stack(\n            [\n                validate_and_transform_baseline(base, hsi_image).to(hsi_image.device)\n                for hsi_image, base in zip(hsi, baseline)\n            ],\n            dim=0,\n        )\n        input_tensor = torch.stack(\n            [hsi_image.get_image().requires_grad_(True).to(hsi_image.device) for hsi_image in hsi], dim=0\n        )\n\n        if isinstance(sliding_window_shapes, tuple):\n            if len(sliding_window_shapes) != 1:\n                raise ValueError(\"Sliding window shapes must be a single integer or a tuple of a single integer\")\n            sliding_window_shapes = sliding_window_shapes[0]\n        if isinstance(strides, tuple):\n            if len(strides) != 1:\n                raise ValueError(\"Strides must be a single integer or a tuple of a single integer\")\n            strides = strides[0]\n\n        if not isinstance(sliding_window_shapes, int):\n            raise TypeError(\"Sliding window shapes must be a single integer\")\n        if not isinstance(strides, int):\n            raise TypeError(\"Strides must be a single integer\")\n\n        if isinstance(hsi, list):\n            full_sliding_window_shapes = list(hsi[0].image.shape)\n            full_sliding_window_shapes[hsi[0].spectral_axis] = sliding_window_shapes\n            full_strides = list(hsi[0].image.shape)\n            full_strides[hsi[0].spectral_axis] = strides\n        else:\n            full_sliding_window_shapes = list(hsi.image.shape)\n            full_sliding_window_shapes[hsi.spectral_axis] = sliding_window_shapes\n            full_strides = list(hsi.image.shape)\n            full_strides[hsi.spectral_axis] = strides\n\n        sliding_window_shapes = tuple(full_sliding_window_shapes)\n        strides = tuple(full_strides)\n\n        assert len(sliding_window_shapes) == len(strides) == 3\n        band_mask = [\n            self._create_segmentation_mask(hsi_image.image.shape, sliding_window_shapes, strides) for hsi_image in hsi\n        ]\n        band_names = {str(ui.item()): ui.item() for ui in torch.unique(band_mask[0])}\n\n        occlusion_attributions = self._attribution_method.attribute(\n            input_tensor,\n            sliding_window_shapes=sliding_window_shapes,\n            strides=strides,\n            target=target,\n            baselines=baseline,\n            additional_forward_args=additional_forward_args,\n            perturbations_per_eval=min(perturbations_per_eval, len(hsi)),\n            show_progress=show_progress,\n        )\n\n        try:\n            spectral_attributes = [\n                HSISpectralAttributes(\n                    hsi=hsi_image,\n                    attributes=attribution,\n                    attribution_method=self.get_name(),\n                    mask=mask,\n                    band_names=band_names,\n                )\n                for hsi_image, attribution, mask in zip(hsi, occlusion_attributions, band_mask)\n            ]\n        except Exception as e:\n            raise HSIAttributesError(f\"Error in generating Occlusion attributions: {e}\") from e\n\n        return spectral_attributes[0] if len(spectral_attributes) == 1 else spectral_attributes\n</code></pre>"},{"location":"reference/#src.meteors.attr.occlusion.Occlusion.attribute","title":"<code>attribute(hsi, target=None, sliding_window_shapes=(1, 1, 1), strides=(1, 1, 1), baseline=None, additional_forward_args=None, perturbations_per_eval=1, show_progress=False)</code>","text":"<p>Method for generating attributions using the Occlusion method.</p> <p>Parameters:</p> Name Type Description Default <code>hsi</code> <code>list[HSI] | HSI</code> <p>Input hyperspectral image(s) for which the attributions are to be computed. If a list of HSI objects is provided, the attributions are computed for each HSI object in the list. The output will be a list of HSIAttributes objects.</p> required <code>target</code> <code>list[int] | int | None</code> <p>target class index for computing the attributions. If None, methods assume that the output has only one class. If the output has multiple classes, the target index must be provided. For multiple input images, a list of target indices can be provided, one for each image or single target value will be used for all images. Defaults to None.</p> <code>None</code> <code>sliding_window_shapes</code> <code>int | tuple[int, int, int]</code> <p>The shape of the sliding window. If an integer is provided, it will be used for all dimensions. Defaults to (1, 1, 1).</p> <code>(1, 1, 1)</code> <code>strides</code> <code>int | tuple[int, int, int]</code> <p>The stride of the sliding window. Defaults to (1, 1, 1). Simply put, the stride is the number of pixels by which the sliding window is moved in each dimension.</p> <code>(1, 1, 1)</code> <code>baseline</code> <code>int | float | Tensor | list[int | float | Tensor]</code> <p>Baselines define reference value which replaces each feature when occluded is computed and can be provided as:     - integer or float representing a constant value used as the baseline for all input pixels.     - tensor with the same shape as the input tensor, providing a baseline for each input pixel.         if the input is a list of HSI objects, the baseline can be a tensor with the same shape as         the input tensor for each HSI object.     - list of integers, floats or tensors with the same shape as the input tensor, providing a baseline         for each input pixel. If the input is a list of HSI objects, the baseline can be a list of         tensors with the same shape as the input tensor for each HSI object. Defaults to None.</p> <code>None</code> <code>additional_forward_args</code> <code>Any</code> <p>If the forward function requires additional arguments other than the inputs for which attributions should not be computed, this argument can be provided. It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple containing multiple additional arguments including tensors or any arbitrary python types. These arguments are provided to forward_func in order following the arguments in inputs. Note that attributions are not computed with respect to these arguments. Default: None</p> <code>None</code> <code>perturbations_per_eval</code> <code>int</code> <p>Allows multiple occlusions to be included in one batch (one call to forward_fn). By default, perturbations_per_eval is 1, so each occlusion is processed individually. Each forward pass will contain a maximum of perturbations_per_eval * #examples samples. For DataParallel models, each batch is split among the available devices, so evaluations on each available device contain at most (perturbations_per_eval * #examples) / num_devices samples. When working with multiple examples, the number of perturbations per evaluation should be set to at least the number of examples. Defaults to 1.</p> <code>1</code> <code>show_progress</code> <code>bool</code> <p>If True, displays a progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>HSIAttributes</code> <code>HSIAttributes | list[HSIAttributes]</code> <p>The computed attributions for the input hyperspectral image(s). if a list of HSI objects is provided, the attributions are computed for each HSI object in the list.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the explainer is not initialized.</p> <code>ValueError</code> <p>If the sliding window shapes or strides are not a tuple of three integers.</p> <code>HSIAttributesError</code> <p>If an error occurs during the generation of the attributions.</p> Example <p>occlusion = Occlusion(explainable_model) hsi = HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68]) attributions = occlusion.attribute(hsi, baseline=0, sliding_window_shapes=(4, 3, 3), strides=(1, 1, 1)) attributions = occlusion.attribute([hsi, hsi], baseline=0, sliding_window_shapes=(4, 3, 3), strides=(1, 2, 2)) len(attributions) 2</p> Source code in <code>src/meteors/attr/occlusion.py</code> <pre><code>def attribute(\n    self,\n    hsi: list[HSI] | HSI,\n    target: list[int] | int | None = None,\n    sliding_window_shapes: int | tuple[int, int, int] = (1, 1, 1),\n    strides: int | tuple[int, int, int] = (1, 1, 1),\n    baseline: int | float | torch.Tensor | list[int | float | torch.Tensor] = None,\n    additional_forward_args: Any = None,\n    perturbations_per_eval: int = 1,\n    show_progress: bool = False,\n) -&gt; HSIAttributes | list[HSIAttributes]:\n    \"\"\"\n    Method for generating attributions using the Occlusion method.\n\n    Args:\n        hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n            If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n            The output will be a list of HSIAttributes objects.\n        target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n            methods assume that the output has only one class. If the output has multiple classes, the target index\n            must be provided. For multiple input images, a list of target indices can be provided, one for each\n            image or single target value will be used for all images. Defaults to None.\n        sliding_window_shapes (int | tuple[int, int, int]):\n            The shape of the sliding window. If an integer is provided, it will be used for all dimensions.\n            Defaults to (1, 1, 1).\n        strides (int | tuple[int, int, int], optional): The stride of the sliding window. Defaults to (1, 1, 1).\n            Simply put, the stride is the number of pixels by which the sliding window is moved in each dimension.\n        baseline (int | float | torch.Tensor | list[int | float | torch.Tensor], optional): Baselines define\n            reference value which replaces each feature when occluded is computed and can be provided as:\n                - integer or float representing a constant value used as the baseline for all input pixels.\n                - tensor with the same shape as the input tensor, providing a baseline for each input pixel.\n                    if the input is a list of HSI objects, the baseline can be a tensor with the same shape as\n                    the input tensor for each HSI object.\n                - list of integers, floats or tensors with the same shape as the input tensor, providing a baseline\n                    for each input pixel. If the input is a list of HSI objects, the baseline can be a list of\n                    tensors with the same shape as the input tensor for each HSI object. Defaults to None.\n        additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n            the inputs for which attributions should not be computed, this argument can be provided.\n            It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n            containing multiple additional arguments including tensors or any arbitrary python types.\n            These arguments are provided to forward_func in order following the arguments in inputs.\n            Note that attributions are not computed with respect to these arguments. Default: None\n        perturbations_per_eval (int, optional): Allows multiple occlusions to be included in one batch\n            (one call to forward_fn). By default, perturbations_per_eval is 1, so each occlusion is processed\n            individually. Each forward pass will contain a maximum of perturbations_per_eval * #examples samples.\n            For DataParallel models, each batch is split among the available devices, so evaluations on each\n            available device contain at most (perturbations_per_eval * #examples) / num_devices samples. When\n            working with multiple examples, the number of perturbations per evaluation should be set to at least\n            the number of examples. Defaults to 1.\n        show_progress (bool, optional): If True, displays a progress bar. Defaults to False.\n\n    Returns:\n        HSIAttributes: The computed attributions for the input hyperspectral image(s). if a list of HSI objects\n            is provided, the attributions are computed for each HSI object in the list.\n\n    Raises:\n        RuntimeError: If the explainer is not initialized.\n        ValueError: If the sliding window shapes or strides are not a tuple of three integers.\n        HSIAttributesError: If an error occurs during the generation of the attributions.\n\n    Example:\n        &gt;&gt;&gt; occlusion = Occlusion(explainable_model)\n        &gt;&gt;&gt; hsi = HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n        &gt;&gt;&gt; attributions = occlusion.attribute(hsi, baseline=0, sliding_window_shapes=(4, 3, 3), strides=(1, 1, 1))\n        &gt;&gt;&gt; attributions = occlusion.attribute([hsi, hsi], baseline=0, sliding_window_shapes=(4, 3, 3), strides=(1, 2, 2))\n        &gt;&gt;&gt; len(attributions)\n        2\n    \"\"\"\n    if self._attribution_method is None:\n        raise RuntimeError(\"Occlusion explainer is not initialized, INITIALIZATION ERROR\")\n\n    if not isinstance(hsi, list):\n        hsi = [hsi]\n\n    if not all(isinstance(hsi_image, HSI) for hsi_image in hsi):\n        raise TypeError(\"All of the input hyperspectral images must be of type HSI\")\n\n    if not isinstance(baseline, list):\n        baseline = [baseline] * len(hsi)\n\n    baseline = torch.stack(\n        [\n            validate_and_transform_baseline(base, hsi_image).to(hsi_image.device)\n            for hsi_image, base in zip(hsi, baseline)\n        ],\n        dim=0,\n    )\n    input_tensor = torch.stack(\n        [hsi_image.get_image().requires_grad_(True).to(hsi_image.device) for hsi_image in hsi], dim=0\n    )\n\n    if isinstance(sliding_window_shapes, int):\n        sliding_window_shapes = (sliding_window_shapes, sliding_window_shapes, sliding_window_shapes)\n    if isinstance(strides, int):\n        strides = (strides, strides, strides)\n\n    if len(strides) != 3:\n        raise ValueError(\"Strides must be a tuple of three integers\")\n    if len(sliding_window_shapes) != 3:\n        raise ValueError(\"Sliding window shapes must be a tuple of three integers\")\n\n    assert len(sliding_window_shapes) == len(strides) == 3\n    occlusion_attributions = self._attribution_method.attribute(\n        input_tensor,\n        sliding_window_shapes=sliding_window_shapes,\n        strides=strides,\n        target=target,\n        baselines=baseline,\n        additional_forward_args=additional_forward_args,\n        perturbations_per_eval=min(perturbations_per_eval, len(hsi)),\n        show_progress=show_progress,\n    )\n\n    try:\n        attributes = [\n            HSIAttributes(hsi=hsi_image, attributes=attribution, attribution_method=self.get_name())\n            for hsi_image, attribution in zip(hsi, occlusion_attributions)\n        ]\n    except Exception as e:\n        raise HSIAttributesError(f\"Error in generating Occlusion attributions: {e}\") from e\n\n    return attributes[0] if len(attributes) == 1 else attributes\n</code></pre>"},{"location":"reference/#src.meteors.attr.occlusion.Occlusion.get_spatial_attributes","title":"<code>get_spatial_attributes(hsi, target=None, sliding_window_shapes=(1, 1), strides=1, baseline=None, additional_forward_args=None, perturbations_per_eval=1, show_progress=False)</code>","text":"<p>Compute spatial attributions for the input HSI using the Occlusion method. In this case, the sliding window is applied to the spatial dimensions only.</p> <p>Parameters:</p> Name Type Description Default <code>hsi</code> <code>list[HSI] | HSI</code> <p>Input hyperspectral image(s) for which the attributions are to be computed. If a list of HSI objects is provided, the attributions are computed for each HSI object in the list. The output will be a list of HSIAttributes objects.</p> required <code>target</code> <code>list[int] | int | None</code> <p>target class index for computing the attributions. If None, methods assume that the output has only one class. If the output has multiple classes, the target index must be provided. For multiple input images, a list of target indices can be provided, one for each image or single target value will be used for all images. Defaults to None.</p> <code>None</code> <code>sliding_window_shapes</code> <code>int | tuple[int, int]</code> <p>The shape of the sliding window for spatial dimensions. If an integer is provided, it will be used for both spatial dimensions. Defaults to (1, 1).</p> <code>(1, 1)</code> <code>strides</code> <code>int | tuple[int, int]</code> <p>The stride of the sliding window for spatial dimensions. Defaults to 1. Simply put, the stride is the number of pixels by which the sliding window is moved in each spatial dimension.</p> <code>1</code> <code>baseline</code> <code>int | float | Tensor | list[int | float | Tensor]</code> <p>Baselines define reference value which replaces each feature when occluded is computed and can be provided as:     - integer or float representing a constant value used as the baseline for all input pixels.     - tensor with the same shape as the input tensor, providing a baseline for each input pixel.         if the input is a list of HSI objects, the baseline can be a tensor with the same shape as         the input tensor for each HSI object.     - list of integers, floats or tensors with the same shape as the input tensor, providing a baseline       for each input pixel. If the input is a list of HSI objects, the baseline can be a list of       tensors with the same shape as the input tensor for each HSI object. Defaults to None.</p> <code>None</code> <code>additional_forward_args</code> <code>Any</code> <p>If the forward function requires additional arguments other than the inputs for which attributions should not be computed, this argument can be provided. It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple containing multiple additional arguments including tensors or any arbitrary python types. These arguments are provided to forward_func in order following the arguments in inputs. Note that attributions are not computed with respect to these arguments. Default: None</p> <code>None</code> <code>perturbations_per_eval</code> <code>int</code> <p>Allows multiple occlusions to be included in one batch (one call to forward_fn). By default, perturbations_per_eval is 1, so each occlusion is processed individually. Each forward pass will contain a maximum of perturbations_per_eval * #examples samples. For DataParallel models, each batch is split among the available devices, so evaluations on each available device contain at most (perturbations_per_eval * #examples) / num_devices samples. When working with multiple examples, the number of perturbations per evaluation should be set to at least the number of examples. Defaults to 1.</p> <code>1</code> <code>show_progress</code> <code>bool</code> <p>If True, displays a progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>HSISpatialAttributes | list[HSISpatialAttributes]</code> <p>HSISpatialAttributes | list[HSISpatialAttributes]: The computed attributions for the input hyperspectral image(s). if a list of HSI objects is provided, the attributions are computed for each HSI object in the list.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the explainer is not initialized.</p> <code>ValueError</code> <p>If the sliding window shapes or strides are not a tuple of two integers.</p> <code>HSIAttributesError</code> <p>If an error occurs during the generation of the attributions</p> Example <p>occlusion = Occlusion(explainable_model) hsi = HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68]) attributions = occlusion.get_spatial_attributes(hsi, baseline=0, sliding_window_shapes=(3, 3), strides=(1, 1)) attributions = occlusion.get_spatial_attributes([hsi, hsi], baseline=0, sliding_window_shapes=(3, 3), strides=(2, 2)) len(attributions) 2</p> Source code in <code>src/meteors/attr/occlusion.py</code> <pre><code>def get_spatial_attributes(\n    self,\n    hsi: list[HSI] | HSI,\n    target: list[int] | int | None = None,\n    sliding_window_shapes: int | tuple[int, int] = (1, 1),\n    strides: int | tuple[int, int] = 1,\n    baseline: int | float | torch.Tensor | list[int | float | torch.Tensor] = None,\n    additional_forward_args: Any = None,\n    perturbations_per_eval: int = 1,\n    show_progress: bool = False,\n) -&gt; HSISpatialAttributes | list[HSISpatialAttributes]:\n    \"\"\"Compute spatial attributions for the input HSI using the Occlusion method. In this case, the sliding window\n    is applied to the spatial dimensions only.\n\n    Args:\n        hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n            If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n            The output will be a list of HSIAttributes objects.\n        target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n            methods assume that the output has only one class. If the output has multiple classes, the target index\n            must be provided. For multiple input images, a list of target indices can be provided, one for each\n            image or single target value will be used for all images. Defaults to None.\n        sliding_window_shapes (int | tuple[int, int]): The shape of the sliding window for spatial dimensions.\n            If an integer is provided, it will be used for both spatial dimensions. Defaults to (1, 1).\n        strides (int | tuple[int, int], optional): The stride of the sliding window for spatial dimensions.\n            Defaults to 1. Simply put, the stride is the number of pixels by which the sliding window is moved\n            in each spatial dimension.\n        baseline (int | float | torch.Tensor | list[int | float | torch.Tensor], optional): Baselines define\n            reference value which replaces each feature when occluded is computed and can be provided as:\n                - integer or float representing a constant value used as the baseline for all input pixels.\n                - tensor with the same shape as the input tensor, providing a baseline for each input pixel.\n                    if the input is a list of HSI objects, the baseline can be a tensor with the same shape as\n                    the input tensor for each HSI object.\n                - list of integers, floats or tensors with the same shape as the input tensor, providing a baseline\n                  for each input pixel. If the input is a list of HSI objects, the baseline can be a list of\n                  tensors with the same shape as the input tensor for each HSI object. Defaults to None.\n        additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n            the inputs for which attributions should not be computed, this argument can be provided.\n            It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n            containing multiple additional arguments including tensors or any arbitrary python types.\n            These arguments are provided to forward_func in order following the arguments in inputs.\n            Note that attributions are not computed with respect to these arguments. Default: None\n        perturbations_per_eval (int, optional): Allows multiple occlusions to be included in one batch\n            (one call to forward_fn). By default, perturbations_per_eval is 1, so each occlusion is processed\n            individually. Each forward pass will contain a maximum of perturbations_per_eval * #examples samples.\n            For DataParallel models, each batch is split among the available devices, so evaluations on each\n            available device contain at most (perturbations_per_eval * #examples) / num_devices samples. When\n            working with multiple examples, the number of perturbations per evaluation should be set to at least\n            the number of examples. Defaults to 1.\n        show_progress (bool, optional): If True, displays a progress bar. Defaults to False.\n\n    Returns:\n        HSISpatialAttributes | list[HSISpatialAttributes]: The computed attributions for the input hyperspectral image(s).\n            if a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n\n    Raises:\n        RuntimeError: If the explainer is not initialized.\n        ValueError: If the sliding window shapes or strides are not a tuple of two integers.\n        HSIAttributesError: If an error occurs during the generation of the attributions\n\n    Example:\n        &gt;&gt;&gt; occlusion = Occlusion(explainable_model)\n        &gt;&gt;&gt; hsi = HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n        &gt;&gt;&gt; attributions = occlusion.get_spatial_attributes(hsi, baseline=0, sliding_window_shapes=(3, 3), strides=(1, 1))\n        &gt;&gt;&gt; attributions = occlusion.get_spatial_attributes([hsi, hsi], baseline=0, sliding_window_shapes=(3, 3), strides=(2, 2))\n        &gt;&gt;&gt; len(attributions)\n        2\n    \"\"\"\n    if self._attribution_method is None:\n        raise RuntimeError(\"Occlusion explainer is not initialized, INITIALIZATION ERROR\")\n\n    if not isinstance(hsi, list):\n        hsi = [hsi]\n\n    if not all(isinstance(hsi_image, HSI) for hsi_image in hsi):\n        raise TypeError(\"All of the input hyperspectral images must be of type HSI\")\n\n    if not isinstance(baseline, list):\n        baseline = [baseline] * len(hsi)\n\n    baseline = torch.stack(\n        [\n            validate_and_transform_baseline(base, hsi_image).to(hsi_image.device)\n            for hsi_image, base in zip(hsi, baseline)\n        ],\n        dim=0,\n    )\n    input_tensor = torch.stack(\n        [hsi_image.get_image().requires_grad_(True).to(hsi_image.device) for hsi_image in hsi], dim=0\n    )\n\n    if isinstance(sliding_window_shapes, int):\n        sliding_window_shapes = (sliding_window_shapes, sliding_window_shapes)\n    if isinstance(strides, int):\n        strides = (strides, strides)\n\n    if len(strides) != 2:\n        raise ValueError(\"Strides must be a tuple of two integers\")\n    if len(sliding_window_shapes) != 2:\n        raise ValueError(\"Sliding window shapes must be a tuple of two integers\")\n\n    list_sliding_window_shapes = list(sliding_window_shapes)\n    list_strides = list(strides)\n    if isinstance(hsi, list):\n        list_sliding_window_shapes.insert(hsi[0].spectral_axis, hsi[0].image.shape[hsi[0].spectral_axis])\n        list_strides.insert(hsi[0].spectral_axis, hsi[0].image.shape[hsi[0].spectral_axis])\n    else:\n        list_sliding_window_shapes.insert(hsi.spectral_axis, hsi.image.shape[hsi.spectral_axis])\n        list_strides.insert(hsi.spectral_axis, hsi.image.shape[hsi.spectral_axis])\n    sliding_window_shapes = tuple(list_sliding_window_shapes)  # type: ignore\n    strides = tuple(list_strides)  # type: ignore\n\n    assert len(sliding_window_shapes) == len(strides) == 3\n    segment_mask = [\n        self._create_segmentation_mask(hsi_image.image.shape, sliding_window_shapes, strides) for hsi_image in hsi\n    ]\n\n    occlusion_attributions = self._attribution_method.attribute(\n        input_tensor,\n        sliding_window_shapes=sliding_window_shapes,\n        strides=strides,\n        target=target,\n        baselines=baseline,\n        additional_forward_args=additional_forward_args,\n        perturbations_per_eval=min(perturbations_per_eval, len(hsi)),\n        show_progress=show_progress,\n    )\n\n    try:\n        spatial_attributes = [\n            HSISpatialAttributes(\n                hsi=hsi_image, attributes=attribution, attribution_method=self.get_name(), mask=mask\n            )\n            for hsi_image, attribution, mask in zip(hsi, occlusion_attributions, segment_mask)\n        ]\n    except Exception as e:\n        raise HSIAttributesError(f\"Error in generating Occlusion attributions: {e}\") from e\n\n    return spatial_attributes[0] if len(spatial_attributes) == 1 else spatial_attributes\n</code></pre>"},{"location":"reference/#src.meteors.attr.occlusion.Occlusion.get_spectral_attributes","title":"<code>get_spectral_attributes(hsi, target=None, sliding_window_shapes=1, strides=1, baseline=None, additional_forward_args=None, perturbations_per_eval=1, show_progress=False)</code>","text":"<p>Compute spectral attributions for the input HSI using the Occlusion method. In this case, the sliding window is applied to the spectral dimension only.</p> <p>Parameters:</p> Name Type Description Default <code>hsi</code> <code>list[HSI] | HSI</code> <p>Input hyperspectral image(s) for which the attributions are to be computed. If a list of HSI objects is provided, the attributions are computed for each HSI object in the list. The output will be a list of HSIAttributes objects.</p> required <code>target</code> <code>list[int] | int | None</code> <p>target class index for computing the attributions. If None, methods assume that the output has only one class. If the output has multiple classes, the target index must be provided. For multiple input images, a list of target indices can be provided, one for each image or single target value will be used for all images. Defaults to None.</p> <code>None</code> <code>sliding_window_shapes</code> <code>int | tuple[int]</code> <p>The size of the sliding window for the spectral dimension. Defaults to 1.</p> <code>1</code> <code>strides</code> <code>int | tuple[int]</code> <p>The stride of the sliding window for the spectral dimension. Defaults to 1. Simply put, the stride is the number of pixels by which the sliding window is moved in spectral dimension.</p> <code>1</code> <code>baseline</code> <code>int | float | Tensor | list[int | float | Tensor]</code> <p>Baselines define reference value which replaces each feature when occluded is computed and can be provided as:     - integer or float representing a constant value used as the baseline for all input pixels.     - tensor with the same shape as the input tensor, providing a baseline for each input pixel.         if the input is a list of HSI objects, the baseline can be a tensor with the same shape as         the input tensor for each HSI object.     - list of integers, floats or tensors with the same shape as the input tensor, providing a baseline       for each input pixel. If the input is a list of HSI objects, the baseline can be a list of       tensors with the same shape as the input tensor for each HSI object. Defaults to None.</p> <code>None</code> <code>additional_forward_args</code> <code>Any</code> <p>If the forward function requires additional arguments other than the inputs for which attributions should not be computed, this argument can be provided. It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple containing multiple additional arguments including tensors or any arbitrary python types. These arguments are provided to forward_func in order following the arguments in inputs. Note that attributions are not computed with respect to these arguments. Default: None</p> <code>None</code> <code>perturbations_per_eval</code> <code>int</code> <p>Allows multiple occlusions to be included in one batch (one call to forward_fn). By default, perturbations_per_eval is 1, so each occlusion is processed individually. Each forward pass will contain a maximum of perturbations_per_eval * #examples samples. For DataParallel models, each batch is split among the available devices, so evaluations on each available device contain at most (perturbations_per_eval * #examples) / num_devices samples. When working with multiple examples, the number of perturbations per evaluation should be set to at least the number of examples. Defaults to 1.</p> <code>1</code> <code>show_progress</code> <code>bool</code> <p>If True, displays a progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>HSISpectralAttributes | list[HSISpectralAttributes]</code> <p>HSISpectralAttributes | list[HSISpectralAttributes]: The computed attributions for the input hyperspectral image(s). if a list of HSI objects is provided, the attributions are computed for each HSI object in the list.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the explainer is not initialized.</p> <code>ValueError</code> <p>If the sliding window shapes or strides are not a tuple of a single integer.</p> <code>TypeError</code> <p>If the sliding window shapes or strides are not a single integer.</p> <code>HSIAttributesError</code> <p>If an error occurs during the generation of the attributions</p> Example <p>occlusion = Occlusion(explainable_model) hsi = HSI(image=torch.ones((10, 240, 240)), wavelengths=torch.arange(10)) attributions = occlusion.get_spectral_attributes(hsi, baseline=0, sliding_window_shapes=3, strides=1) attributions = occlusion.get_spectral_attributes([hsi, hsi], baseline=0, sliding_window_shapes=3, strides=2) len(attributions) 2</p> Source code in <code>src/meteors/attr/occlusion.py</code> <pre><code>def get_spectral_attributes(\n    self,\n    hsi: list[HSI] | HSI,\n    target: list[int] | int | None = None,\n    sliding_window_shapes: int | tuple[int] = 1,\n    strides: int | tuple[int] = 1,\n    baseline: int | float | torch.Tensor | list[int | float | torch.Tensor] = None,\n    additional_forward_args: Any = None,\n    perturbations_per_eval: int = 1,\n    show_progress: bool = False,\n) -&gt; HSISpectralAttributes | list[HSISpectralAttributes]:\n    \"\"\"Compute spectral attributions for the input HSI using the Occlusion method. In this case, the sliding window\n    is applied to the spectral dimension only.\n\n    Args:\n        hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n            If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n            The output will be a list of HSIAttributes objects.\n        target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n            methods assume that the output has only one class. If the output has multiple classes, the target index\n            must be provided. For multiple input images, a list of target indices can be provided, one for each\n            image or single target value will be used for all images. Defaults to None.\n        sliding_window_shapes (int | tuple[int]): The size of the sliding window for the spectral dimension.\n            Defaults to 1.\n        strides (int | tuple[int], optional): The stride of the sliding window for the spectral dimension.\n            Defaults to 1. Simply put, the stride is the number of pixels by which the sliding window is moved\n            in spectral dimension.\n        baseline (int | float | torch.Tensor | list[int | float | torch.Tensor], optional): Baselines define\n            reference value which replaces each feature when occluded is computed and can be provided as:\n                - integer or float representing a constant value used as the baseline for all input pixels.\n                - tensor with the same shape as the input tensor, providing a baseline for each input pixel.\n                    if the input is a list of HSI objects, the baseline can be a tensor with the same shape as\n                    the input tensor for each HSI object.\n                - list of integers, floats or tensors with the same shape as the input tensor, providing a baseline\n                  for each input pixel. If the input is a list of HSI objects, the baseline can be a list of\n                  tensors with the same shape as the input tensor for each HSI object. Defaults to None.\n        additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n            the inputs for which attributions should not be computed, this argument can be provided.\n            It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n            containing multiple additional arguments including tensors or any arbitrary python types.\n            These arguments are provided to forward_func in order following the arguments in inputs.\n            Note that attributions are not computed with respect to these arguments. Default: None\n        perturbations_per_eval (int, optional): Allows multiple occlusions to be included in one batch\n            (one call to forward_fn). By default, perturbations_per_eval is 1, so each occlusion is processed\n            individually. Each forward pass will contain a maximum of perturbations_per_eval * #examples samples.\n            For DataParallel models, each batch is split among the available devices, so evaluations on each\n            available device contain at most (perturbations_per_eval * #examples) / num_devices samples. When\n            working with multiple examples, the number of perturbations per evaluation should be set to at least\n            the number of examples. Defaults to 1.\n        show_progress (bool, optional): If True, displays a progress bar. Defaults to False.\n\n    Returns:\n        HSISpectralAttributes | list[HSISpectralAttributes]: The computed attributions for the input hyperspectral\n            image(s). if a list of HSI objects is provided, the attributions are computed for each HSI object in\n            the list.\n\n    Raises:\n        RuntimeError: If the explainer is not initialized.\n        ValueError: If the sliding window shapes or strides are not a tuple of a single integer.\n        TypeError: If the sliding window shapes or strides are not a single integer.\n        HSIAttributesError: If an error occurs during the generation of the attributions\n\n    Example:\n        &gt;&gt;&gt; occlusion = Occlusion(explainable_model)\n        &gt;&gt;&gt; hsi = HSI(image=torch.ones((10, 240, 240)), wavelengths=torch.arange(10))\n        &gt;&gt;&gt; attributions = occlusion.get_spectral_attributes(hsi, baseline=0, sliding_window_shapes=3, strides=1)\n        &gt;&gt;&gt; attributions = occlusion.get_spectral_attributes([hsi, hsi], baseline=0, sliding_window_shapes=3, strides=2)\n        &gt;&gt;&gt; len(attributions)\n        2\n    \"\"\"\n    if self._attribution_method is None:\n        raise RuntimeError(\"Occlusion explainer is not initialized, INITIALIZATION ERROR\")\n\n    if not isinstance(hsi, list):\n        hsi = [hsi]\n\n    if not all(isinstance(hsi_image, HSI) for hsi_image in hsi):\n        raise TypeError(\"All of the input hyperspectral images must be of type HSI\")\n\n    if not isinstance(baseline, list):\n        baseline = [baseline] * len(hsi)\n\n    baseline = torch.stack(\n        [\n            validate_and_transform_baseline(base, hsi_image).to(hsi_image.device)\n            for hsi_image, base in zip(hsi, baseline)\n        ],\n        dim=0,\n    )\n    input_tensor = torch.stack(\n        [hsi_image.get_image().requires_grad_(True).to(hsi_image.device) for hsi_image in hsi], dim=0\n    )\n\n    if isinstance(sliding_window_shapes, tuple):\n        if len(sliding_window_shapes) != 1:\n            raise ValueError(\"Sliding window shapes must be a single integer or a tuple of a single integer\")\n        sliding_window_shapes = sliding_window_shapes[0]\n    if isinstance(strides, tuple):\n        if len(strides) != 1:\n            raise ValueError(\"Strides must be a single integer or a tuple of a single integer\")\n        strides = strides[0]\n\n    if not isinstance(sliding_window_shapes, int):\n        raise TypeError(\"Sliding window shapes must be a single integer\")\n    if not isinstance(strides, int):\n        raise TypeError(\"Strides must be a single integer\")\n\n    if isinstance(hsi, list):\n        full_sliding_window_shapes = list(hsi[0].image.shape)\n        full_sliding_window_shapes[hsi[0].spectral_axis] = sliding_window_shapes\n        full_strides = list(hsi[0].image.shape)\n        full_strides[hsi[0].spectral_axis] = strides\n    else:\n        full_sliding_window_shapes = list(hsi.image.shape)\n        full_sliding_window_shapes[hsi.spectral_axis] = sliding_window_shapes\n        full_strides = list(hsi.image.shape)\n        full_strides[hsi.spectral_axis] = strides\n\n    sliding_window_shapes = tuple(full_sliding_window_shapes)\n    strides = tuple(full_strides)\n\n    assert len(sliding_window_shapes) == len(strides) == 3\n    band_mask = [\n        self._create_segmentation_mask(hsi_image.image.shape, sliding_window_shapes, strides) for hsi_image in hsi\n    ]\n    band_names = {str(ui.item()): ui.item() for ui in torch.unique(band_mask[0])}\n\n    occlusion_attributions = self._attribution_method.attribute(\n        input_tensor,\n        sliding_window_shapes=sliding_window_shapes,\n        strides=strides,\n        target=target,\n        baselines=baseline,\n        additional_forward_args=additional_forward_args,\n        perturbations_per_eval=min(perturbations_per_eval, len(hsi)),\n        show_progress=show_progress,\n    )\n\n    try:\n        spectral_attributes = [\n            HSISpectralAttributes(\n                hsi=hsi_image,\n                attributes=attribution,\n                attribution_method=self.get_name(),\n                mask=mask,\n                band_names=band_names,\n            )\n            for hsi_image, attribution, mask in zip(hsi, occlusion_attributions, band_mask)\n        ]\n    except Exception as e:\n        raise HSIAttributesError(f\"Error in generating Occlusion attributions: {e}\") from e\n\n    return spectral_attributes[0] if len(spectral_attributes) == 1 else spectral_attributes\n</code></pre>"},{"location":"reference/#src.meteors.attr.saliency.Saliency","title":"<code>Saliency</code>","text":"<p>               Bases: <code>Explainer</code></p> <p>Saliency explainer class for generating attributions using the Saliency method. This baseline method for computing input attribution calculates gradients with respect to inputs. It also has an option to return the absolute value of the gradients, which is the default behaviour. Implementation of this method is based on the <code>captum</code> repository</p> <p>Attributes:</p> Name Type Description <code>_attribution_method</code> <code>Saliency</code> <p>The Saliency method from the <code>captum</code> library.</p> <p>Parameters:</p> Name Type Description Default <code>explainable_model</code> <code>ExplainableModel | Explainer</code> <p>The explainable model to be explained.</p> required Source code in <code>src/meteors/attr/saliency.py</code> <pre><code>class Saliency(Explainer):\n    \"\"\"\n    Saliency explainer class for generating attributions using the Saliency method.\n    This baseline method for computing input attribution calculates gradients with respect to inputs.\n    It also has an option to return the absolute value of the gradients, which is the default behaviour.\n    Implementation of this method is based on the [`captum` repository](https://captum.ai/api/saliency.html)\n\n    Attributes:\n        _attribution_method (CaptumSaliency): The Saliency method from the `captum` library.\n\n    Args:\n        explainable_model (ExplainableModel | Explainer): The explainable model to be explained.\n    \"\"\"\n\n    def __init__(self, explainable_model: ExplainableModel):\n        super().__init__(explainable_model)\n\n        self._attribution_method = CaptumSaliency(explainable_model.forward_func)\n\n    def attribute(\n        self,\n        hsi: list[HSI] | HSI,\n        target: list[int] | int | None = None,\n        abs: bool = True,\n        additional_forward_args: Any = None,\n    ) -&gt; HSIAttributes | list[HSIAttributes]:\n        \"\"\"\n        Method for generating attributions using the Saliency method.\n\n        Args:\n            hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n                If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n                The output will be a list of HSIAttributes objects.\n            target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n                methods assume that the output has only one class. If the output has multiple classes, the target index\n                must be provided. For multiple input images, a list of target indices can be provided, one for each\n                image or single target value will be used for all images. Defaults to None.\n            abs (bool, optional): Returns absolute value of gradients if set to True,\n                otherwise returns the (signed) gradients if False. Default: True\n            additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n                the inputs for which attributions should not be computed, this argument can be provided.\n                It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n                containing multiple additional arguments including tensors or any arbitrary python types.\n                These arguments are provided to forward_func in order following the arguments in inputs.\n                Note that attributions are not computed with respect to these arguments. Default: None\n\n        Returns:\n            HSIAttributes | list[HSIAttributes]: The computed attributions for the input hyperspectral image(s).\n                if a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n\n        Raises:\n            RuntimeError: If the explainer is not initialized.\n            HSIAttributesError: If an error occurs during the generation of the attributions\n\n        Examples:\n            &gt;&gt;&gt; saliency = Saliency(explainable_model)\n            &gt;&gt;&gt; hsi = HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n            &gt;&gt;&gt; attributions = saliency.attribute(hsi)\n            &gt;&gt;&gt; attributions = saliency.attribute([hsi, hsi])\n            &gt;&gt;&gt; len(attributions)\n            2\n        \"\"\"\n        if self._attribution_method is None:\n            raise RuntimeError(\"Saliency explainer is not initialized, INITIALIZATION ERROR\")\n\n        if not isinstance(hsi, list):\n            hsi = [hsi]\n\n        if not all(isinstance(hsi_image, HSI) for hsi_image in hsi):\n            raise TypeError(\"All of the input hyperspectral images must be of type HSI\")\n\n        input_tensor = torch.stack(\n            [hsi_image.get_image().requires_grad_(True).to(hsi_image.device) for hsi_image in hsi], dim=0\n        )\n\n        saliency_attributions = self._attribution_method.attribute(\n            input_tensor, target=target, abs=abs, additional_forward_args=additional_forward_args\n        )\n\n        try:\n            attributes = [\n                HSIAttributes(hsi=hsi_image, attributes=attribution, attribution_method=self.get_name())\n                for hsi_image, attribution in zip(hsi, saliency_attributions)\n            ]\n        except Exception as e:\n            raise HSIAttributesError(f\"Error in generating Saliency attributions: {e}\") from e\n\n        return attributes[0] if len(attributes) == 1 else attributes\n</code></pre>"},{"location":"reference/#src.meteors.attr.saliency.Saliency.attribute","title":"<code>attribute(hsi, target=None, abs=True, additional_forward_args=None)</code>","text":"<p>Method for generating attributions using the Saliency method.</p> <p>Parameters:</p> Name Type Description Default <code>hsi</code> <code>list[HSI] | HSI</code> <p>Input hyperspectral image(s) for which the attributions are to be computed. If a list of HSI objects is provided, the attributions are computed for each HSI object in the list. The output will be a list of HSIAttributes objects.</p> required <code>target</code> <code>list[int] | int | None</code> <p>target class index for computing the attributions. If None, methods assume that the output has only one class. If the output has multiple classes, the target index must be provided. For multiple input images, a list of target indices can be provided, one for each image or single target value will be used for all images. Defaults to None.</p> <code>None</code> <code>abs</code> <code>bool</code> <p>Returns absolute value of gradients if set to True, otherwise returns the (signed) gradients if False. Default: True</p> <code>True</code> <code>additional_forward_args</code> <code>Any</code> <p>If the forward function requires additional arguments other than the inputs for which attributions should not be computed, this argument can be provided. It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple containing multiple additional arguments including tensors or any arbitrary python types. These arguments are provided to forward_func in order following the arguments in inputs. Note that attributions are not computed with respect to these arguments. Default: None</p> <code>None</code> <p>Returns:</p> Type Description <code>HSIAttributes | list[HSIAttributes]</code> <p>HSIAttributes | list[HSIAttributes]: The computed attributions for the input hyperspectral image(s). if a list of HSI objects is provided, the attributions are computed for each HSI object in the list.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the explainer is not initialized.</p> <code>HSIAttributesError</code> <p>If an error occurs during the generation of the attributions</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; saliency = Saliency(explainable_model)\n&gt;&gt;&gt; hsi = HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n&gt;&gt;&gt; attributions = saliency.attribute(hsi)\n&gt;&gt;&gt; attributions = saliency.attribute([hsi, hsi])\n&gt;&gt;&gt; len(attributions)\n2\n</code></pre> Source code in <code>src/meteors/attr/saliency.py</code> <pre><code>def attribute(\n    self,\n    hsi: list[HSI] | HSI,\n    target: list[int] | int | None = None,\n    abs: bool = True,\n    additional_forward_args: Any = None,\n) -&gt; HSIAttributes | list[HSIAttributes]:\n    \"\"\"\n    Method for generating attributions using the Saliency method.\n\n    Args:\n        hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n            If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n            The output will be a list of HSIAttributes objects.\n        target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n            methods assume that the output has only one class. If the output has multiple classes, the target index\n            must be provided. For multiple input images, a list of target indices can be provided, one for each\n            image or single target value will be used for all images. Defaults to None.\n        abs (bool, optional): Returns absolute value of gradients if set to True,\n            otherwise returns the (signed) gradients if False. Default: True\n        additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n            the inputs for which attributions should not be computed, this argument can be provided.\n            It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n            containing multiple additional arguments including tensors or any arbitrary python types.\n            These arguments are provided to forward_func in order following the arguments in inputs.\n            Note that attributions are not computed with respect to these arguments. Default: None\n\n    Returns:\n        HSIAttributes | list[HSIAttributes]: The computed attributions for the input hyperspectral image(s).\n            if a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n\n    Raises:\n        RuntimeError: If the explainer is not initialized.\n        HSIAttributesError: If an error occurs during the generation of the attributions\n\n    Examples:\n        &gt;&gt;&gt; saliency = Saliency(explainable_model)\n        &gt;&gt;&gt; hsi = HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n        &gt;&gt;&gt; attributions = saliency.attribute(hsi)\n        &gt;&gt;&gt; attributions = saliency.attribute([hsi, hsi])\n        &gt;&gt;&gt; len(attributions)\n        2\n    \"\"\"\n    if self._attribution_method is None:\n        raise RuntimeError(\"Saliency explainer is not initialized, INITIALIZATION ERROR\")\n\n    if not isinstance(hsi, list):\n        hsi = [hsi]\n\n    if not all(isinstance(hsi_image, HSI) for hsi_image in hsi):\n        raise TypeError(\"All of the input hyperspectral images must be of type HSI\")\n\n    input_tensor = torch.stack(\n        [hsi_image.get_image().requires_grad_(True).to(hsi_image.device) for hsi_image in hsi], dim=0\n    )\n\n    saliency_attributions = self._attribution_method.attribute(\n        input_tensor, target=target, abs=abs, additional_forward_args=additional_forward_args\n    )\n\n    try:\n        attributes = [\n            HSIAttributes(hsi=hsi_image, attributes=attribution, attribution_method=self.get_name())\n            for hsi_image, attribution in zip(hsi, saliency_attributions)\n        ]\n    except Exception as e:\n        raise HSIAttributesError(f\"Error in generating Saliency attributions: {e}\") from e\n\n    return attributes[0] if len(attributes) == 1 else attributes\n</code></pre>"},{"location":"reference/#src.meteors.attr.noise_tunnel.NoiseTunnel","title":"<code>NoiseTunnel</code>","text":"<p>               Bases: <code>BaseNoiseTunnel</code></p> <p>Noise Tunnel is a method that is used to explain the model's predictions by adding noise to the input tensor. The noise is added to the input tensor, and the model's output is computed. The process is repeated multiple times to obtain a distribution of the model's output. The final attribution is computed as the mean of the outputs. For more information about the method, see <code>captum</code> documentation.</p> <p>Parameters:</p> Name Type Description Default <code>chained_explainer</code> <p>The explainable method that will be used to compute the attributions.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the callable object is not an instance of the Explainer class</p> Source code in <code>src/meteors/attr/noise_tunnel.py</code> <pre><code>class NoiseTunnel(BaseNoiseTunnel):\n    \"\"\"Noise Tunnel is a method that is used to explain the model's predictions by adding noise to the input tensor.\n    The noise is added to the input tensor, and the model's output is computed. The process is repeated multiple times\n    to obtain a distribution of the model's output. The final attribution is computed as the mean of the outputs.\n    For more information about the method, see [`captum` documentation](https://captum.ai/api/noise_tunnel.html).\n\n    Arguments:\n        chained_explainer: The explainable method that will be used to compute the attributions.\n\n    Raises:\n        RuntimeError: If the callable object is not an instance of the Explainer class\n    \"\"\"\n\n    @staticmethod\n    def perturb_input(\n        input: torch.Tensor,\n        n_samples: int = 1,\n        perturbation_axis: None | tuple[int | slice] = None,\n        stdevs: float = 1,\n        **kwargs: Any,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        The default perturbation function used in the noise tunnel with small enhancement for hyperspectral images.\n        It randomly adds noise to the input tensor from a normal distribution with a given standard deviation.\n        The noise is added to the selected bands (channels) of the input tensor.\n        The bands to be perturbed are selected based on the `perturbation_axis` parameter.\n        By default all bands are perturbed, which is equivalent to the standard noise tunnel method.\n\n        Args:\n            input (torch.Tensor): An input tensor to be perturbed. It should have the shape (C, H, W).\n            n_samples (int): A number of samples to be drawn - number of perturbed inputs to be generated.\n            perturbation_axis (None | tuple[int | slice]): The indices of the bands to be perturbed.\n                If set to None, all bands are perturbed. Defaults to None.\n            stdevs (float): The standard deviation of gaussian noise with zero mean that is added to each input\n                in the batch. Defaults to 1.0.\n\n        Returns:\n            torch.Tensor: A perturbed tensor, which contains `n_samples` perturbed inputs.\n        \"\"\"\n        if n_samples &lt; 1:\n            raise ValueError(\"Number of perturbated samples to be generated must be greater than 0\")\n\n        # the perturbation\n        perturbed_input = input.clone().unsqueeze(0)\n        # repeat the perturbed_input on the first dimension n_samples times\n        perturbed_input = perturbed_input.repeat_interleave(n_samples, dim=0)\n\n        # the perturbation shape\n        if perturbation_axis is None:\n            perturbation_shape = perturbed_input.shape\n        else:\n            perturbation_axis = (slice(None),) + perturbation_axis  # type: ignore\n            perturbation_shape = perturbed_input[perturbation_axis].shape\n\n        # the noise\n        noise = torch.normal(0, stdevs, size=perturbation_shape).to(input.device)\n\n        # add the noise to the perturbed_input\n        if perturbation_axis is None:\n            perturbed_input += noise\n        else:\n            perturbed_input[perturbation_axis] += noise\n\n        perturbed_input.requires_grad_(True)\n\n        return perturbed_input\n\n    def attribute(\n        self,\n        hsi: list[HSI] | HSI,\n        target: list[int] | int | None = None,\n        additional_forward_args: Any = None,\n        n_samples: int = 5,\n        steps_per_batch: int = 1,\n        perturbation_axis: None | tuple[int | slice] = None,\n        stdevs: float | tuple[float, ...] = 1.0,\n        method: Literal[\"smoothgrad\", \"smoothgrad_sq\", \"vargrad\"] = \"smoothgrad\",\n    ) -&gt; HSIAttributes | list[HSIAttributes]:\n        \"\"\"\n        Method for generating attributions using the Noise Tunnel method.\n\n        Args:\n            hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n                If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n                The output will be a list of HSIAttributes objects.\n            baseline (int | float | torch.Tensor, optional): Baselines define reference value which replaces each\n                feature when occluded is computed and can be provided as:\n                    - integer or float representing a constant value used as the baseline for all input pixels.\n                    - tensor with the same shape as the input tensor, providing a baseline for each input pixel.\n                        if the input is a list of HSI objects, the baseline can be a tensor with the same shape as\n                        the input tensor for each HSI object.\n            target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n                methods assume that the output has only one class. If the output has multiple classes, the target index\n                must be provided. For multiple input images, a list of target indices can be provided, one for each\n                image or single target value will be used for all images. Defaults to None.\n            additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n                the inputs for which attributions should not be computed, this argument can be provided.\n                It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n                containing multiple additional arguments including tensors or any arbitrary python types.\n                These arguments are provided to forward_func in order following the arguments in inputs.\n                Note that attributions are not computed with respect to these arguments. Default: None\n            n_samples (int, optional): The number of randomly generated examples per sample in the input batch.\n                Random examples are generated by adding gaussian random noise to each sample.\n                Default: 5 if nt_samples is not provided.\n            steps_per_batch (int, optional): The number of the n_samples that will be processed together.\n                With the help of this parameter we can avoid out of memory situation and reduce the number of randomly\n                generated examples per sample in each batch. Default: None if steps_per_batch is not provided.\n                In this case all nt_samples will be processed together.\n            perturbation_axis (None | tuple[int | slice], optional): The indices of the input image to be perturbed.\n                If set to None, all bands are perturbed, which corresponds to a traditional noise tunnel method.\n                Defaults to None.\n            stdevs (float | tuple[float, ...], optional): The standard deviation of gaussian noise with zero mean that\n                is added to each input in the batch. If stdevs is a single float value then that same value is used\n                for all inputs. If stdevs is a tuple, then the length of the tuple must match the number of inputs as\n                each value in the tuple is used for the corresponding input. Default: 1.0\n            method (Literal[\"smoothgrad\", \"smoothgrad_sq\", \"vargrad\"], optional): Smoothing type of the attributions.\n                smoothgrad, smoothgrad_sq or vargrad Default: smoothgrad if type is not provided.\n\n        Returns:\n            HSIAttributes | list[HSIAttributes]: The computed attributions for the input hyperspectral image(s).\n                if a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n\n        Raises:\n            HSIAttributesError: If an error occurs during the generation of the attributions.\n\n        Examples:\n            &gt;&gt;&gt; noise_tunnel = NoiseTunnel(explainable_model)\n            &gt;&gt;&gt; hsi = HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n            &gt;&gt;&gt; attributions = noise_tunnel.attribute(hsi)\n            &gt;&gt;&gt; attributions = noise_tunnel.attribute([hsi, hsi])\n            &gt;&gt;&gt; len(attributions)\n            2\n        \"\"\"\n        if isinstance(stdevs, list):\n            stdevs = tuple(stdevs)\n\n        if not isinstance(hsi, list):\n            hsi = [hsi]\n\n        if not all([isinstance(input, HSI) for input in hsi]):\n            raise TypeError(\"All inputs must be HSI objects\")\n\n        if isinstance(stdevs, tuple):\n            if len(stdevs) != len(hsi):\n                raise ValueError(\n                    \"The number of stdevs must match the number of input images, number of stdevs:\"\n                    f\"{len(stdevs)}, number of input images: {len(hsi)}\"\n                )\n        else:\n            stdevs = tuple([stdevs] * len(hsi))\n\n        if not isinstance(target, list):\n            target = [target] * len(hsi)  # type: ignore\n\n        nt_attributes = torch.empty((n_samples, len(hsi)) + hsi[0].image.shape, device=hsi[0].device)\n\n        for batch in range(0, len(hsi)):\n            input = hsi[batch]\n            targeted = target[batch]\n            stdev = stdevs[batch]\n            perturbed_input = self.perturb_input(input.image, n_samples, perturbation_axis, stdev)\n            nt_attributes[:, batch] = self._forward_loop(\n                perturbed_input, input, targeted, additional_forward_args, n_samples, steps_per_batch\n            )\n\n        nt_attributes = self._aggregate_attributions(nt_attributes, method)\n\n        try:\n            attributes = [\n                HSIAttributes(hsi=hsi_image, attributes=attribution, attribution_method=self.get_name())\n                for hsi_image, attribution in zip(hsi, nt_attributes)\n            ]\n        except Exception as e:\n            raise HSIAttributesError(f\"Error in generating NoiseTunnel attributions: {e}\") from e\n\n        return attributes[0] if len(attributes) == 1 else attributes\n</code></pre>"},{"location":"reference/#src.meteors.attr.noise_tunnel.NoiseTunnel.attribute","title":"<code>attribute(hsi, target=None, additional_forward_args=None, n_samples=5, steps_per_batch=1, perturbation_axis=None, stdevs=1.0, method='smoothgrad')</code>","text":"<p>Method for generating attributions using the Noise Tunnel method.</p> <p>Parameters:</p> Name Type Description Default <code>hsi</code> <code>list[HSI] | HSI</code> <p>Input hyperspectral image(s) for which the attributions are to be computed. If a list of HSI objects is provided, the attributions are computed for each HSI object in the list. The output will be a list of HSIAttributes objects.</p> required <code>baseline</code> <code>int | float | Tensor</code> <p>Baselines define reference value which replaces each feature when occluded is computed and can be provided as:     - integer or float representing a constant value used as the baseline for all input pixels.     - tensor with the same shape as the input tensor, providing a baseline for each input pixel.         if the input is a list of HSI objects, the baseline can be a tensor with the same shape as         the input tensor for each HSI object.</p> required <code>target</code> <code>list[int] | int | None</code> <p>target class index for computing the attributions. If None, methods assume that the output has only one class. If the output has multiple classes, the target index must be provided. For multiple input images, a list of target indices can be provided, one for each image or single target value will be used for all images. Defaults to None.</p> <code>None</code> <code>additional_forward_args</code> <code>Any</code> <p>If the forward function requires additional arguments other than the inputs for which attributions should not be computed, this argument can be provided. It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple containing multiple additional arguments including tensors or any arbitrary python types. These arguments are provided to forward_func in order following the arguments in inputs. Note that attributions are not computed with respect to these arguments. Default: None</p> <code>None</code> <code>n_samples</code> <code>int</code> <p>The number of randomly generated examples per sample in the input batch. Random examples are generated by adding gaussian random noise to each sample. Default: 5 if nt_samples is not provided.</p> <code>5</code> <code>steps_per_batch</code> <code>int</code> <p>The number of the n_samples that will be processed together. With the help of this parameter we can avoid out of memory situation and reduce the number of randomly generated examples per sample in each batch. Default: None if steps_per_batch is not provided. In this case all nt_samples will be processed together.</p> <code>1</code> <code>perturbation_axis</code> <code>None | tuple[int | slice]</code> <p>The indices of the input image to be perturbed. If set to None, all bands are perturbed, which corresponds to a traditional noise tunnel method. Defaults to None.</p> <code>None</code> <code>stdevs</code> <code>float | tuple[float, ...]</code> <p>The standard deviation of gaussian noise with zero mean that is added to each input in the batch. If stdevs is a single float value then that same value is used for all inputs. If stdevs is a tuple, then the length of the tuple must match the number of inputs as each value in the tuple is used for the corresponding input. Default: 1.0</p> <code>1.0</code> <code>method</code> <code>Literal['smoothgrad', 'smoothgrad_sq', 'vargrad']</code> <p>Smoothing type of the attributions. smoothgrad, smoothgrad_sq or vargrad Default: smoothgrad if type is not provided.</p> <code>'smoothgrad'</code> <p>Returns:</p> Type Description <code>HSIAttributes | list[HSIAttributes]</code> <p>HSIAttributes | list[HSIAttributes]: The computed attributions for the input hyperspectral image(s). if a list of HSI objects is provided, the attributions are computed for each HSI object in the list.</p> <p>Raises:</p> Type Description <code>HSIAttributesError</code> <p>If an error occurs during the generation of the attributions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; noise_tunnel = NoiseTunnel(explainable_model)\n&gt;&gt;&gt; hsi = HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n&gt;&gt;&gt; attributions = noise_tunnel.attribute(hsi)\n&gt;&gt;&gt; attributions = noise_tunnel.attribute([hsi, hsi])\n&gt;&gt;&gt; len(attributions)\n2\n</code></pre> Source code in <code>src/meteors/attr/noise_tunnel.py</code> <pre><code>def attribute(\n    self,\n    hsi: list[HSI] | HSI,\n    target: list[int] | int | None = None,\n    additional_forward_args: Any = None,\n    n_samples: int = 5,\n    steps_per_batch: int = 1,\n    perturbation_axis: None | tuple[int | slice] = None,\n    stdevs: float | tuple[float, ...] = 1.0,\n    method: Literal[\"smoothgrad\", \"smoothgrad_sq\", \"vargrad\"] = \"smoothgrad\",\n) -&gt; HSIAttributes | list[HSIAttributes]:\n    \"\"\"\n    Method for generating attributions using the Noise Tunnel method.\n\n    Args:\n        hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n            If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n            The output will be a list of HSIAttributes objects.\n        baseline (int | float | torch.Tensor, optional): Baselines define reference value which replaces each\n            feature when occluded is computed and can be provided as:\n                - integer or float representing a constant value used as the baseline for all input pixels.\n                - tensor with the same shape as the input tensor, providing a baseline for each input pixel.\n                    if the input is a list of HSI objects, the baseline can be a tensor with the same shape as\n                    the input tensor for each HSI object.\n        target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n            methods assume that the output has only one class. If the output has multiple classes, the target index\n            must be provided. For multiple input images, a list of target indices can be provided, one for each\n            image or single target value will be used for all images. Defaults to None.\n        additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n            the inputs for which attributions should not be computed, this argument can be provided.\n            It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n            containing multiple additional arguments including tensors or any arbitrary python types.\n            These arguments are provided to forward_func in order following the arguments in inputs.\n            Note that attributions are not computed with respect to these arguments. Default: None\n        n_samples (int, optional): The number of randomly generated examples per sample in the input batch.\n            Random examples are generated by adding gaussian random noise to each sample.\n            Default: 5 if nt_samples is not provided.\n        steps_per_batch (int, optional): The number of the n_samples that will be processed together.\n            With the help of this parameter we can avoid out of memory situation and reduce the number of randomly\n            generated examples per sample in each batch. Default: None if steps_per_batch is not provided.\n            In this case all nt_samples will be processed together.\n        perturbation_axis (None | tuple[int | slice], optional): The indices of the input image to be perturbed.\n            If set to None, all bands are perturbed, which corresponds to a traditional noise tunnel method.\n            Defaults to None.\n        stdevs (float | tuple[float, ...], optional): The standard deviation of gaussian noise with zero mean that\n            is added to each input in the batch. If stdevs is a single float value then that same value is used\n            for all inputs. If stdevs is a tuple, then the length of the tuple must match the number of inputs as\n            each value in the tuple is used for the corresponding input. Default: 1.0\n        method (Literal[\"smoothgrad\", \"smoothgrad_sq\", \"vargrad\"], optional): Smoothing type of the attributions.\n            smoothgrad, smoothgrad_sq or vargrad Default: smoothgrad if type is not provided.\n\n    Returns:\n        HSIAttributes | list[HSIAttributes]: The computed attributions for the input hyperspectral image(s).\n            if a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n\n    Raises:\n        HSIAttributesError: If an error occurs during the generation of the attributions.\n\n    Examples:\n        &gt;&gt;&gt; noise_tunnel = NoiseTunnel(explainable_model)\n        &gt;&gt;&gt; hsi = HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n        &gt;&gt;&gt; attributions = noise_tunnel.attribute(hsi)\n        &gt;&gt;&gt; attributions = noise_tunnel.attribute([hsi, hsi])\n        &gt;&gt;&gt; len(attributions)\n        2\n    \"\"\"\n    if isinstance(stdevs, list):\n        stdevs = tuple(stdevs)\n\n    if not isinstance(hsi, list):\n        hsi = [hsi]\n\n    if not all([isinstance(input, HSI) for input in hsi]):\n        raise TypeError(\"All inputs must be HSI objects\")\n\n    if isinstance(stdevs, tuple):\n        if len(stdevs) != len(hsi):\n            raise ValueError(\n                \"The number of stdevs must match the number of input images, number of stdevs:\"\n                f\"{len(stdevs)}, number of input images: {len(hsi)}\"\n            )\n    else:\n        stdevs = tuple([stdevs] * len(hsi))\n\n    if not isinstance(target, list):\n        target = [target] * len(hsi)  # type: ignore\n\n    nt_attributes = torch.empty((n_samples, len(hsi)) + hsi[0].image.shape, device=hsi[0].device)\n\n    for batch in range(0, len(hsi)):\n        input = hsi[batch]\n        targeted = target[batch]\n        stdev = stdevs[batch]\n        perturbed_input = self.perturb_input(input.image, n_samples, perturbation_axis, stdev)\n        nt_attributes[:, batch] = self._forward_loop(\n            perturbed_input, input, targeted, additional_forward_args, n_samples, steps_per_batch\n        )\n\n    nt_attributes = self._aggregate_attributions(nt_attributes, method)\n\n    try:\n        attributes = [\n            HSIAttributes(hsi=hsi_image, attributes=attribution, attribution_method=self.get_name())\n            for hsi_image, attribution in zip(hsi, nt_attributes)\n        ]\n    except Exception as e:\n        raise HSIAttributesError(f\"Error in generating NoiseTunnel attributions: {e}\") from e\n\n    return attributes[0] if len(attributes) == 1 else attributes\n</code></pre>"},{"location":"reference/#src.meteors.attr.noise_tunnel.NoiseTunnel.perturb_input","title":"<code>perturb_input(input, n_samples=1, perturbation_axis=None, stdevs=1, **kwargs)</code>  <code>staticmethod</code>","text":"<p>The default perturbation function used in the noise tunnel with small enhancement for hyperspectral images. It randomly adds noise to the input tensor from a normal distribution with a given standard deviation. The noise is added to the selected bands (channels) of the input tensor. The bands to be perturbed are selected based on the <code>perturbation_axis</code> parameter. By default all bands are perturbed, which is equivalent to the standard noise tunnel method.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>An input tensor to be perturbed. It should have the shape (C, H, W).</p> required <code>n_samples</code> <code>int</code> <p>A number of samples to be drawn - number of perturbed inputs to be generated.</p> <code>1</code> <code>perturbation_axis</code> <code>None | tuple[int | slice]</code> <p>The indices of the bands to be perturbed. If set to None, all bands are perturbed. Defaults to None.</p> <code>None</code> <code>stdevs</code> <code>float</code> <p>The standard deviation of gaussian noise with zero mean that is added to each input in the batch. Defaults to 1.0.</p> <code>1</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A perturbed tensor, which contains <code>n_samples</code> perturbed inputs.</p> Source code in <code>src/meteors/attr/noise_tunnel.py</code> <pre><code>@staticmethod\ndef perturb_input(\n    input: torch.Tensor,\n    n_samples: int = 1,\n    perturbation_axis: None | tuple[int | slice] = None,\n    stdevs: float = 1,\n    **kwargs: Any,\n) -&gt; torch.Tensor:\n    \"\"\"\n    The default perturbation function used in the noise tunnel with small enhancement for hyperspectral images.\n    It randomly adds noise to the input tensor from a normal distribution with a given standard deviation.\n    The noise is added to the selected bands (channels) of the input tensor.\n    The bands to be perturbed are selected based on the `perturbation_axis` parameter.\n    By default all bands are perturbed, which is equivalent to the standard noise tunnel method.\n\n    Args:\n        input (torch.Tensor): An input tensor to be perturbed. It should have the shape (C, H, W).\n        n_samples (int): A number of samples to be drawn - number of perturbed inputs to be generated.\n        perturbation_axis (None | tuple[int | slice]): The indices of the bands to be perturbed.\n            If set to None, all bands are perturbed. Defaults to None.\n        stdevs (float): The standard deviation of gaussian noise with zero mean that is added to each input\n            in the batch. Defaults to 1.0.\n\n    Returns:\n        torch.Tensor: A perturbed tensor, which contains `n_samples` perturbed inputs.\n    \"\"\"\n    if n_samples &lt; 1:\n        raise ValueError(\"Number of perturbated samples to be generated must be greater than 0\")\n\n    # the perturbation\n    perturbed_input = input.clone().unsqueeze(0)\n    # repeat the perturbed_input on the first dimension n_samples times\n    perturbed_input = perturbed_input.repeat_interleave(n_samples, dim=0)\n\n    # the perturbation shape\n    if perturbation_axis is None:\n        perturbation_shape = perturbed_input.shape\n    else:\n        perturbation_axis = (slice(None),) + perturbation_axis  # type: ignore\n        perturbation_shape = perturbed_input[perturbation_axis].shape\n\n    # the noise\n    noise = torch.normal(0, stdevs, size=perturbation_shape).to(input.device)\n\n    # add the noise to the perturbed_input\n    if perturbation_axis is None:\n        perturbed_input += noise\n    else:\n        perturbed_input[perturbation_axis] += noise\n\n    perturbed_input.requires_grad_(True)\n\n    return perturbed_input\n</code></pre>"},{"location":"reference/#src.meteors.attr.noise_tunnel.HyperNoiseTunnel","title":"<code>HyperNoiseTunnel</code>","text":"<p>               Bases: <code>BaseNoiseTunnel</code></p> <p>Hyper Noise Tunnel is our novel method, designed specifically to explain hyperspectral satellite images. It is inspired by the behaviour of the classical Noise Tunnel (Smooth Grad) method, but instead of sampling noise into the original image, it randomly masks some of the bands with the baseline. In the process, the created noised samples are close to the distribution of the original image yet differ enough to smoothen the produced attribution map.</p> <p>Parameters:</p> Name Type Description Default <code>chained_explainer</code> <p>The explainable method that will be used to compute the attributions.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the callable object is not an instance of the Explainer class</p> Source code in <code>src/meteors/attr/noise_tunnel.py</code> <pre><code>class HyperNoiseTunnel(BaseNoiseTunnel):\n    \"\"\"Hyper Noise Tunnel is our novel method, designed specifically to explain hyperspectral satellite images. It is\n    inspired by the behaviour of the classical Noise Tunnel (Smooth Grad) method, but instead of sampling noise into the\n    original image, it randomly masks some of the bands with the baseline. In the process, the created _noised_ samples\n    are close to the distribution of the original image yet differ enough to smoothen the produced attribution map.\n\n    Arguments:\n        chained_explainer: The explainable method that will be used to compute the attributions.\n\n    Raises:\n        RuntimeError: If the callable object is not an instance of the Explainer class\n    \"\"\"\n\n    @staticmethod\n    def perturb_input(\n        input: torch.Tensor,\n        baseline: torch.Tensor | None = None,\n        n_samples: int = 1,\n        perturbation_prob: float = 0.5,\n        num_perturbed_bands: int | None = None,\n        **kwargs: Any,\n    ) -&gt; torch.Tensor:\n        \"\"\"The perturbation function used in the hyper noise tunnel. It randomly selects a subset of the input bands\n        that will be masked out and replaced with the baseline. The parameters `num_perturbed_bands` and\n        `perturbation_prob` control the number of bands that will be perturbed (masked). If `num_perturbed_bands` is\n        set, it will be used as the number of bands to perturb, which will be randomly selected. Otherwise, the number\n        of bands will be drawn from a binomial distribution with `perturbation_prob` as the probability of success.\n\n        Args:\n            input (torch.Tensor): An input tensor to be perturbed. It should have the shape (C, H, W).\n            baseline (torch.Tensor | None, optional): A tensor that will be used to replace the perturbed bands.\n            n_samples (int): A number of samples to be drawn - number of perturbed inputs to be generated.\n            perturbation_prob (float, optional): A probability that each band will be perturbed intependently.\n                Defaults to 0.5.\n            num_perturbed_bands (int | None, optional): A number of perturbed bands in the whole image.\n                If set to None, the bands are perturbed with probability `perturbation_prob` each. Defaults to None.\n\n        Returns:\n            torch.Tensor: A perturbed tensor, which contains `n_samples` perturbed inputs.\n        \"\"\"\n        # validate the baseline against the input\n        if baseline is None:\n            raise ValueError(\"Baseline must be provided for the HyperNoiseTunnel method\")\n\n        if baseline.shape != input.shape:\n            raise ShapeMismatchError(f\"Baseline shape {baseline.shape} does not match input shape {input.shape}\")\n\n        if n_samples &lt; 1:\n            raise ValueError(\"Number of perturbated samples to be generated must be greater than 0\")\n\n        if perturbation_prob &lt; 0 or perturbation_prob &gt; 1:\n            raise ValueError(\"Perturbation probability must be in the range [0, 1]\")\n\n        # the perturbation\n        perturbed_input = input.clone().unsqueeze(0)\n        # repeat the perturbed_input on the first dimension n_samples times\n        perturbed_input = perturbed_input.repeat_interleave(n_samples, dim=0)\n\n        n_samples_x_channels_shape = (\n            n_samples,\n            input.shape[0],\n        )  # shape of the tensor containing the perturbed channels for each sample\n\n        channels_to_be_perturbed: torch.Tensor = torch.zeros(n_samples_x_channels_shape, device=input.device).bool()\n\n        if num_perturbed_bands is None:\n            channel_perturbation_probabilities = (\n                torch.ones(n_samples_x_channels_shape, device=input.device) * perturbation_prob\n            )\n            channels_to_be_perturbed = torch.bernoulli(channel_perturbation_probabilities).bool()\n\n        else:\n            if num_perturbed_bands &lt; 0 or num_perturbed_bands &gt; input.shape[0]:\n                raise ValueError(\n                    f\"Cannot perturb {num_perturbed_bands} bands in the input with {input.shape[0]} channels. The number of perturbed bands must be in the range [0, {input.shape[0]}]\"\n                )\n\n            channels_to_be_perturbed = torch_random_choice(\n                input.shape[0], num_perturbed_bands, n_samples, device=input.device\n            )\n\n        # now having chosen the perturbed channels, we can replace them with the baseline\n        reshaped_baseline = baseline.unsqueeze(0).repeat_interleave(n_samples, dim=0)\n        perturbed_input[channels_to_be_perturbed] = reshaped_baseline[channels_to_be_perturbed]\n\n        perturbed_input.requires_grad_(True)\n\n        return perturbed_input\n\n    def attribute(\n        self,\n        hsi: list[HSI] | HSI,\n        baseline: int | float | torch.Tensor | list[int | float | torch.Tensor] | None = None,\n        target: list[int] | int | None = None,\n        additional_forward_args: Any = None,\n        n_samples: int = 5,\n        steps_per_batch: int = 1,\n        perturbation_prob: float = 0.5,\n        num_perturbed_bands: int | None = None,\n        method: Literal[\"smoothgrad\", \"smoothgrad_sq\", \"vargrad\"] = \"smoothgrad\",\n    ) -&gt; HSIAttributes | list[HSIAttributes]:\n        \"\"\"\n        Method for generating attributions using the Hyper Noise Tunnel method.\n\n        Args:\n            hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n                If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n                The output will be a list of HSIAttributes objects.\n            baseline (int | float | torch.Tensor | list[int | float | torch.Tensor], optional): Baselines define reference value which\n                replaces each feature when occluded is computed and can be provided as:\n                    - integer or float representing a constant value used as the baseline for all input pixels.\n                    - tensor with the same shape as the input tensor, providing a baseline for each input pixel.\n                        if the input is a list of HSI objects, the baseline can be a tensor with the same shape as\n                        the input tensor for each HSI object or a list of tensors with the same length as the input list.\n            target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n                methods assume that the output has only one class. If the output has multiple classes, the target index\n                must be provided. For multiple input images, a list of target indices can be provided, one for each\n                image or single target value will be used for all images. Defaults to None.\n            additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n                the inputs for which attributions should not be computed, this argument can be provided.\n                It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n                containing multiple additional arguments including tensors or any arbitrary python types.\n                These arguments are provided to forward_func in order following the arguments in inputs.\n                Note that attributions are not computed with respect to these arguments. Default: None\n            n_samples (int, optional):The number of randomly generated examples per sample in the input batch.\n                Random examples are generated by adding gaussian random noise to each sample.\n                Default: 5 if nt_samples is not provided.\n            steps_per_batch (int, optional): The number of the n_samples that will be processed together.\n                With the help of this parameter we can avoid out of memory situation and reduce the number of randomly\n                generated examples per sample in each batch. Default: None if steps_per_batch is not provided.\n                In this case all nt_samples will be processed together.\n            perturbation_prob (float, optional): The probability that each band will be perturbed independently.\n                Defaults to 0.5.\n            num_perturbed_bands (int | None, optional): The number of perturbed bands in the whole image.\n                The bands to be perturbed are selected randomly with no replacement.\n                If set to None, the bands are perturbed with probability `perturbation_prob` each. Defaults to None.\n            method (Literal[\"smoothgrad\", \"smoothgrad_sq\", \"vargrad\"], optional): Smoothing type of the attributions.\n                smoothgrad, smoothgrad_sq or vargrad Default: smoothgrad if type is not provided.\n\n        Returns:\n            HSIAttributes | list[HSIAttributes]: The computed attributions for the input hyperspectral image(s).\n                if a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n\n        Raises:\n            HSIAttributesError: If an error occurs during the generation of the attributions.\n\n        Examples:\n            &gt;&gt;&gt; hyper_noise_tunnel = HyperNoiseTunnel(explainable_model)\n            &gt;&gt;&gt; hsi = HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n            &gt;&gt;&gt; attributions = hyper_noise_tunnel.attribute(hsi)\n            &gt;&gt;&gt; attributions = hyper_noise_tunnel.attribute([hsi, hsi])\n            &gt;&gt;&gt; len(attributions)\n            2\n        \"\"\"\n        change_orientation = []\n        original_orientation = []\n\n        if not isinstance(hsi, list):\n            hsi = [hsi]\n\n        if not all([isinstance(input, HSI) for input in hsi]):\n            raise TypeError(\"All inputs must be HSI objects\")\n\n        for i in range(len(hsi)):\n            if hsi[i].orientation != (\"C\", \"H\", \"W\"):\n                change_orientation.append(True)\n                original_orientation.append(hsi[i].orientation)\n                hsi[i] = hsi[i].change_orientation(\"CHW\")\n            else:\n                change_orientation.append(False)\n\n        if not isinstance(baseline, list):\n            baseline = [baseline] * len(hsi)\n        elif len(baseline) != len(hsi):\n            raise ValueError(\"The number of baseline must match the number of input images\")\n\n        baseline = [validate_and_transform_baseline(base, hsi_image) for base, hsi_image in zip(baseline, hsi)]\n\n        if not isinstance(target, list):\n            target = [target] * len(hsi)  # type: ignore\n\n        hnt_attributes = torch.empty((n_samples, len(hsi)) + hsi[0].image.shape, device=hsi[0].device)\n        for batch in range(0, len(hsi)):\n            input = hsi[batch]\n            targeted = target[batch]\n            base = baseline[batch]\n            perturbed_input = self.perturb_input(input.image, base, n_samples, perturbation_prob, num_perturbed_bands)\n\n            hnt_attributes[:, batch] = self._forward_loop(\n                perturbed_input, input, targeted, additional_forward_args, n_samples, steps_per_batch\n            )\n\n        hnt_attributes = self._aggregate_attributions(hnt_attributes, method)\n\n        try:\n            attributes = [\n                HSIAttributes(hsi=hsi_image, attributes=attribution, attribution_method=self.get_name())\n                for hsi_image, attribution in zip(hsi, hnt_attributes)\n            ]\n        except Exception as e:\n            raise HSIAttributesError(f\"Error in generating HyperNoiseTunnel attributions: {e}\") from e\n\n        for i in range(len(change_orientation)):\n            if change_orientation[i]:\n                attributes[i].hsi = attributes[i].hsi.change_orientation(original_orientation[i])\n\n        return attributes[0] if len(attributes) == 1 else attributes\n</code></pre>"},{"location":"reference/#src.meteors.attr.noise_tunnel.HyperNoiseTunnel.attribute","title":"<code>attribute(hsi, baseline=None, target=None, additional_forward_args=None, n_samples=5, steps_per_batch=1, perturbation_prob=0.5, num_perturbed_bands=None, method='smoothgrad')</code>","text":"<p>Method for generating attributions using the Hyper Noise Tunnel method.</p> <p>Parameters:</p> Name Type Description Default <code>hsi</code> <code>list[HSI] | HSI</code> <p>Input hyperspectral image(s) for which the attributions are to be computed. If a list of HSI objects is provided, the attributions are computed for each HSI object in the list. The output will be a list of HSIAttributes objects.</p> required <code>baseline</code> <code>int | float | Tensor | list[int | float | Tensor]</code> <p>Baselines define reference value which replaces each feature when occluded is computed and can be provided as:     - integer or float representing a constant value used as the baseline for all input pixels.     - tensor with the same shape as the input tensor, providing a baseline for each input pixel.         if the input is a list of HSI objects, the baseline can be a tensor with the same shape as         the input tensor for each HSI object or a list of tensors with the same length as the input list.</p> <code>None</code> <code>target</code> <code>list[int] | int | None</code> <p>target class index for computing the attributions. If None, methods assume that the output has only one class. If the output has multiple classes, the target index must be provided. For multiple input images, a list of target indices can be provided, one for each image or single target value will be used for all images. Defaults to None.</p> <code>None</code> <code>additional_forward_args</code> <code>Any</code> <p>If the forward function requires additional arguments other than the inputs for which attributions should not be computed, this argument can be provided. It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple containing multiple additional arguments including tensors or any arbitrary python types. These arguments are provided to forward_func in order following the arguments in inputs. Note that attributions are not computed with respect to these arguments. Default: None</p> <code>None</code> <code>n_samples</code> <code>int</code> <p>The number of randomly generated examples per sample in the input batch. Random examples are generated by adding gaussian random noise to each sample. Default: 5 if nt_samples is not provided.</p> <code>5</code> <code>steps_per_batch</code> <code>int</code> <p>The number of the n_samples that will be processed together. With the help of this parameter we can avoid out of memory situation and reduce the number of randomly generated examples per sample in each batch. Default: None if steps_per_batch is not provided. In this case all nt_samples will be processed together.</p> <code>1</code> <code>perturbation_prob</code> <code>float</code> <p>The probability that each band will be perturbed independently. Defaults to 0.5.</p> <code>0.5</code> <code>num_perturbed_bands</code> <code>int | None</code> <p>The number of perturbed bands in the whole image. The bands to be perturbed are selected randomly with no replacement. If set to None, the bands are perturbed with probability <code>perturbation_prob</code> each. Defaults to None.</p> <code>None</code> <code>method</code> <code>Literal['smoothgrad', 'smoothgrad_sq', 'vargrad']</code> <p>Smoothing type of the attributions. smoothgrad, smoothgrad_sq or vargrad Default: smoothgrad if type is not provided.</p> <code>'smoothgrad'</code> <p>Returns:</p> Type Description <code>HSIAttributes | list[HSIAttributes]</code> <p>HSIAttributes | list[HSIAttributes]: The computed attributions for the input hyperspectral image(s). if a list of HSI objects is provided, the attributions are computed for each HSI object in the list.</p> <p>Raises:</p> Type Description <code>HSIAttributesError</code> <p>If an error occurs during the generation of the attributions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hyper_noise_tunnel = HyperNoiseTunnel(explainable_model)\n&gt;&gt;&gt; hsi = HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n&gt;&gt;&gt; attributions = hyper_noise_tunnel.attribute(hsi)\n&gt;&gt;&gt; attributions = hyper_noise_tunnel.attribute([hsi, hsi])\n&gt;&gt;&gt; len(attributions)\n2\n</code></pre> Source code in <code>src/meteors/attr/noise_tunnel.py</code> <pre><code>def attribute(\n    self,\n    hsi: list[HSI] | HSI,\n    baseline: int | float | torch.Tensor | list[int | float | torch.Tensor] | None = None,\n    target: list[int] | int | None = None,\n    additional_forward_args: Any = None,\n    n_samples: int = 5,\n    steps_per_batch: int = 1,\n    perturbation_prob: float = 0.5,\n    num_perturbed_bands: int | None = None,\n    method: Literal[\"smoothgrad\", \"smoothgrad_sq\", \"vargrad\"] = \"smoothgrad\",\n) -&gt; HSIAttributes | list[HSIAttributes]:\n    \"\"\"\n    Method for generating attributions using the Hyper Noise Tunnel method.\n\n    Args:\n        hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n            If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n            The output will be a list of HSIAttributes objects.\n        baseline (int | float | torch.Tensor | list[int | float | torch.Tensor], optional): Baselines define reference value which\n            replaces each feature when occluded is computed and can be provided as:\n                - integer or float representing a constant value used as the baseline for all input pixels.\n                - tensor with the same shape as the input tensor, providing a baseline for each input pixel.\n                    if the input is a list of HSI objects, the baseline can be a tensor with the same shape as\n                    the input tensor for each HSI object or a list of tensors with the same length as the input list.\n        target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n            methods assume that the output has only one class. If the output has multiple classes, the target index\n            must be provided. For multiple input images, a list of target indices can be provided, one for each\n            image or single target value will be used for all images. Defaults to None.\n        additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n            the inputs for which attributions should not be computed, this argument can be provided.\n            It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n            containing multiple additional arguments including tensors or any arbitrary python types.\n            These arguments are provided to forward_func in order following the arguments in inputs.\n            Note that attributions are not computed with respect to these arguments. Default: None\n        n_samples (int, optional):The number of randomly generated examples per sample in the input batch.\n            Random examples are generated by adding gaussian random noise to each sample.\n            Default: 5 if nt_samples is not provided.\n        steps_per_batch (int, optional): The number of the n_samples that will be processed together.\n            With the help of this parameter we can avoid out of memory situation and reduce the number of randomly\n            generated examples per sample in each batch. Default: None if steps_per_batch is not provided.\n            In this case all nt_samples will be processed together.\n        perturbation_prob (float, optional): The probability that each band will be perturbed independently.\n            Defaults to 0.5.\n        num_perturbed_bands (int | None, optional): The number of perturbed bands in the whole image.\n            The bands to be perturbed are selected randomly with no replacement.\n            If set to None, the bands are perturbed with probability `perturbation_prob` each. Defaults to None.\n        method (Literal[\"smoothgrad\", \"smoothgrad_sq\", \"vargrad\"], optional): Smoothing type of the attributions.\n            smoothgrad, smoothgrad_sq or vargrad Default: smoothgrad if type is not provided.\n\n    Returns:\n        HSIAttributes | list[HSIAttributes]: The computed attributions for the input hyperspectral image(s).\n            if a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n\n    Raises:\n        HSIAttributesError: If an error occurs during the generation of the attributions.\n\n    Examples:\n        &gt;&gt;&gt; hyper_noise_tunnel = HyperNoiseTunnel(explainable_model)\n        &gt;&gt;&gt; hsi = HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n        &gt;&gt;&gt; attributions = hyper_noise_tunnel.attribute(hsi)\n        &gt;&gt;&gt; attributions = hyper_noise_tunnel.attribute([hsi, hsi])\n        &gt;&gt;&gt; len(attributions)\n        2\n    \"\"\"\n    change_orientation = []\n    original_orientation = []\n\n    if not isinstance(hsi, list):\n        hsi = [hsi]\n\n    if not all([isinstance(input, HSI) for input in hsi]):\n        raise TypeError(\"All inputs must be HSI objects\")\n\n    for i in range(len(hsi)):\n        if hsi[i].orientation != (\"C\", \"H\", \"W\"):\n            change_orientation.append(True)\n            original_orientation.append(hsi[i].orientation)\n            hsi[i] = hsi[i].change_orientation(\"CHW\")\n        else:\n            change_orientation.append(False)\n\n    if not isinstance(baseline, list):\n        baseline = [baseline] * len(hsi)\n    elif len(baseline) != len(hsi):\n        raise ValueError(\"The number of baseline must match the number of input images\")\n\n    baseline = [validate_and_transform_baseline(base, hsi_image) for base, hsi_image in zip(baseline, hsi)]\n\n    if not isinstance(target, list):\n        target = [target] * len(hsi)  # type: ignore\n\n    hnt_attributes = torch.empty((n_samples, len(hsi)) + hsi[0].image.shape, device=hsi[0].device)\n    for batch in range(0, len(hsi)):\n        input = hsi[batch]\n        targeted = target[batch]\n        base = baseline[batch]\n        perturbed_input = self.perturb_input(input.image, base, n_samples, perturbation_prob, num_perturbed_bands)\n\n        hnt_attributes[:, batch] = self._forward_loop(\n            perturbed_input, input, targeted, additional_forward_args, n_samples, steps_per_batch\n        )\n\n    hnt_attributes = self._aggregate_attributions(hnt_attributes, method)\n\n    try:\n        attributes = [\n            HSIAttributes(hsi=hsi_image, attributes=attribution, attribution_method=self.get_name())\n            for hsi_image, attribution in zip(hsi, hnt_attributes)\n        ]\n    except Exception as e:\n        raise HSIAttributesError(f\"Error in generating HyperNoiseTunnel attributions: {e}\") from e\n\n    for i in range(len(change_orientation)):\n        if change_orientation[i]:\n            attributes[i].hsi = attributes[i].hsi.change_orientation(original_orientation[i])\n\n    return attributes[0] if len(attributes) == 1 else attributes\n</code></pre>"},{"location":"reference/#src.meteors.attr.noise_tunnel.HyperNoiseTunnel.perturb_input","title":"<code>perturb_input(input, baseline=None, n_samples=1, perturbation_prob=0.5, num_perturbed_bands=None, **kwargs)</code>  <code>staticmethod</code>","text":"<p>The perturbation function used in the hyper noise tunnel. It randomly selects a subset of the input bands that will be masked out and replaced with the baseline. The parameters <code>num_perturbed_bands</code> and <code>perturbation_prob</code> control the number of bands that will be perturbed (masked). If <code>num_perturbed_bands</code> is set, it will be used as the number of bands to perturb, which will be randomly selected. Otherwise, the number of bands will be drawn from a binomial distribution with <code>perturbation_prob</code> as the probability of success.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>An input tensor to be perturbed. It should have the shape (C, H, W).</p> required <code>baseline</code> <code>Tensor | None</code> <p>A tensor that will be used to replace the perturbed bands.</p> <code>None</code> <code>n_samples</code> <code>int</code> <p>A number of samples to be drawn - number of perturbed inputs to be generated.</p> <code>1</code> <code>perturbation_prob</code> <code>float</code> <p>A probability that each band will be perturbed intependently. Defaults to 0.5.</p> <code>0.5</code> <code>num_perturbed_bands</code> <code>int | None</code> <p>A number of perturbed bands in the whole image. If set to None, the bands are perturbed with probability <code>perturbation_prob</code> each. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A perturbed tensor, which contains <code>n_samples</code> perturbed inputs.</p> Source code in <code>src/meteors/attr/noise_tunnel.py</code> <pre><code>@staticmethod\ndef perturb_input(\n    input: torch.Tensor,\n    baseline: torch.Tensor | None = None,\n    n_samples: int = 1,\n    perturbation_prob: float = 0.5,\n    num_perturbed_bands: int | None = None,\n    **kwargs: Any,\n) -&gt; torch.Tensor:\n    \"\"\"The perturbation function used in the hyper noise tunnel. It randomly selects a subset of the input bands\n    that will be masked out and replaced with the baseline. The parameters `num_perturbed_bands` and\n    `perturbation_prob` control the number of bands that will be perturbed (masked). If `num_perturbed_bands` is\n    set, it will be used as the number of bands to perturb, which will be randomly selected. Otherwise, the number\n    of bands will be drawn from a binomial distribution with `perturbation_prob` as the probability of success.\n\n    Args:\n        input (torch.Tensor): An input tensor to be perturbed. It should have the shape (C, H, W).\n        baseline (torch.Tensor | None, optional): A tensor that will be used to replace the perturbed bands.\n        n_samples (int): A number of samples to be drawn - number of perturbed inputs to be generated.\n        perturbation_prob (float, optional): A probability that each band will be perturbed intependently.\n            Defaults to 0.5.\n        num_perturbed_bands (int | None, optional): A number of perturbed bands in the whole image.\n            If set to None, the bands are perturbed with probability `perturbation_prob` each. Defaults to None.\n\n    Returns:\n        torch.Tensor: A perturbed tensor, which contains `n_samples` perturbed inputs.\n    \"\"\"\n    # validate the baseline against the input\n    if baseline is None:\n        raise ValueError(\"Baseline must be provided for the HyperNoiseTunnel method\")\n\n    if baseline.shape != input.shape:\n        raise ShapeMismatchError(f\"Baseline shape {baseline.shape} does not match input shape {input.shape}\")\n\n    if n_samples &lt; 1:\n        raise ValueError(\"Number of perturbated samples to be generated must be greater than 0\")\n\n    if perturbation_prob &lt; 0 or perturbation_prob &gt; 1:\n        raise ValueError(\"Perturbation probability must be in the range [0, 1]\")\n\n    # the perturbation\n    perturbed_input = input.clone().unsqueeze(0)\n    # repeat the perturbed_input on the first dimension n_samples times\n    perturbed_input = perturbed_input.repeat_interleave(n_samples, dim=0)\n\n    n_samples_x_channels_shape = (\n        n_samples,\n        input.shape[0],\n    )  # shape of the tensor containing the perturbed channels for each sample\n\n    channels_to_be_perturbed: torch.Tensor = torch.zeros(n_samples_x_channels_shape, device=input.device).bool()\n\n    if num_perturbed_bands is None:\n        channel_perturbation_probabilities = (\n            torch.ones(n_samples_x_channels_shape, device=input.device) * perturbation_prob\n        )\n        channels_to_be_perturbed = torch.bernoulli(channel_perturbation_probabilities).bool()\n\n    else:\n        if num_perturbed_bands &lt; 0 or num_perturbed_bands &gt; input.shape[0]:\n            raise ValueError(\n                f\"Cannot perturb {num_perturbed_bands} bands in the input with {input.shape[0]} channels. The number of perturbed bands must be in the range [0, {input.shape[0]}]\"\n            )\n\n        channels_to_be_perturbed = torch_random_choice(\n            input.shape[0], num_perturbed_bands, n_samples, device=input.device\n        )\n\n    # now having chosen the perturbed channels, we can replace them with the baseline\n    reshaped_baseline = baseline.unsqueeze(0).repeat_interleave(n_samples, dim=0)\n    perturbed_input[channels_to_be_perturbed] = reshaped_baseline[channels_to_be_perturbed]\n\n    perturbed_input.requires_grad_(True)\n\n    return perturbed_input\n</code></pre>"},{"location":"tutorials/attr_showcase/","title":"Attribution Methods for Hyperspectral Image Analysis using <code>attr</code>","text":"<p>This notebook showcase attribution methods from <code>meteors.attr</code> to interpret predictions from a hyperspectral image regression model using European Space Agency (ESA)'s HYPERVIEW Challenge dataset, which predicts <code>4</code>soil parameters from hyperspectral ground images.</p> <p>The model used once in this notebook is one of the top-performing models in the challenge. The trained model architecture is based on the Vision Transformer (ViT) and CLIP (Contrastive Language-Image Pretraining), and its fine-tuned weights are open-sourced under the Apache License in the Hugging Face Model Hub. In the same place the original implementation of the CLIP model can be found. </p> <p>Prerequisites: Install the meteors package from PyPI:</p> <pre><code>pip install meteors\n</code></pre> <p>which includes required dependencies. The <code>clip_model</code> module contains the code needed for additional preprocessing and model loading and can be downloaded from the Vignettes in the <code>meteors</code> repository</p>"},{"location":"tutorials/attr_showcase/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Loading the Model and Hyperspectral data</li> <li>2. Convert data into HSI image and preview the images</li> <li>3. Analyze HSI data with Attribution Algorithms</li> <li>3.1. Gradient Based Methods</li> <li>3.2. Occlusion Based Methods</li> <li>3.3. Perturbation Based Methods</li> </ul> <pre><code>import os\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\n\nimport meteors as mt\n\nfrom clip_utils import load_base_clip, download\n\n# Always try to set the seed for repeatability :)\ntorch.manual_seed(0)\n</code></pre> <pre><code>&lt;torch._C.Generator at 0x10c951590&gt;\n</code></pre> <pre><code>device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n</code></pre> <pre><code>Using device: cpu\n</code></pre>"},{"location":"tutorials/attr_showcase/#1-loading-the-model-and-hyperspectral-data","title":"1. Loading the Model and Hyperspectral data","text":"<p>The dataset used for training this model can be found on the official page for the HYPERVIEW Challenge.</p> <p>In the cell below, we load the model.</p> <pre><code>download_root = os.path.expanduser(\"~/.cache/clip\")\nnum_classes = 4\n</code></pre> <p>Load the CLIP model with the HYPERVIEW head</p> <pre><code>model = load_base_clip(download_root=download_root, class_num=num_classes)\n</code></pre> <p>Load the pre-trained weights</p> <pre><code>vit_checkpoint_path = download(\n    \"https://huggingface.co/KPLabs/HYPERVIEW-VisionTransformer/resolve/main/VisionTransformer.pt\",\n    download_root,\n    error_checksum=False,\n)\nmodel.load_state_dict(torch.load(vit_checkpoint_path, map_location=device))\nmodel.eval()\nmodel = model.to(device)\n</code></pre> <p>And the hyperspectral data.</p> <pre><code>def _shape_pad(data):\n    max_edge = np.max(data.shape[1:])\n    shape = (max_edge, max_edge)\n    padded = np.pad(\n        data,\n        ((0, 0), (0, (shape[0] - data.shape[1])), (0, (shape[1] - data.shape[2]))),\n        \"constant\",\n        constant_values=0.0,\n    )\n    return padded\n\n\ndef load_single_npz_image(image_path):\n    with np.load(image_path) as npz:\n        data = npz[\"data\"]\n        mask = npz[\"mask\"]\n\n        mask = 1 - mask.astype(int)\n\n        mask = _shape_pad(mask)\n        data = _shape_pad(data)\n\n        mask = mask.transpose((1, 2, 0))\n        data = data.transpose((1, 2, 0))\n        data = data / 5419\n\n        return data, mask\n\n\ndef get_eval_transform(image_shape):\n    return transforms.Compose(\n        [\n            transforms.Resize((image_shape, image_shape)),\n        ]\n    )\n</code></pre> <pre><code>data, mask = load_single_npz_image(\"data/0.npz\")\nmasked_data = data * mask\nmasked_data = torch.from_numpy(masked_data.astype(np.float32)).permute(2, 0, 1)\neval_tr = get_eval_transform(224)\n\nimage_torch = eval_tr(masked_data)\nnot_masked_image_torch = eval_tr(torch.from_numpy(data.astype(np.float32)).permute(2, 0, 1))\n\nprint(f\"Original data shape: {data.shape}\")\nprint(f\"Original mask shape: {mask.shape}\")\nprint(f\"Transformed data shape: {image_torch.shape}\")\n</code></pre> <pre><code>Original data shape: (89, 89, 150)\nOriginal mask shape: (89, 89, 150)\nTransformed data shape: torch.Size([150, 224, 224])\n</code></pre> <p>As specified in the HYPERVIEW Challenge dataset description, we will also utilize the information about bands wavelengths.</p> <pre><code>with open(\"data/wavelenghts.txt\", \"r\") as f:\n    wavelengths = f.readline()\nwavelengths = [float(wave.strip()) for wave in wavelengths.split(\",\")]\n</code></pre>"},{"location":"tutorials/attr_showcase/#2-convert-data-into-hsi-image-and-preview-the-images","title":"2. Convert data into HSI image and preview the images","text":"<p>Now, having the raw data - the tensor representing the image, its wavelengths and the image orientation, we can to combine this information into a complete hyperspectral image. To create the hyperspectral image, we will use the <code>HSI</code> data class from the <code>meteors</code> package.</p> <p>From now on, we will use the <code>HSI</code> class to represent the hyperspectral image. The <code>HSI</code> class provides a set of methods to work with hyperspectral images, including visualization.</p> <p>Additionally, we may provide the binary mask, which may cover data irrelevant for the task, as suggested by the challenge dataset providers. Thus, We create a binary mask from the image, where 1 is the masked region and 0 is the unmasked region.</p> <pre><code>binary_mask = (image_torch &gt; 0.0).int()\n</code></pre> <p>The HSI object has the following attributes:</p> <ul> <li><code>data</code> - the preprocessed hyperspectral image data in numpy or pytorch with the shape (bands, height, width)</li> <li><code>wavelengths</code> - the wavelengths list of the hyperspectral image</li> <li><code>orientation</code> - the orientation of the image, here it is <code>CWH</code> (Channels, Width, Height)</li> <li><code>binary_mask</code> - the binary mask of the image</li> <li><code>device</code> - the device where the image data will be stored. We can provide it later with the <code>.to(device)</code> method</li> </ul> <pre><code>hsi_0 = mt.HSI(\n    image=not_masked_image_torch,\n    wavelengths=wavelengths,\n    orientation=\"CWH\",\n    binary_mask=binary_mask,\n    device=device,\n)\n</code></pre> <pre><code>fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n\nmt.visualize.visualize_hsi(hsi_0, ax1, use_mask=True)\nax1.set_title(\"Masked Image\")\n\nax2.imshow(binary_mask[0, ...].T.cpu().numpy(), cmap=\"gray\")\nax2.set_title(\"Binary Mask\")\n\nmt.visualize.visualize_hsi(hsi_0, ax3, use_mask=False)\nax3.set_title(\"Unmasked Image\")\n\nfig.suptitle(\"Sample image from the HYPERVIEW dataset\")\nplt.show()\n</code></pre> <p></p> <p>On the left plot, we see the input RGB representation of the data, in the middle the mask of the hyperspectral image, and on the right the whole hyperspectral image itself.</p> <p>The <code>HSI</code> dataclass automatically provides us the clean RGB chart of the hyperspectral image and releases us from the obligation of selecting the specific wavelengths to be plotted and considering the image orientation or processing.</p> <p>Now, we can provide the hyperspectral image to the model and get the prediction. The model will return the predictions for the 4 classes of the hyperspectral image.</p> <pre><code>original_prediction = model(not_masked_image_torch.unsqueeze(0))\nhsi_prediction = model(hsi_0.image.unsqueeze(0))\nassert torch.allclose(original_prediction, hsi_prediction, atol=1e-3)\n</code></pre> <p>The classes of the HYPERVIEW dataset</p> <pre><code>prediction_dict = {0: \"Phosphorus\", 1: \"Potassium\", 2: \"Magnesium\", 3: \"pH\"}\n</code></pre> <pre><code>predictions = {prediction_dict[i]: float(hsi_prediction[0, i].cpu().detach().numpy()) for i in range(4)}\npredictions = pd.Series(predictions)\npredictions\n</code></pre> <pre><code>Phosphorus    0.210551\nPotassium     0.350670\nMagnesium     0.391935\npH            0.883228\ndtype: float64\n</code></pre>"},{"location":"tutorials/attr_showcase/#3-analyze-hsi-data-with-attribution-algorithms","title":"3. Analyze HSI data with Attribution Algorithms","text":"<p>The <code>Meteors</code> package provides several attribution algorithms grouped into three categories (If you want to learn more about each method just click the method and it will redirect you to the documentation):</p> <ul> <li>Gradient Based: <code>Saliency</code>, <code>InputXGradient</code>, <code>IntegratedGradients</code></li> <li>Occlusion Based: <code>Occlusion</code></li> <li>Perturbation Based: <code>NoiseTunnel</code>, <code>HyperNoiseTunnel</code></li> </ul> <p>Let's explore how these methods help interpret our hyperspectral image model's predictions.</p> <p>Note Each attribution method in this tutorial can only analyze one class at a time for multi-class models. We'll focus on analyzing the <code>P</code> class (index 0) in our examples, though similar analyses could be performed for other classes.</p> <p>Before we start, let's prepare our model for the interpretation task by wrapping it with the <code>meteors</code> helper class.</p> <pre><code>explainable_model = mt.models.ExplainableModel(model, \"regression\")\n</code></pre>"},{"location":"tutorials/attr_showcase/#31-gradient-based-methods","title":"3.1. Gradient Based Methods","text":"<p>Gradient-based attribution methods compute the importance of each input feature by analyzing the model's gradients with respect to the input. These methods are computationally efficient and provide insights into how the model's predictions change with small input variations.</p>"},{"location":"tutorials/attr_showcase/#saliency","title":"Saliency","text":"<p>Simple yet effective method that computes the gradient of the output with respect to the input. The magnitude of these gradients indicates which input features most strongly influence the prediction.</p> <p>Captum Documentation</p> <p>Paper: Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps</p>"},{"location":"tutorials/attr_showcase/#inputxgradient","title":"InputXGradient","text":"<p>An extension of Saliency that multiplies the input by its gradient. This helps reduce noise in the attribution maps and better highlights relevant features by accounting for both feature values and their gradients.</p> <p>Captum Documentation</p> <p>Paper: Not Just a Black Box: Learning Important Features Through Propagating Activation Differences</p>"},{"location":"tutorials/attr_showcase/#integratedgradients","title":"IntegratedGradients","text":"<p>More sophisticated approach that accumulates gradients along a path from a baseline to the input. This method addresses the gradient saturation problem found in simpler approaches. It works by:</p> <ol> <li>Accumulate gradients along a straight-line path from a baseline (usually zero) to the input</li> <li>Compute an integral of gradients to capture the cumulative feature importance</li> <li>Use multiple interpolated steps between baseline and input for better approximation</li> <li>Satisfy important theoretical properties like completeness (attributions sum to the difference between output and baseline)</li> </ol> <p>Captum Documentation</p> <p>Paper: Axiomatic Attribution for Deep Networks</p> <pre><code>saliency = mt.attr.Saliency(explainable_model)\ninputxgrad = mt.attr.InputXGradient(explainable_model)\nig = mt.attr.IntegratedGradients(explainable_model)\n</code></pre> <p>To use these methods, we need to just call the <code>attribute</code> method on the interpreter model with this arguments (more details in the documentation):</p> <ul> <li><code>hsi</code> - the hyperspectral image data</li> <li><code>target</code> - the target index class to be analyzed: 0 for <code>P</code> (Phosphorus) class</li> </ul> <p>and to visualize the results, we can use one of the visualization methods provided by the <code>meteors</code> package.</p> <pre><code>%%time\nsaliency_attr = saliency.attribute(\n    hsi_0,\n    target=0,  # Class index to be analized\n)\nmt.visualize.visualize_attributes(saliency_attr, use_pyplot=True)\n</code></pre> <p></p> <pre><code>CPU times: user 13.1 s, sys: 1.99 s, total: 15.1 s\nWall time: 5.44 s\n</code></pre> <p>The plots that we use to visualize our attributions are created from the <code>visualize</code>. The <code>visualize_attributes</code> function visualizes the attribution with two spatial plots on top and two spectral plots below. The spatial plots present correlation of the spatial pixels with the output. If the correlation is negative (red), it means that this pixel lowered the model estimation for class <code>P</code>; if positive (green), it increased the prediction; and if around zero (white), then it was not impactful. In the lower plots, we see attributions aggregated per wavelength, showcasing how each wavelength correlated with the output - once again, negative values lowered, positive ones increased, and values close to zero were not impactful. These attributions are aggregated spatially and spectrally with mean over spatial or spectral axis.</p> <pre><code>%%time\ninputxgrad_attr = inputxgrad.attribute(hsi_0, target=0)\nmt.visualize.visualize_attributes(inputxgrad_attr, use_pyplot=True)\n</code></pre> <p></p> <pre><code>CPU times: user 13.3 s, sys: 1.95 s, total: 15.3 s\nWall time: 6.34 s\n</code></pre> <p>For the <code>IntegratedGradients</code> method, we may also also provid:</p> <ul> <li><code>return_convergence_delta</code> - if set to <code>True</code>, the method will return the convergence delta, which is the difference between the approximated integral and the true integral. This can be useful to assess the quality of the approximation. The value of the delta is stored in <code>score</code> attribute of the <code>Attribution</code> object.</li> <li><code>n_steps</code> - the number of steps to approximate the integral - more steps will probably yield better results but will also take longer to compute.</li> </ul> <pre><code>%%time\nig_attr = ig.attribute(\n    hsi_0,\n    target=0,\n    baseline=0.0,\n    return_convergence_delta=True,\n    n_steps=50,\n)\nprint(f\"Convergence Delta: {ig_attr.score}\")\nmt.visualize.visualize_attributes(ig_attr, use_pyplot=True)\n</code></pre> <pre><code>Convergence Delta: 0.0011369975069135917\n</code></pre> <p></p> <pre><code>CPU times: user 9min 3s, sys: 5min, total: 14min 3s\nWall time: 5min 45s\n</code></pre> <p>Each method calculates feature importance on a per-pixel basis, which can be visualized across both spatial and spectral domains. Our analysis of the three methods revealed distinct characteristics.</p> <p>The saliency method, while computationally fastest, produced noisy attributions and incorrectly highlighted importance in masked (zero-value) regions.</p> <p>The input \u00d7 gradient method offered clearer results with well-defined importance regions. It performed comparably to integrated gradients but with significantly faster computation time. The integrated gradients method, though computationally intensive, typically provided the most precise attributions. However, in this particular case, its results were not substantially better than those from the input \u00d7 gradient method. Its computation time can be adjusted through the number of integration steps, offering flexibility in balancing attribution quality against processing speed.</p>"},{"location":"tutorials/attr_showcase/#32-occlusion-based-methods","title":"3.2. Occlusion Based Methods","text":"<p>Occlusion-based methods offer an intuitive approach to understanding model predictions by systematically blocking (occluding) portions of the input and observing how the model's output changes. Unlike gradient methods, these techniques don't require access to model gradients, making them applicable to any model architecture.</p>"},{"location":"tutorials/attr_showcase/#occlusion","title":"Occlusion","text":"<p>This method works by:</p> <ol> <li>Slide a mask (window) across the input image</li> <li>Set the masked region to a baseline value (typically zero or mean)</li> <li>Measure the difference in model output between original and occluded inputs </li> <li>Create an attribution map where higher values indicate regions whose occlusion significantly impacts the prediction</li> </ol> <p>Captum Documentation</p> <p>Paper: Visualizing and Understanding Convolutional Networks</p> <p>For hyperspectral images, occlusion can be performed along the spectral dimension (blocking individual bands) or spatially (masking image regions).</p> <pre><code>occlusion = mt.attr.Occlusion(explainable_model)\n</code></pre> <p>occlusion method can be used with the following arguments:</p> <ul> <li><code>hsi</code> - the hyperspectral image data</li> <li><code>target</code> - the target index class to be analyzed: 0 for <code>P</code> (Phosphorus) class</li> <li><code>baseline</code> - the baseline value to replace the occluded region. It should be a float value.</li> <li><code>sliding_window_shapes</code> - the shape of the sliding window to occlude the image. It should be a tuple of three integers 3D occlusion.</li> <li><code>strides</code> - the strides of the sliding window. It should be a tuple of three integers 3D occlusion.</li> </ul> <pre><code>%%time\nocclusion_attr = occlusion.attribute(\n    hsi_0,\n    target=0,\n    baseline=0.0,\n    sliding_window_shapes=(50, 50, 50),\n    strides=(30, 30, 30),\n)\nmt.visualize.visualize_attributes(occlusion_attr, use_pyplot=True)\n</code></pre> <p></p> <pre><code>CPU times: user 25min 12s, sys: 1min 45s, total: 26min 57s\nWall time: 7min 6s\n</code></pre> <p>The visualizations are similar to those from gradient-based models; however, the granularity of the attributions is much smaller. This is due to the <code>sliding_window_shapes</code> and <code>strides</code> parameters. If we specified smaller <code>sliding_window_shapes</code>, we would get more granular attributions, and depending on the mask step (<code>strides</code>), we could also cover more or less pixels. The most important considerations when choosing the <code>strides</code> and <code>sliding_window_shapes</code> are that <code>strides</code> should not be bigger than <code>sliding_window_shapes</code>, as it is beneficial for masks to overlap minimally in every step. Additionally, we should start first with the large mask, as decreasing the mask size requires more steps to obtain the attributions, which means longer computation time.</p> <pre><code>%%time\nocclusion_spatial_attr = occlusion.get_spatial_attributes(\n    hsi_0, target=0, baseline=0.0, sliding_window_shapes=(50, 50), strides=(30, 30)\n)\nmt.visualize.visualize_spatial_attributes(occlusion_spatial_attr, use_pyplot=True)\n</code></pre> <p></p> <pre><code>CPU times: user 4min 44s, sys: 19.1 s, total: 5min 3s\nWall time: 59.1 s\n</code></pre> <p>The <code>Occlusion</code> method also allows us to calculate the mask only for spatial dimensions rather than for every pixel. The parameters remain the same, but notice that we set a 2D shape instead of a 3D shape. We don't need to pass 3D <code>sliding_window_shapes</code> and <code>strides</code> because for the spatial axes we set the shape automatically to the maximum. This means that the mask and the step will always cover all spectral pixels in the given spatial context, allowing for more specific spatial analysis.</p> <p>The visualization process is similar to before but now uses the <code>visualize_spatial_attributes</code> function, which produces three components: the original image on the left, an attribution map in the center (where colors and values represent the correlation with the output class), and a mask visualization on the right. Note that the displayed mask is just an simple version how mask behaves, as the <code>strides</code> is not larger than the mask size. In practice, the masks should always overlap, but for visualization simplicity, we assumed non-overlapping masks.</p> <pre><code>%%time\nocclusion_spectral_attr = occlusion.get_spectral_attributes(\n    hsi_0, target=0, baseline=0.0, sliding_window_shapes=(8), strides=(6)\n)\nmt.visualize.visualize_spectral_attributes(occlusion_spectral_attr, use_pyplot=True)\n</code></pre> <p></p> <pre><code>CPU times: user 2min 16s, sys: 8.48 s, total: 2min 25s\nWall time: 25.2 s\n</code></pre> <p>Just as we can perform analysis in the spatial context, we can also analyze the spectral dimension. For spectral analysis, we specify only one-dimensional shapes for <code>sliding_window_shapes</code> and <code>strides</code>, as the mask will cover all spatial pixels for each spectral position. The <code>visualize_spectral_attributes</code> function presents two plots: the left plot shows the attribution scores for each wavelength, colored by mask ID, while the right plot uses the same color coding but displays the attributions as a bar plot, with groups on the x-axis to better compare contributions across different mask IDs.</p> <p>TLDR: The occlusion method provides visualizations similar to gradient-based approaches but offers more granular control over spectral and spatial analysis. By selectively occluding regions, we can analyze either entire spatial or spectral regions independently. Although our results appear blurry due to large occlusion regions, they reveal clear patterns: spatial occlusion shows higher importance in non-masked regions as expected, while spectral analysis indicates that lower wavelengths have more positive importance and higher wavelengths have more negative importance. While the precision of results can be improved by using smaller occlusion regions, this comes at the cost of increased computation time.</p>"},{"location":"tutorials/attr_showcase/#33-perturbation-based-methods","title":"3.3. Perturbation Based Methods","text":"<p>Perturbation-based methods analyze model behavior by introducing controlled noise or variations to the input. These methods help understand model robustness and feature importance by observing how predictions change under different types of perturbations.</p> <p>Note: Both methods below require base attribution algorithms (e.g., Saliency, IntegratedGradients) to compute attributions for perturbed samples. Based on previous results, we'll use Saliency as the base method.</p>"},{"location":"tutorials/attr_showcase/#noisetunnel-smoothgrad","title":"NoiseTunnel (SmoothGrad)","text":"<p>A technique that reduces attribution noise by:</p> <ol> <li>Add Gaussian noise to the input multiple times</li> <li>Compute attributions for each noisy sample</li> <li>Average the results to produce smoother, more reliable attribution maps</li> </ol> <p>This approach helps eliminate random fluctuations and highlights consistently important features.</p> <p>Captum Documentation</p> <p>Paper: SmoothGrad: removing noise by adding noise</p>"},{"location":"tutorials/attr_showcase/#hypernoisetunnel","title":"HyperNoiseTunnel","text":"<p>HyperNoiseTunnel is a novel attribution method designed specifically for hyperspectral image analysis. Unlike traditional approaches like SmoothGrad that add Gaussian noise, this method works by strategically masking spectral bands with baseline values. This process can be controlled through either a probability of masking each band independently or by specifying a fixed number of bands to mask.</p> <p>The method generates perturbed samples that maintain the original hyperspectral data distribution while introducing meaningful variations. By preserving unmasked bands and systematically replacing selected ones with baseline values, it produces smoother attribution maps that retain important spectral relationships. This makes it particularly valuable for remote sensing applications, where understanding the contribution of specific spectral bands is crucial for model interpretation. </p> <p>The method's workflow proceeds as follows:</p> <ol> <li>Randomly mask spectral bands with baseline values instead of adding Gaussian noise</li> <li>Generate samples that better preserve the original hyperspectral data distribution</li> <li>Produce smoother attribution maps while maintaining spectral relationships</li> <li>Offer more interpretable results for hyperspectral applications compared to traditional noise-based methods</li> </ol> <p>This specialized approach provides insights into band-specific contributions while maintaining the physical meaning of spectral signatures.</p> <pre><code>base_attr = mt.attr.InputXGradient(explainable_model)\nnt_attr = mt.attr.NoiseTunnel(base_attr)\nhnt_attr = mt.attr.HyperNoiseTunnel(base_attr)\n</code></pre> <p>The <code>NoiseTunnel</code> and <code>HyperNoiseTunnel</code> methods can be used with the following arguments:</p> <ul> <li><code>hsi</code> - the hyperspectral image data</li> <li><code>target</code> - the target index class to be analyzed: 0 for <code>P</code> (Phosphorus) class</li> <li><code>n_samples</code> - the number of perturbed samples to generate</li> <li><code>method</code> - how the final aggregation of all the generatated perturbation attributions should be calculated</li> <li><code>baseline</code> - for the <code>HyperNoiseTunnel</code> method, the baseline value to replace the occluded region. It should be a float value.</li> </ul> <pre><code>%%time\nnt_attr = nt_attr.attribute(\n    hsi_0,\n    target=0,\n    n_samples=3,\n    method=\"smoothgrad\",\n)\nmt.visualize.visualize_attributes(nt_attr, use_pyplot=True)\n</code></pre> <p></p> <pre><code>CPU times: user 34.4 s, sys: 4.04 s, total: 38.5 s\nWall time: 7.2 s\n</code></pre> <p>Compared to previous methods and results in the spectral and spatial plots, we observe reduced number of important pixels. This method is specifically designed to eliminate noisy attributions that show high impact regardless of small perturbations, which is particularly evident in the spatial plots where only a few pixels deviate from white. </p> <p>While we set the number of perturbed samples (<code>n_samples</code>) to <code>3</code> in this example for demonstration purposes, this parameter significantly influences the results. Although increasing this value would lead to longer computation times, we encourage users to experiment with higher values and evaluate the resulting attributions to find the optimal balance between accuracy and computational efficiency.</p> <pre><code>%%time\nhnt_attr = hnt_attr.attribute(hsi_0, target=0, n_samples=50, baseline=0.0)\nmt.visualize.visualize_attributes(hnt_attr, use_pyplot=True)\n</code></pre> <p></p> <pre><code>CPU times: user 9min 3s, sys: 1min 18s, total: 10min 21s\nWall time: 2min 4s\n</code></pre> <p>The HyperNoiseTunnel can produce spatial results as shown above, but as described in the method overview, its spectral analysis capabilities are particularly noteworthy. The analysis reveals that lower wavelength bands have less influence on the method's output, while moving further along the spectral axis, the impact of \"removed\" pixels increases. Notably, the higher wavelengths show a strong negative influence on the output.</p> <p>To summarize, while the Noise Tunnel (SmoothGrad) method produces results similar to Input x Gradient, it offers distinct advantages. In spatial visualization, it effectively eliminates low-value variables, highlighting only the most significant patches. This suggests that some regions identified by Input x Gradient might be noise, while these highlighted patches are crucial for prediction. The HyperNoiseTunnel's spectral analysis demonstrates improved clarity and consistency compared to the standard Noise Tunnel, producing patterns more aligned with occlusion-based methods. This similarity to occlusion results indicates that HyperNoiseTunnel provides more reliable and interpretable spectral insights.</p>"},{"location":"tutorials/introduction/","title":"\ud83c\udf93 Introduction to Tutorials","text":"<p>Welcome to the Meteors tutorials! These tutorials are designed to help you get started with using Meteors for explaining and visualizing hyperspectral and multispectral images. Whether you're new to Meteors or have some experience, these tutorials will guide you through various features and techniques step by step.</p>"},{"location":"tutorials/introduction/#tutorial-list","title":"\ud83d\udcda Tutorial List","text":"<ol> <li> <p>LIME:</p> <ul> <li>HYPERVIEW Challenge</li> <li>Local Interpretable Model-agnostic Explanations (LIME) for hyperspectral images</li> <li>Vision Transformer (ViT) model for hyperspectral image regression</li> </ul> </li> <li> <p>Attribution Methods <code>attr</code>:</p> <ul> <li>HYPERVIEW Challenge</li> <li>Gradient, Occlusion and Perturbation based methods from <code>attr</code> for hyperspectral images</li> <li>Vision Transformer (ViT) model for hyperspectral image regression</li> </ul> </li> <li> <p>Segmentation:</p> <ul> <li>Segmentation of Sentinel-2 multispectral images from CloudSEN12 dataset</li> <li>U-Net model for semantic segmentation from CloudSEN12 authors</li> <li>LIME and Integrated Gradients for explaining segmentation predictions</li> </ul> </li> </ol>"},{"location":"tutorials/introduction/#getting-started","title":"\ud83d\ude80 Getting Started","text":"<p>To get started with the tutorials, make sure you have Meteors installed and set up on your machine. If you haven't done so already, follow the installation instructions in the Quickstart tutorial.</p> <p>Each tutorial is self-contained and focuses on a specific topic or technique. You can follow the tutorials in order or jump to the ones that interest you the most. The tutorials provide step-by-step instructions, code examples, and explanations to help you understand and apply the concepts effectively.</p>"},{"location":"tutorials/introduction/#tips-and-tricks","title":"\ud83d\udca1 Tips and Tricks","text":"<ul> <li>Take your time and experiment with the code examples provided in the tutorials.</li> <li>Don't hesitate to modify the code and try different parameters to see how they affect the results.</li> <li>Refer to the API Reference for detailed information on the Meteors functions and classes used in the tutorials.</li> <li>If you encounter any issues or have questions, feel free to reach out to the Meteors community or open an issue on the GitHub repository.</li> </ul>"},{"location":"tutorials/introduction/#lets-get-started","title":"\ud83c\udf89 Let's Get Started!","text":"<p>Ready to dive into the world of hyperspectral image explanation with Meteors? Choose a tutorial from the list above and start your journey!</p> <p>Happy learning and exploring with Meteors! \u2604\ufe0f\ud83d\udd0d\u2728</p>"},{"location":"tutorials/lime/","title":"Interpreting Hyperspectral Images with LIME: HYPERVIEW Challenge","text":"<p>This notebook demonstrates the usability of the Local Interpretable Model-agnostic Explanations (LIME) algorithm to interpret the predictions of a hyperspectral image regression model. The dataset used in this notebook is from the HYPERVIEW Challenge organized by The European Space Agency (ESA). The goal of the challenge is to predict <code>4</code> soil parameters based on the hyperspectral images of the ground.</p> <p>The model used in this notebook is one of the top-performing models in the challenge. The trained model architecture is based on the Vision Transformer (ViT) and CLIP (Contrastive Language-Image Pretraining), and its fine-tuned weights are open-sourced under the Apache License in the Hugging Face Model Hub. In the same place the original implementation of the CLIP model can be found. </p> <p>Note: Before running this notebook, make sure to install the required libraries used in the notebook. It should be sufficient to install the package <code>meteors</code> from PyPI, as it carry all the required dependencies, but in some cases, you might need to install additional ones. The <code>clip_model</code> module contains the code needed for additional preprocessing and model loading and can be downloaded from the Vignettes in the <code>meteors</code> repository</p>"},{"location":"tutorials/lime/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Loading the Model</li> <li>2. Loading the Hyperspectral data from HYPERVIEW Challenge</li> <li>3. Convert data into HSI image and preview the images</li> <li>4. Analyze HSI data with LIME</li> <li>4.1. Spatial Analysis</li> <li>4.2. Spectral Analysis</li> </ul> <pre><code>import os\nimport torch\nimport urllib\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\n\nimport meteors as mt\n\nfrom clip_utils import load_base_clip, download\n\n# Always try to set the seed for repeatability :)\ntorch.manual_seed(0)\n</code></pre> <pre><code>&lt;torch._C.Generator at 0x112dcd590&gt;\n</code></pre> <pre><code>device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n</code></pre> <pre><code>Using device: cpu\n</code></pre>"},{"location":"tutorials/lime/#1-loading-the-model","title":"1. Loading the Model","text":"<p>The dataset used for training this model can be found on the official page for the HYPERVIEW Challenge.</p> <p>The following code loads pre-trained CLIP weights and fine-tuned weights for the model.</p> <pre><code>download_root = os.path.expanduser(\"~/.cache/clip\")\nnum_classes = 4\n</code></pre> <p>Load the CLIP model with the HYPERVIEW head</p> <pre><code>model = load_base_clip(download_root=download_root, class_num=num_classes)\n</code></pre> <p>Load the pre-trained weights</p> <pre><code>vit_checkpoint_path = download(\n    \"https://huggingface.co/KPLabs/HYPERVIEW-VisionTransformer/resolve/main/VisionTransformer.pt\",\n    download_root,\n    error_checksum=False,\n)\nmodel.load_state_dict(torch.load(vit_checkpoint_path, map_location=device))\nmodel.eval()\nmodel = model.to(device)\n</code></pre>"},{"location":"tutorials/lime/#2-load-the-hyperspectral-data-from-hyperview-challenge","title":"2. Load the Hyperspectral data from HYPERVIEW Challenge","text":"<p>In this notebook, we will use the sample images from the HYPERVIEW Challenge dataset. The images are stored in the vignettes folder for the lime example in the <code>data</code> folder. The images are in the <code>.npy</code> format. The images are 3D hyperspectral images with 150 bands and various spatial dimensions. The images are stored in the raw format and contain both the hyperspectral image and the mask applied while training the model.</p> <p>First, we need to define the loading and preprocessing functions for the data.</p> <pre><code>def _shape_pad(data):\n    max_edge = np.max(data.shape[1:])\n    shape = (max_edge, max_edge)\n    padded = np.pad(\n        data,\n        ((0, 0), (0, (shape[0] - data.shape[1])), (0, (shape[1] - data.shape[2]))),\n        \"constant\",\n        constant_values=0.0,\n    )\n    return padded\n\n\ndef load_single_npz_image(image_path):\n    with np.load(image_path) as npz:\n        data = npz[\"data\"]\n        mask = npz[\"mask\"]\n\n        mask = 1 - mask.astype(int)\n\n        mask = _shape_pad(mask)\n        data = _shape_pad(data)\n\n        mask = mask.transpose((1, 2, 0))\n        data = data.transpose((1, 2, 0))\n        data = data / 5419\n\n        return data, mask\n\n\ndef get_eval_transform(image_shape):\n    return transforms.Compose(\n        [\n            transforms.Resize((image_shape, image_shape)),\n        ]\n    )\n</code></pre> <p>Now, let's load the hyperspectral image using functions we defined earlier.</p> <pre><code>data, mask = load_single_npz_image(\"data/0.npz\")\nmasked_data = data * mask\nmasked_data = torch.from_numpy(masked_data.astype(np.float32)).permute(2, 0, 1)\neval_tr = get_eval_transform(224)\n\nimage_torch = eval_tr(masked_data)\nnot_masked_image_torch = eval_tr(torch.from_numpy(data.astype(np.float32)).permute(2, 0, 1))\n\nprint(f\"Original data shape: {data.shape}\")\nprint(f\"Original mask shape: {mask.shape}\")\nprint(f\"Transformed data shape: {image_torch.shape}\")\n</code></pre> <pre><code>Original data shape: (89, 89, 150)\nOriginal mask shape: (89, 89, 150)\nTransformed data shape: torch.Size([150, 224, 224])\n</code></pre> <p>As we can see, the image is a 3D NumPy array with a shape transformed to (150, 224, 224), where 150 is the number of bands and 224x224 is the spatial dimension of the image. We will keep the image in both masked and unmasked formats.</p> <p>Now, we need to specify the wavelengths of the bands in the image. The wavelengths were provided in the challenge dataset, but to avoid loading additional files, we save it as a simple pythonic list.</p> <pre><code>with open(\"data/wavelenghts.txt\", \"r\") as f:\n    wavelengths = f.readline()\nwavelengths = [float(wave.strip()) for wave in wavelengths.split(\",\")]\n</code></pre>"},{"location":"tutorials/lime/#3-convert-data-into-hsi-image-and-preview-the-images","title":"3. Convert Data into HSI Image and Preview the Images","text":"<p>Now, having the raw data - the tensor representing the image, its wavelengths and the image orientation, we can to combine this information into a complete hyperspectral image. To create the hyperspectral image, we will use the <code>HSI</code> data class from the <code>meteors</code> package.</p> <p>The <code>HSI</code> (HyperSpectral Image) class takes the hyperspectral image data, the wavelength data, and the orientation of the image as input and creates the meaningful hyperspectral image, that can be easily analysed in any downstream task.</p> <p>Additionally, we may provide the binary mask, which may cover data irrelevant for the task, as suggested by the challenge dataset providers. In our case we cover the regions where there is land whose parameters we do not want to estimate, for instance some forests, roads or rivers. If we learned the model on such unmasked data, it could falsely underestimate the predictions, since the measured soil parameters in such uncultivated land is usually lower. We create a binary mask from the image, where 1 is the masked region and 0 is the unmasked region.</p> <pre><code>binary_mask = (image_torch &gt; 0.0).int()\n</code></pre> <p>The HSI object attributes:</p> <ul> <li><code>data</code>: Preprocessed hyperspectral image data (numpy/pytorch array, shape: bands \u00d7 height \u00d7 width)</li> <li><code>wavelengths</code>: List of hyperspectral image wavelengths</li> <li><code>orientation</code>: Image orientation, e.g. <code>CWH</code> (Channels \u00d7 Width \u00d7 Height)</li> <li><code>binary_mask</code>: Binary mask for the image</li> <li><code>device</code>: Storage device for image data (can be set later via <code>.to(device)</code>)</li> </ul> <pre><code>hsi_0 = mt.HSI(\n    image=not_masked_image_torch,\n    wavelengths=wavelengths,\n    orientation=\"CWH\",\n    binary_mask=binary_mask,\n    device=device,\n)\n</code></pre> <p>Now, let's view the hyperspectral image along with the masked and unmasked versions.</p> <pre><code>fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n\nmt.visualize.visualize_hsi(hsi_0, ax1, use_mask=True)\nax1.set_title(\"Masked Image\")\n\nax2.imshow(binary_mask[0, ...].T.cpu().numpy(), cmap=\"gray\")\nax2.set_title(\"Binary Mask\")\n\nmt.visualize.visualize_hsi(hsi_0, ax3, use_mask=False)\nax3.set_title(\"Unmasked Image\")\n\nfig.suptitle(\"Sample image from the HYPERVIEW dataset\")\nplt.show()\n</code></pre> <p></p> <p>The <code>HSI</code> dataclass streamlines hyperspectral image processing by automatically generating clean RGB visualizations from the complex hyperspectral data. This automation eliminates common manual tasks like wavelength band selection and image orientation correction. The dataclass handles the conversion from high-dimensional spectral data to a standard RGB representation, making it easier to visually inspect and validate the hyperspectral imagery before analysis.</p> <p>For model prediction, we can directly input the processed hyperspectral image into our classifier. The model performs multi-class classification across 4 distinct classes.</p> <pre><code>original_prediction = model(not_masked_image_torch.unsqueeze(0))\nhsi_prediction = model(hsi_0.image.unsqueeze(0))\nassert torch.allclose(original_prediction, hsi_prediction, atol=1e-3)\n</code></pre> <p>The classes of the HYPERVIEW dataset</p> <pre><code>prediction_dict = {0: \"Phosphorus\", 1: \"Potassium\", 2: \"Magnesium\", 3: \"pH\"}\n</code></pre> <pre><code>predictions = {prediction_dict[i]: float(hsi_prediction[0, i].cpu().detach().numpy()) for i in range(4)}\npredictions = pd.Series(predictions)\npredictions\n</code></pre> <pre><code>Phosphorus    0.210551\nPotassium     0.350670\nMagnesium     0.391935\npH            0.883228\ndtype: float64\n</code></pre>"},{"location":"tutorials/lime/#4-analyze-hsi-data-with-lime","title":"4. Analyze HSI Data with LIME","text":"<p>LIME (Local Interpretable Model-agnostic Explanations) is a model-agnostic algorithm that explains the predictions of a model by approximating the model's decision boundary around the prediction. It transforms the local area of the sample into interpretable space by creating superbands or superpixels which are the blocks that make up the sample. The algorithm generates a set of perturbed samples around the input sample and fits a linear model to the predictions of the perturbed samples. The linear model is then used to explain the prediction of the input sample.</p> <pre><code>img = Image.open(urllib.request.urlopen(\"https://ema.drwhy.ai/figure/lime_introduction.png\"))\nfig, ax = plt.subplots(figsize=(7, 7))\n\nax.imshow(img)\nax.axis(\"off\")\n\nplt.figtext(\n    0.5,\n    0.01,\n    'Illustration of LIME method idea. The colored areas corresponds to the descison regions of a complex classification model, which we aim to explain. The black cross indicates the sample (observation) of interest. Dots correspond to artificial data around the instance of interest.The dashed line represents a simple linear model fitted to the artificial data and explained model predictions. The simple surrogate model \"explains\" local behaviour of the black-box model around the instance of interest',\n    wrap=True,\n    horizontalalignment=\"center\",\n)\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>An image above with explanation of LIME idea is taken from the Explanatory Model Analysis book, by Przemyslaw Biecek and Tomasz Burzykowski (2021).</p> <p>To initialize the LIME object, we need to provide the following parameters:</p> <ul> <li><code>explainable_model</code>: The model to be explained, used to predict the hyperspectral image and the task type. In order to integrate any model with our methods, the model has to be wrapped in a class from the meteors package, providing additional info for the LIME explainer, such as the problem type</li> <li><code>interpretable_model</code>: The model used to approximate the decision boundary of the model. It could be any kind of easily interpretable model. The package provides implementations of Ridge and Lasso linear models ported from <code>sklearn</code> library and in this notebook we use a Lasso regression model</li> </ul> <p>Meteors package supports explaining the model solving 3 different machine learning problems:</p> <ul> <li>Regression</li> <li>Classification</li> <li>Segmentation</li> </ul> <p>Since the linear, interpretable models used in the package for creating the LIME explanations do not solve directly the segmentation problem, we apply a handy trick that converts this problem into a regression problem that can be solved by linear models. This idea is inspired from the <code>captum</code> library and involves appropriately counting pixels in the segmentation mask so that the number of pixels in each segment can be estimated by the regression model.</p> <p>First, lets wrap the model in the meteors model wrapper</p> <pre><code>explainable_model = mt.models.ExplainableModel(model, \"regression\")\n</code></pre> <p>next create the interpretable model with regularization strength of 0.001</p> <pre><code>interpretable_model = mt.models.SkLearnLasso(alpha=0.001)\n</code></pre> <p>Let's initialize the LIME explainer with the CLIP model!</p> <pre><code>lime = mt.attr.Lime(explainable_model, interpretable_model)\n</code></pre>"},{"location":"tutorials/lime/#4-analyze-hsi-data-with-lime_1","title":"4. Analyze HSI data with LIME","text":"<p>Our implementation of LIME explainer enables to analyze HSI data based on the spatial or spectral dimension. It allows to investigate which regions or which spectral bands are the most relevant for the model. The rest of the notebook will be divided into spectral and spatial analysis of HSI data.</p>"},{"location":"tutorials/lime/#41-spatial-analysis","title":"4.1. Spatial Analysis","text":"<p>Similar to the original implementation of LIME for images, we will create spatial superpixels which are perturbed and used to train a surrogate model for explaining. These explanations will produce a correlation map with the output for each superpixel. Firstly, to generate attributions, we need to prepare the segmentation mask. Essentially, the segmentation mask is a torch tensor or numpy ndarray that contains information to which superpixel (region) does the specified pixel belongs. Integer values in such tensor represent the labels of superpixels. The mask should have the same shape as the image, but should be repeated along the channels dimension. The package now supports three methods to create the mask:</p> <ul> <li>Using SLIC for superpixel detection</li> <li>Using patch segmentation. Knowing that ViT uses squared non-overlapping sliding windows, we can also make superpixels based on the same technique</li> <li>Providing a custom mask</li> </ul> <p>In this notebook we will present, how can we utilize different segmentation masks to investigate the model's performance and explore interesting areas in the images</p> <pre><code>segmentation_mask_slic = lime.get_segmentation_mask(hsi_0, segmentation_method=\"slic\")\nsegmentation_mask_patch = lime.get_segmentation_mask(hsi_0, segmentation_method=\"patch\", patch_size=14)\n</code></pre> <pre><code>fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n\nax1.imshow(segmentation_mask_slic[0, ...].T)\nax1.set_title(\"SLIC Mask Segmentation\")\n\nmt.visualize.visualize_hsi(hsi_0, ax2, use_mask=True)\nax2.set_title(\"Original HSI Image\")\n\nax3.imshow(segmentation_mask_patch[0, ...].T)\nax3.set_title(\"Patch Mask Segmentation\")\n\nfig.suptitle(\"Segmentation masks for the LIME explainer\")\n\nplt.show()\n</code></pre> <p></p> <p>Now, since we have our HSI sample data, segmentation mask and LIME model prepared, we will produce attribution maps for the segments. To do this, we simply need to execute one method <code>get_spatial_attributes</code>, and provide the HSI data, segmentation mask, and target class to be analyzed. If our model predicts more than one class (the model's output is multidimensional), we need to specify which target class we want to analyze. For each class, the analysis of the correlation with segments can be different.</p> <pre><code>spatial_attributes = lime.get_spatial_attributes(\n    hsi_0,\n    segmentation_mask_slic,\n    target=1,\n    n_samples=10,\n    perturbations_per_eval=4,\n)\n</code></pre> <p>method <code>get_spatial_attributes</code> apart from the 3 required fields:</p> <ul> <li><code>hsi</code> - the hyperspectral image data</li> <li><code>mask</code> - the segmentation mask</li> <li><code>target</code> - the target index class to be analyzed: K - potassium - 1</li> </ul> <p>takes as well few optional hyperparameters of explanations. Those are:</p> <ul> <li><code>n_samples</code> - it is a number of the generated artificial samples on which the linear model is trained. The larger number of samples, the explanations produced by the linear model are usually better, since its predictions should better mimic the predictions of the explained model. However, the larger <code>n_samples</code> the longer attribution takes to be performed</li> <li><code>perturbations_per_eval</code> - an inner batch size, this parameter may fasten the attribution, depending your machine capacity</li> <li><code>verbose</code> - a parameter specifying whether to output a progress bar, that makes the waiting time for attribution more pleasant</li> </ul> <p>The method has also an option to generate the segmentaion mask by itself, utilizing the static method <code>get_segmentation_mask</code> method under the hood.</p> <p>More information about the <code>get_spatial_attributes</code> function can be found in its reference page on the documentation website.</p> <p>The obtained <code>spatial_attributes</code> object is an object containing all the necessary data about the explanation. It consists of few fields:</p> <ul> <li><code>hsi</code> - a HyperSpectral Image object, </li> <li><code>mask</code> - here used for creation superpixels </li> <li><code>attributes</code> - explanations produced by the explainer of the same shape as the HSI</li> <li><code>score</code> - R2 score of the linear model used for the attribution</li> </ul> <p>Now, let us see how the attributions look like! </p> <p>Using the visualization capabilities provided by the <code>meteors</code> package, it is incredibely easy.</p> <pre><code>mt.visualize.visualize_spatial_attributes(spatial_attributes)\nplt.show()\n</code></pre> <p></p> <p>The plot presents three components (from the left)</p> <ol> <li>On the left, the original image provided to LIME.</li> <li>In the middle, the attribution map for each segment.</li> <li>On the right, the segmented mask with IDs of the segments. The number 0 represents the masked region, and because it surrounds each segment, the placement of the number is in the middle of the segment.</li> </ol> <p>The colors in the attribution map have the following meanings:</p> <ul> <li>Red: This superpixel is negatively correlated with the input. In our case, it means that the presence of this superpixel contributed to lowering the value for the target class 1.</li> <li>White: This segment did not have a significant impact on the output.</li> <li>Green: This superpixel is positively correlated with the output. Its presence increases the value for the class <code>1</code>.</li> </ul> <p>To validate how well the surrogate model was trained, we also provide the <code>score</code> attribute, which indicates the <code>R2</code> metric (coefficient of determination) that measures how well the surrogate model was trained.</p> <pre><code>spatial_attributes.score\n</code></pre> <pre><code>0.9498702883720398\n</code></pre> <p>High R2 score for the surrogate model hints that the attributions are sensible. In case the R2 score were very low, it could mean that the surrogate linear model can't tell which regions are more important for the explainable model preditions.</p> <p>Let's analyze the attribution maps for class <code>0</code> representing phosphorus estimation. We can simply modify the <code>target</code> parameter, utilizing the same segmentation mask, and rerun the explanation process.</p> <pre><code>spatial_attributes = lime.get_spatial_attributes(hsi_0, segmentation_mask_slic, target=0, n_samples=10)\n</code></pre> <pre><code>mt.visualize.visualize_spatial_attributes(spatial_attributes)\nprint(f\"R2 metric: {spatial_attributes.score:.4f}\")\nplt.show()\n</code></pre> <pre><code>R2 metric: 0.6901\n</code></pre> <p></p> <p>In our case, the green regions may correspond to areas with a higher concentration of the parameter being tested - here phosporus. </p>"},{"location":"tutorials/lime/#different-segmentation-masks","title":"Different segmentation masks","text":"<p>The most semantically meaningful segmentation mask is the one created by slic method. It contains superpixels that are created based on the image structure, which choice is very similiar to the regions that a human would choose</p> <p>However, <code>meteors</code> also supports creating the patch segmentation mask, but we are planning to increase support for different methods very soon.</p>"},{"location":"tutorials/lime/#patch-segmentation","title":"Patch segmentation","text":"<p>Patch segmentation mask tests the importance of regions shaped as rectangles. It is designed to work well with the ViT architecure, but it gives less semantic meaning.</p> <p>Let's check the <code>patch</code> segmentation mask and see how it affects the attribution maps.</p> <pre><code>spatial_attributes = lime.get_spatial_attributes(hsi_0, segmentation_mask_patch, target=1, n_samples=10)\n</code></pre> <pre><code>fig, ax = mt.visualize.visualize_spatial_attributes(spatial_attributes)\nprint(f\"R2 metric: {spatial_attributes.score:.4f}\")\nfig.suptitle(\"Patch Mask Segmentation for Potassium\")\nplt.show()\n</code></pre> <pre><code>R2 metric: 0.8699\n</code></pre> <p></p> <p>As we can see the Lime explainer focused on different regions of the image, which are not consistent with the previous results. Let's try using a larger number of perturbed samples. In this way, the linear model will be trained on much more perturbed versions of the original image and will be able to mimic the predictions of the explained model better.</p> <pre><code>spatial_attributes = lime.get_spatial_attributes(\n    hsi_0, segmentation_mask_patch, target=1, n_samples=100, perturbations_per_eval=10\n)\n</code></pre> <pre><code>fig, ax = mt.visualize.visualize_spatial_attributes(spatial_attributes)\nprint(f\"R2 metric: {spatial_attributes.score:.4f}\")\nfig.suptitle(\"LIME Explanation for Potassium with increased number of perturbed samples\")\nplt.show()\n</code></pre> <pre><code>R2 metric: 0.5538\n</code></pre> <p></p> <p>The explainer focuses again on the part of the image in the lower right corner and marks it as positively correlated with the output. It may suggests that the model indeed finds something interesting in this important region.</p>"},{"location":"tutorials/lime/#custom-segmentation-mask","title":"Custom segmentation mask","text":"<p>Additionally, an user might want to create their own segmentation mask, or modify the one created by the package.</p> <p>Therefore, we can inspect this lower right region more thoroughly by creating a more specific segmentation mask based on the slic one.</p> <pre><code>thorough_segmentation_mask_slic = segmentation_mask_slic.clone()\nthorough_segmentation_mask_slic[(thorough_segmentation_mask_slic != 10) &amp; (thorough_segmentation_mask_slic != 0)] = 1\n\nspatial_attributes = lime.get_spatial_attributes(\n    hsi_0, thorough_segmentation_mask_slic, target=1, n_samples=100, perturbations_per_eval=10\n)\n</code></pre> <pre><code>mt.visualize.visualize_spatial_attributes(spatial_attributes)\nprint(f\"R2 metric: {spatial_attributes.score:.4f}\")\n</code></pre> <pre><code>R2 metric: 0.9755\n</code></pre> <p></p> <p>it is visible, that this superpixel covers area that seems to be the most important region for the model. Why? This is another problem to be explained.</p>"},{"location":"tutorials/lime/#mask-importance","title":"Mask importance","text":"<p>Using an another custom segmentation mask, we can inspect, if binary mask covers regions that should not be relevant for the model. Now we will use the binary mask used for covering irrelevant regions as a segmentation mask to verify our hypothesis.</p> <p>Firstly, let's create a another HSI object, this time a plain image without any covering binary mask</p> <pre><code>image_without_mask = not_masked_image_torch.clone()\nhsi_without_mask = mt.HSI(\n    image=image_without_mask,\n    wavelengths=wavelengths,\n    orientation=\"CWH\",\n    device=device,\n)\n</code></pre> <p>Now, this image with the segmentation mask that consists of only two classes, we may explore, how each region is important for the model. To do so, we repeat this process for the potassium class.</p> <pre><code>segmentation_mask_from_binary = binary_mask + 1\nspatial_attributes = lime.get_spatial_attributes(\n    hsi_without_mask, segmentation_mask_from_binary, target=1, n_samples=10\n)\n</code></pre> <pre><code>fig, ax = mt.visualize.visualize_spatial_attributes(spatial_attributes)\nfig.suptitle(\"Importance of regions covered by the mask for Potassium class\")\nprint(f\"R2 metric: {spatial_attributes.score:.4f}\")\n</code></pre> <pre><code>R2 metric: 0.4493\n</code></pre> <p></p> <p>as we can see, the model correctly has learned that the relevant information is not covered with the mask. The region that was covered by default using the mask is strongly negatively correlated with the output, which may suggest that indeed, mask covers some regions, causing the model to output lower values for the estimated soil parameter</p>"},{"location":"tutorials/lime/#42-spectral-analysis","title":"4.2. Spectral Analysis","text":"<p>The spectral analysis is similar to the spatial one, but instead of analyzing the spatial dimension of the hyperspectral images, we analyze the spectral dimension - the image bands. In the process we group the specific channels of the image into superbands (groups of bands) and investigate importances of such groups. The spectral or band mask is a similar torch tensor or numpy ndarray as the segmentation mask, but instead of grouping regions it groups image channels. In the similar manner it is repeated along the width and height dimensions of the image.</p> <p>Since this kind of spectral analysis has sense only in hyperspectral imaginery, we paid special attention to this novel feature. As in the case of segmentation mask, user has several options how to create the band mask, to ensure that they had no difficulty using the analysis package and could rather focus on explaining the model. In the current package version user can:</p> <ul> <li>provide the spectral indices or indexes of the commmonly recognized bands</li> <li>specify exactly which wavelengths should compose the band mask</li> <li>specify which wavelength indices corresponding to the wavelength list from the explained HSI object should be used</li> </ul> <p>All these band masks can be obtained using one simple method <code>get_band_mask</code>, which detailed documentation may also be found in the reference. Now we will go through these different methods of creating the band mask and create some attributions.</p>"},{"location":"tutorials/lime/#band-and-indices-names","title":"Band and indices names","text":"<p>This, definetely the fastest for the user, method provides a quick way to explore importance of some well known superbands. To create the band mask using this approach, all we need to do is to pass a list or a dictionary of the band names:</p> <pre><code>band_mask, band_names = lime.get_band_mask(hsi_0, [\"R\", \"G\", \"B\"])\n</code></pre> <pre><code>\u001b[32m2024-11-18 00:18:29.065\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m827\u001b[0m - \u001b[33m\u001b[1mSegments G and B are overlapping, overlapping wavelengths will be assigned to only one\u001b[0m\n</code></pre> <p>this method outpus a tuple of two variables:</p> <ul> <li><code>band_mask</code> - a created band mask</li> <li><code>band_names</code> - a dictionary containing mapping from provided labels and segment indices</li> </ul> <p>Note The warning indicates that certain band groups have overlapping wavelengths. In such cases, each wavelength will be uniquely assigned to only one band group, avoiding duplicate assignments.</p> <p>In this case we created a band mask that contains 4 superpixels - one for each of the base colours and another one including all the background.</p> <pre><code>band_names\n</code></pre> <pre><code>{'R': 1, 'G': 2, 'B': 3}\n</code></pre> <pre><code>plt.scatter(wavelengths, band_mask.squeeze(1).squeeze(1))\n\nplt.title(\"Band Mask for the RGB bands\")\n\nplt.show()\n</code></pre> <p></p> <p>The plot presents how bands are grouped. The bands with the value 0 creates the additional band group <code>not_included</code> which also will be used in the analysis. You may also notice that the overlapping wavelengths are only assigned to one group.</p> <p>Now, we can analyze the hyperspectral image based on the spectral dimension. We will use the same LIME model as in the spatial analysis (initialize with the same parameters), but we will provide the band mask instead of the segmentation mask and also band names.</p> <pre><code>lime = mt.attr.Lime(\n    explainable_model=mt.models.ExplainableModel(model, \"regression\"),\n    interpretable_model=mt.models.SkLearnLasso(alpha=0.001),\n)\n</code></pre> <pre><code>spectral_attributes = lime.get_spectral_attributes(\n    hsi_0,\n    band_mask=band_mask,\n    target=1,\n    band_names=band_names,\n    n_samples=10,\n)\n</code></pre> <p>The spectral attributions are similar to spatial attributes consisting of <code>hsi</code>, <code>mask</code>, <code>attributes</code> and <code>score</code> of the linear model.</p> <pre><code>assert len(wavelengths) == spectral_attributes.flattened_attributes.shape[0]\nplt.scatter(wavelengths, spectral_attributes.flattened_attributes)\nplt.title(\"Spectral Attributes Map\")\nplt.show()\n</code></pre> <p></p> <p>But again as with spatial analysis it is much easier to visualize the results using the provided meteors visualization functions.</p> <pre><code>fig, ax = mt.visualize.visualize_spectral_attributes(spectral_attributes, show_not_included=True)\nfig.suptitle(\"Spectral Attributes for Potassium class and RGB bands\")\nplt.show()\n</code></pre> <p></p> <p>The plot this time consists of two parts. On the left, we have the attribution value per band for the hyperspectral image it helps to identify, which particular bands are important, whilist the right plot helps to identify the most important superbands and compare magnitudes of its importance. </p> <p>On the plot above, we may see that the red superband is much more important than the green and blue ones. How would the situation change if we compared red superband and blue and green superband together?</p> <p>Fortunately, thanks to the method <code>get_band_mask</code> it is incredibely easy - we just need to specify bands that will produce the superband.</p> <pre><code>band_mask, band_names = lime.get_band_mask(hsi_0, [\"R\", [\"G\", \"B\"]])\n</code></pre> <p>and as before we can create the attributions for the selected superbands using LIME explainer.</p> <pre><code>spectral_attributes = lime.get_spectral_attributes(\n    hsi_0,\n    band_mask=band_mask,\n    target=1,\n    band_names=band_names,\n    n_samples=10,\n)\n</code></pre> <pre><code>fig, ax = mt.visualize.visualize_spectral_attributes(spectral_attributes, show_not_included=True)\nfig.suptitle(\"Spectral Attributes for Potassium class and R and GB superbands\")\nplt.show()\n</code></pre> <p></p> <p>it looks that, indeed, green and blue superbands combined are more important than the red one.</p> <p>To validate the model, we again can use the <code>score</code> attribute, which indicates the <code>R2</code> metric of how well-trained the surrogate model was.</p> <pre><code>spectral_attributes.score\n</code></pre> <pre><code>0.65287184715271\n</code></pre> <p>All the band names are sourced from the Awesome Spectral Indices repository and handled using the <code>spyndex</code> library. Therefore, we can explore all the bands or try out some more exotic combinations using the predefined band indices here </p> <p>We will use now one of the indices taken from the library, a Bare Soil Index, which is a combination of couple of base bands. It can be defined as $$ BI = \\frac{(S1 + R) - (N + B)}{(S1 + R) + (N + B)} $$</p> <p>where S1 corresponds to SWIR 1 band, R and B to red and blue respectively and N to NIR band. It is used, as the name suggests, to detect bare soil in the hyperspectral imaginery and possibly can be used as well to detect soil parameters.</p> <pre><code>band_mask, band_names = lime.get_band_mask(hsi_0, [\"G\", \"BI\"])\n</code></pre> <pre><code>\u001b[32m2024-11-18 00:18:59.549\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m827\u001b[0m - \u001b[33m\u001b[1mSegments G and BI are overlapping, overlapping wavelengths will be assigned to only one\u001b[0m\n</code></pre> <p>now, using the same methods as before, we can attribute the new superbands using the LIME explainer and visualize the output</p> <pre><code>band_names\n</code></pre> <pre><code>{'G': 1, 'BI': 2}\n</code></pre> <pre><code>spectral_attributes = lime.get_spectral_attributes(\n    hsi_0,\n    band_mask=band_mask,\n    target=1,\n    band_names=band_names,\n    n_samples=10,\n)\n</code></pre> <pre><code>mt.visualize.visualize_spectral_attributes(spectral_attributes, show_not_included=True)\nprint(f\"R2 metric: {spectral_attributes.score:.4f}\")\nplt.show()\n</code></pre> <pre><code>R2 metric: 0.9540\n</code></pre> <p></p> <p>it seems that the superband BI is actually irrelevant for the model in this specific case. It looks that the model does not base its predictions for the current image on this specific bands</p> <p>In this way, we may investigate if the model uses the bands that were commonly used for the similar tasks in the literature, which could help us debbuging the model!  Now we will use some bands, that should really be important to the model.</p>"},{"location":"tutorials/lime/#wavelengths-ranges","title":"Wavelengths ranges","text":"<p>In some cases, we do not want to use any well known band combinations. In our team, we had access to knowledge of domain experts who gave us the exact wavelength values that are used to detect potassium, phosphorus, magnessium and pH in the soil. Now we can utilize this knowledge and create our own superbands. </p> <p>Now we will try out the values for the potassium. Unfortunately, not all the wavelengths provided are exactly mentioned in our wavelengths list, thus we need to find the closest corresponding indices to the values given by the experts.</p> <pre><code>potassium_superband_indices = [0, 1, 4, 10, 43, 46, 47]\npotassium_superband_wavelengths = [wavelengths[i] for i in potassium_superband_indices]\n</code></pre> <pre><code>potassium_superband_wavelengths\n</code></pre> <pre><code>[462.08, 465.27, 474.86, 494.04, 599.53, 609.12, 612.32]\n</code></pre> <pre><code>band_dict = {\"potassium\": potassium_superband_indices, \"another_superpixel\": [i for i in range(20, 30)]}\nband_dict\n</code></pre> <pre><code>{'potassium': [0, 1, 4, 10, 43, 46, 47],\n 'another_superpixel': [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]}\n</code></pre> <pre><code>band_mask, band_names = lime.get_band_mask(hsi_0, band_indices=band_dict)\n</code></pre> <pre><code>band_names\n</code></pre> <pre><code>{'potassium': 1, 'another_superpixel': 2}\n</code></pre> <pre><code>spectral_attributes = lime.get_spectral_attributes(\n    hsi_0,\n    band_mask=band_mask,\n    target=1,\n    band_names=band_names,\n    n_samples=100,\n)\n</code></pre> <pre><code>mt.visualize.visualize_spectral_attributes(spectral_attributes, show_not_included=True)\nprint(f\"R2 metric: {spectral_attributes.score:.4f}\")\nplt.show()\n</code></pre> <pre><code>R2 metric: 0.4785\n</code></pre> <p></p> <p>As it turns out, in this case, the bands given us by the experts are not necessarily important for the model. We need to remember, that this analysis is performed for solely one image. Perhaps, this is just one outlier and in different cases model might actually use the specified bands. For such cases, we can utilize the global explanations which are attributions aggregeated for multiple input images. </p>"},{"location":"tutorials/lime/#global-attributions","title":"Global attributions","text":"<p>An interesting capability unique to spectral analysis is the ability to aggregate results across multiple samples, allowing us to transition from local interpretation to global interpretation. This is usually not possible for spatial analysis, as the images differ significantly when it comes to the covered land. Aggregating spatial information is challenging due to the lack of straightforward method for determining which parts of different images are similar, as the covered land can vary significantly. On the contrary, spectral analysis benefits from consistent bands accross images, allowing for specification of common superbands.</p> <p>To give an idea how to perform such analysis, we need a second sample of the hyperspectral image.</p> <pre><code>data, mask = load_single_npz_image(\"data/1.npz\")\nmasked_data = data * mask\nmasked_data = torch.from_numpy(masked_data.astype(np.float32)).permute(2, 0, 1)\neval_tr = get_eval_transform(224)\n\nimage_torch_1 = eval_tr(masked_data)\nnot_masked_image_torch_1 = eval_tr(torch.from_numpy(data.astype(np.float32)).permute(2, 0, 1))\n</code></pre> <pre><code>binary_mask_1 = (image_torch_1 &gt; 0.0).int()\n\nhsi_1 = mt.HSI(\n    image=not_masked_image_torch_1, wavelengths=wavelengths, orientation=\"CWH\", binary_mask=binary_mask_1, device=device\n)\n\nax = mt.visualize.visualize_hsi(hsi_1, use_mask=True)\nax.set_title(\"Another sample image from the HYPERVIEW dataset\")\n</code></pre> <pre><code>Text(0.5, 1.0, 'Another sample image from the HYPERVIEW dataset')\n</code></pre> <p></p> <p>Now, once the image is properly loaded and preprocessed, let's get the attributions for the second sample, using the same band mask as before</p> <pre><code>spectral_attributes_1 = lime.get_spectral_attributes(\n    hsi_1, band_mask=band_mask, target=1, band_names=band_names, n_samples=100\n)\n</code></pre> <pre><code>mt.visualize.visualize_spectral_attributes(spectral_attributes, show_not_included=True)\nprint(f\"R2 metric: {spectral_attributes.score:.4f}\")\nplt.show()\n</code></pre> <pre><code>R2 metric: 0.4785\n</code></pre> <p></p> <p>To get the global interpretation we will provide the list of attributions to the meteors visualizer to create the global interpretation visualization.</p> <pre><code>mt.visualize.visualize_spectral_attributes([spectral_attributes, spectral_attributes_1], show_not_included=True)\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>As it turns out, the model does not necessarily use the specified bands in the prediction of the potassium class. This is probably insufficient to conclude anything using only attributions from 2 images, especially because the <code>score</code> of the explanations was low, but our model suprisingly does not use the expected wavelengths.</p>"},{"location":"tutorials/segmentation/","title":"Cloud Segmentation with Meteors","text":"<p>This notebook presents the methods from the <code>meteors</code> package that can be used for explaining the remote sensing models implementing the segmentation problem. In the notebook we will walk through: - reformulation of the segmentation problem into regression - LIME explanations - Gradient-based explanations on the example of Integrated Gradients</p> <p>The objective of a model, implementing such task, is to divide the input image into similar spatial regions.  In this case, we will focus on cloud segmentation for multispectral imagery - the explained model will simply divide the input image into clouds or terrain.</p> <p>We'll be using the <code>UNetMobV2_V1</code> model to perform the cloud segmentation, which is based on research presented in the paper CloudSEN12: A Global Dataset for Semantic Understanding of Cloud and Cloud Shadow in Sentinel-2.</p> <p>Note: Before running this notebook, make sure to install all the required libraries used in the notebook. It should be sufficient to install the newest version of the package meteors from PyPI, as it carry all the required dependencies. This can be done with a single line:</p> <p><code>pip install meteors</code></p> <p>In some cases, however, a specific setup on your machine might require installing some additional ones. The cloudsen12_models module contains all the code needed for additional preprocessing and model loading. You can download it directly from the Vignettes in the <code>meteors</code> repository.</p>"},{"location":"tutorials/segmentation/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Loading the Model</li> <li>2. Loading Sample Images</li> <li>3. Interpreting the Model</li> <li>3.1. Interpreting the Model with LIME</li> <li>3.2. Interpreting the Model with Gradient Methods</li> </ul>"},{"location":"tutorials/segmentation/#1-loading-the-model","title":"1. Loading the Model","text":"<p>We'll use a modified version of the <code>UNetMobV2_V1</code> model from the <code>cloudsen12_models</code> module, which contains code sourced from github repository xai4space/cloudsen12_models, where you can find a modification of the original UNetMobV2_V1 model implementation, which was slighlty adjusted for the PyTorch inputs and outputs and have reduced dependencies. This implementation has been adapted to ork seamlessly with PyTorch inputs and outputs, while preserving the gradients and have reduced external dependencies.</p> <pre><code>from cloudsen12_models import cloudsen12\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport meteors as mt\nimport torch\n\nimport meteors.visualize as vis\n</code></pre> <p>Firstly, let's set the random seed for reproducibility of the results</p> <pre><code>torch.manual_seed(44)\nnp.random.seed(44)\n</code></pre> <pre><code>model = cloudsen12.load_model_by_name(name=\"UNetMobV2_V1\", weights_folder=\"cloudsen12_models\")\ncloudsen12_segment_interpretation = [\"clear\", \"Thick cloud\", \"Thin cloud\", \"Cloud shadow\"]\n</code></pre>"},{"location":"tutorials/segmentation/#2-loading-sample-images","title":"2. Loading Sample Images","text":""},{"location":"tutorials/segmentation/#loading-the-images","title":"Loading the images","text":"<p>To faciliate the analysis and allow for better reproduction of the examples, the images used in this notebook are already preprocessed and ready to be segmented. The original image is sourced from the Google Earth Engine, from a COPERNICUS/S2_HARMONIZED collection and a S2A_MSIL1C_20240417T064631_N0510_R020_T40RCN_20240417T091941 tile. </p> <p>The image was later preprocessed according to the code from this notebook.</p> <pre><code>img = np.load(\"data/0.npz\")[\"img\"]\n\nwith open(\"data/wavelenghts.txt\", \"r\") as f:\n    sentinel_central_wavelengths = f.readline()\nsentinel_central_wavelengths = [float(wave.strip()) for wave in sentinel_central_wavelengths.split(\",\")]\n</code></pre> <p>The Meteros package provides an easy-to-use wrapper for <code>HSI</code> hyperspectral images. To take full advantage of the wrapper's potential, we also need to include wavelengths of the object. </p> <p>The <code>HSI</code> data object can be easily visualized in RGB format.</p> <pre><code>hsi = mt.HSI(image=img, wavelengths=sentinel_central_wavelengths)\n</code></pre> <p>The original image is stored in the <code>image</code> attribute of the <code>HSI</code> object. Let's now test that the model can predict cloud segmentation on this image.</p> <pre><code>cloudmask = model.predict(hsi.image.unsqueeze(0))\ncloudmask_hard_labels = torch.argmax(cloudmask, dim=1).type(torch.uint8)\n</code></pre> <p>The model output is a 4-channel tensor, where each channel represents a different class (soft labels). To get the final prediction, we need to use the <code>argmax</code> function to select the class with the highest probability.</p> <pre><code>cloudmask_hard_labels.shape\n</code></pre> <pre><code>torch.Size([1, 622, 916])\n</code></pre> <p>Let's now visualize the model's prediction on the image.</p> <pre><code>fig, ax = plt.subplots(1, 2, figsize=(14, 5), sharey=True, tight_layout=True)\ncloudsen12.plot_cloudSEN12mask(cloudmask_hard_labels.squeeze(), ax[1])\nax[1].set_title(\"Cloud mask\")\n\nvis.visualize_hsi(hsi, ax[0])\nax[0].set_title(\"input Sentinel-2 image\")\nfig.suptitle(\"Cloud mask segmentation\", fontsize=16)\n\nplt.show()\n</code></pre> <p></p>"},{"location":"tutorials/segmentation/#3-model-analysis","title":"3 Model analysis","text":"<p>The XAI methods implemented in the <code>meteors</code> package are designed for creating explanations for single-output problems, usually classification or regression. However, the segmentation models ouput multi-dimensional arrays, which cannot be analysed easily by standard XAI methods. </p> <p>Fortunately, using a simple trick, we can create a low-dimentional output, by postprocessing the model's outputs. The postprocessing is usually aggregation performed by the region type, and in short, it comes down to simply counting pixels in each class. The <code>meteors</code> package provides a simple aggregation function <code>agg_segmentation_postprocessing</code> for this purpose, but feel free to experiment with other aggregation functions as well.</p> <pre><code>from meteors.utils import agg_segmentation_postprocessing\n\npostprocessing = agg_segmentation_postprocessing(\n    soft_labels=True,\n    classes_numb=len(cloudsen12_segment_interpretation),\n    class_axis=1,\n)\n</code></pre> <p>The <code>agg_segmentation_postprocessing</code> function requires several parameters to properly handle different model outputs. Let's examine each parameter in detail:</p> <ul> <li><code>soft_labels</code> parameter specifies whether the analysed model outputs soft labels (probabilities of pixels belonging to each class) or hard labels (one class is assigned for each pixel). In case the model uses hard labels, the shape of the output should be 3 dimensional, with batch size as its first dimension. The output's shape should be 4 dimensional otherwise</li> <li><code>classes_numb</code> is a parameter specifying how many classes are predicted by the model</li> <li><code>class_axis</code> used only for soft labels. It specifies on which axis the class dimension is. If the <code>class_axis</code> equals to 0, then batch size should be at the second dimension.</li> </ul> <p>Let's now use the <code>agg_segmentation_postprocessing</code> function to convert the model's output into a single numerical output. We'll then use this output to interpret the model with meteors' XAI methods.</p> <pre><code>aggregated_segmentation_mask = postprocessing(cloudmask)\naggregated_segmentation_mask\n</code></pre> <pre><code>tensor([[461402.5625,  50634.7188,      0.0000,  33250.5391]],\n       grad_fn=&lt;SumBackward1&gt;)\n</code></pre> <pre><code>cloudmask.shape\n</code></pre> <pre><code>torch.Size([1, 4, 622, 916])\n</code></pre> <pre><code>aggregated_segmentation_mask.shape\n</code></pre> <pre><code>torch.Size([1, 4])\n</code></pre> <pre><code>plt.bar(np.arange(4), aggregated_segmentation_mask.detach().squeeze())\nplt.xticks(np.arange(4), cloudsen12_segment_interpretation)\nplt.title(\"Aggregated cloudSEN12 segmentation\")\nplt.ylabel(\"Number of pixels\")\nplt.xlabel(\"Class\")\n</code></pre> <pre><code>Text(0.5, 0, 'Class')\n</code></pre> <p></p> <p>The <code>agg_segmentation_postprocessing</code> function is used to aggregate the model's pixel-wise predictions into a single numerical output. An important thing to note is that the sum of the aggregated values is not necessarily needs to equal to the number of pixels in the image, in case there are soft labels used. In that case the aggregation function takes the probabilities of the argmax class for each pixel and sums them up. </p> <p>Now that we tested how to aggregate the model's output, we can proceed with the explanation. One last thing is to properly initialize the model inside the <code>meteors</code> environment. This is done by creating an object that contains all the necessary information to perform an explanation, which is:</p> <ul> <li>the forward function of the model</li> <li>the problem type, here <code>segmentation</code></li> <li>in case the problem type is <code>segmentation</code>, the required parameter is the <code>postprocessing_output</code>, which is a method for aggregating the outputs as we did before.</li> </ul> <pre><code>explainable_model = mt.models.ExplainableModel(\n    model.predict, problem_type=\"segmentation\", postprocessing_output=postprocessing\n)\n</code></pre> <p>And let's check that model forward function now returns the aggregated output with the gradients.</p> <pre><code>aggregated_segmentation_mask = explainable_model(hsi.image.unsqueeze(0))\naggregated_segmentation_mask\n</code></pre> <pre><code>tensor([[461402.5625,  50634.7188,      0.0000,  33250.5391]],\n       grad_fn=&lt;SumBackward1&gt;)\n</code></pre>"},{"location":"tutorials/segmentation/#31-interpreting-the-model-with-lime","title":"3.1 Interpreting the Model with LIME","text":"<p>We'll use the LIME (Local Interpretable Model-agnostic Explanations) method to understand our model's behavior. For a comprehensive understanding of LIME and its implementation in <code>meteors</code>, refer to our LIME tutorial.</p> <p>The intepretable model of LIME is a linear model LASSO model with a regularization parameter of 0.001.</p> <pre><code>interpretable_model = mt.models.SkLearnLasso(alpha=0.01)\nlime = mt.attr.Lime(explainable_model=explainable_model, interpretable_model=interpretable_model)\n</code></pre>"},{"location":"tutorials/segmentation/#spatial-analysis","title":"Spatial analysis","text":"<p>Firstly, let us verify, which image regions contribute the most to the specific class predictions - we will perform spatial analysis of the model. LIME explanation method requires the input feature space to be low-dimensional and to satisfy this condition, we will create a LIME segmentation mask, which will help understanding LIME which regions are more relevant for the cloudsen model. Please note here, that t</p> <p>Now, we will create the LIME segmentation mask, and later create attributions for the class 0 - the clear sky.</p> <p>We will use the <code>SLIC</code> algorithm to generate this mask. The <code>SLIC</code> algorithm is a superpixel segmentation method that groups pixels into regions based on their similarity.</p> <p>Note: The segmentation mask created for the LIME method is something completely different from the output created by the cloudsen model.</p> <pre><code>lime_seg_mask = lime.get_segmentation_mask(hsi, segmentation_method=\"slic\", num_interpret_features=50)\n</code></pre> <p>method <code>get_spatial_attributes</code> apart from the 3 required fields:</p> <ul> <li><code>hsi</code> - the hyperspectral image data</li> <li><code>mask</code> - the segmentation mask</li> <li><code>target</code> - the target index class to be analyzed: 0 for clear</li> </ul> <p>takes as well few optional hyperparameters of explanations. Where one of the most important is:</p> <ul> <li><code>n_samples</code> - it is a number of the generated artificial samples on which the linear model is trained. The larger number of samples, the explanations produced by the linear model are usually better, since its predictions should better mimic the predictions of the explained model. However, the larger <code>n_samples</code> the longer attribution takes to be performed</li> </ul> <pre><code>attributes = lime.get_spatial_attributes(\n    hsi,\n    target=0,\n    segmentation_mask=lime_seg_mask,\n    n_samples=100,\n)\n</code></pre> <pre><code>\u001b[32m2024-11-21 01:19:46.869\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.explainer\u001b[0m:\u001b[36mdevice\u001b[0m:\u001b[36m143\u001b[0m - \u001b[33m\u001b[1mNot a torch model, setting device to cpu\u001b[0m\n</code></pre> <p>to validate how well the trained surrogate model predicts the explained model outputs, we can use the <code>score</code> attribute of the explanations:</p> <pre><code>print(f\"Score of the attribution: {attributes.score}\")\n</code></pre> <pre><code>Score of the attribution: 0.9968178272247314\n</code></pre> <p>Score of the LIME attribution is the $R^2$ score of the interpretable model predictions. The value of this parameter is between 0 and 1, the larger the value the better the explanations. We can see, that this explanation is particulary good.</p> <p>And let's now visualize the LIME spatial explanation.</p> <pre><code>fig, ax = mt.visualize.visualize_spatial_attributes(attributes, use_pyplot=False)\nfig.suptitle(\"LIME explanation for the UNetMobV2_V1 model and class 0 (clear sky)\")\n\nax[2].clear()\ncloudsen12.plot_cloudSEN12mask(cloudmask_hard_labels, ax=ax[2])\nax[2].set_title(\"UNetMobV2_V1 segmentation\")\nfig.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>The figure above is a slight modification of the baseline figure provided by the <code>meteors</code> pacakge, which summarizes the spatial analysis for a specific image. </p> <p>The colors in the attribution map, presented in the center, have the following meaning:</p> <ul> <li>Red: This superpixel is negatively correlated with the input. In our case, it means that the presence of this superpixel contributed to lowering the value for the target class <code>0</code> (clear terrain).</li> <li>White: This segment did not have a significant impact on the output.</li> <li>Green: This superpixel is positively correlated with the output. Its presence increases the value for the class <code>0</code>.</li> </ul> <p>As we can see in the figure above, the most relevant regions are those with the actual clouds. This observation is confirmation that the model actually works properly - in case the regions with high absolute attribution values are perturbed, model perfoms the segmentation worse.</p> <pre><code>attributes = lime.get_spatial_attributes(hsi, target=1, segmentation_mask=lime_seg_mask, n_samples=100)\n\nfig, ax = mt.visualize.visualize_spatial_attributes(attributes, use_pyplot=False)\nfig.suptitle(\"LIME explanation for the UNetMobV2_V1 model and class 1 (thick cloud)\")\nax[2].clear()\ncloudsen12.plot_cloudSEN12mask(cloudmask_hard_labels, ax=ax[2])\nax[2].set_title(\"UNetMobV2_V1 segmentation\")\nprint(f\"Score of the attribution: {attributes.score}\")\nfig.tight_layout()\nplt.show()\n</code></pre> <pre><code>Score of the attribution: 0.9983850717544556\n</code></pre> <p></p> <p>For the class thick cloud, the situation is very similar. Again the regions with the actual clouds contribute the most to the prediction. This time, the attribution is positive, which suggests that modification in these regions increases the number of pixels classified as thick cloud class. This result is consistent with the previous one.</p>"},{"location":"tutorials/segmentation/#spectral-analysis","title":"Spectral analysis","text":"<p>Now, we will perform spectral analysis, which focuses on different bands and their impact on the models predictions. For the multispectral images, we can analyse the impact of all the bands seperately, or group them into superbands. Firstly, let's investigate, which of the bands are the most important.</p> <p>To do so, we will need to create a band mask - a similar object to the lime segmentation mask, but with aggregations in the spectral dimension.</p> <pre><code>band_indices = {\n    \"B01\": 0,\n    \"B02\": 1,\n    \"B03\": 2,\n    \"B04\": 3,\n    \"B05\": 4,\n    \"B06\": 5,\n    \"B07\": 6,\n    \"B08\": 7,\n    \"B8A\": 8,\n    \"B09\": 9,\n    \"B10\": 10,\n    \"B11\": 11,\n    \"B12\": 12,\n}\n\nband_mask, band_names = lime.get_band_mask(hsi, band_indices=band_indices)\nattributes = lime.get_spectral_attributes(hsi, target=0, band_mask=band_mask, band_names=band_names, n_samples=100)\n</code></pre> <p>Once the attributions are calculated, we can easily visualize them as previously but now focusing on the spatial explanations.</p> <pre><code>mt.visualize.visualize_spectral_attributes(attributes)\nprint(f\"Score of the attribution: {attributes.score}\")\nfig = plt.gcf()\nfig.suptitle(\"LIME explanation for the UNetMobV2_V1 model and class 0 (clear)\")\nplt.show()\n</code></pre> <pre><code>Score of the attribution: 0.45737916231155396\n</code></pre> <p></p> <p>In the chart above, we can see which of the specified bands contribute the most to the model's prediction. On the left, we can exactly see the correlation with the output and the bands wavelenghts, whereas plot on the right allows to easily compare the attribution values. </p> <p>As of the paper on the dataset CloudSEN12 used in model training, for segmentation of the class clear, the experts used primarily the visible bands: B02, B03, B04, and NIR band B08. This is consisistent with our explanations, even though the score of the explanations is low.</p> <p>Similarily, as in the case of the spatial explanations, we can also analyse easily different classes, by passing a different target value to the function. In this case, we will analyse the thick cloud class (index 1).</p> <pre><code>attributes = lime.get_spectral_attributes(\n    hsi,\n    target=1,\n    band_mask=band_mask,\n    band_names=band_names,\n    n_samples=200,\n    perturbations_per_eval=4,\n)\n\nmt.visualize.visualize_spectral_attributes(attributes)\nprint(f\"Score of the attribution: {attributes.score}\")\nfig = plt.gcf()\nfig.suptitle(\"LIME explanation for the UNetMobV2_V1 model and class 1 (thick cloud)\")\nplt.show()\n</code></pre> <pre><code>Score of the attribution: 0.32696086168289185\n</code></pre> <p></p>"},{"location":"tutorials/segmentation/#32-interpreting-the-model-with-gradient-methods","title":"3.2. Interpreting the Model with Gradient Methods","text":"<p>To further understand the model's behavior, we can use any other attribution method implemented in the package. More detailed description of such methods can be found in vignette on various attribution methods. These methods allows to analyze how the model's predictions change with respect to the input image, usually providing a pixel-wise attribution of the model's output, showing which pixels are the most influential in the model's decision-making process. </p> <p>In this tutorial we will use one of many implemented methods in the package - the Integrated Gradients, a powerful gradient-based attribution technique with methematical foundations. For detailed description of the method, please refer to the original paper.</p> <pre><code>ig = mt.attr.IntegratedGradients(explainable_model=explainable_model)\nattributes = ig.attribute(hsi, target=0, n_steps=10)\n</code></pre> <pre><code>fig, ax = vis.visualize_attributes(attributes)\nprint(f\"Convergence of the integrated gradients: {attributes.score}\")\nfig.suptitle(\"Integrated Gradient explanation for the UNetMobV2_V1 model and class 0 (clear)\")\n</code></pre> <pre><code>Convergence of the integrated gradients: None\n\n\n\n\n\nText(0.5, 0.98, 'Integrated Gradient explanation for the UNetMobV2_V1 model and class 0 (clear)')\n</code></pre> <p></p> <p>Even though the Integrated Gradient methods hasn't converged, we can read the attributions and analyze the model. The attribution visualization provides two complementary perspectives on model behavior:</p> <p>Top Images - Spatial Attribution</p> <ul> <li>Shows spatialy which regions in the image influence model predictions</li> <li>Helps identify areas crucial for detecting specific features (e.g., cloud edges or clear sky)</li> <li>Intensity indicates strength of influence on the prediction</li> </ul> <p>Bottom Images - Spectral Attribution</p> <ul> <li>Reveals which spectral bands are most important for predictions</li> <li>Highlights bands that provide strong cloud/ground contrast</li> <li>Helps understand which wavelengths are key for feature discrimination</li> </ul> <p>The similar attributions can be calculated for another classes, e.g. cloud shadow.</p> <pre><code>attributes = ig.attribute(hsi, target=3, n_steps=10)\nprint(f\"Convergence of the integrated gradients: {attributes.score}\")\nfig, ax = vis.visualize_attributes(attributes)\nfig.suptitle(\"Integrated Gradient explanation for the UNetMobV2_V1 model and class 3 (cloud shadow)\")\n</code></pre> <pre><code>Convergence of the integrated gradients: None\n\n\n\n\n\nText(0.5, 0.98, 'Integrated Gradient explanation for the UNetMobV2_V1 model and class 3 (cloud shadow)')\n</code></pre> <p></p> <p>Looking at the attribution map for class 3 (cloud shadow), we can observe a strong correlation between areas of actual cloud shadows in the original image and regions of high attribution values. Additionaly we see that in the spectral context the most important bands are the first ones, wheras the further bands are not as important.</p>"}]}