
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../introduction/">
      
      
        <link rel="next" href="../../reference/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.36">
    
    
      
        <title>LIME - Meteors</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.06209087.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#interpreting-hyperspectral-images-with-lime-hyperview-challenge" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <div data-md-color-scheme="default" data-md-component="outdated" hidden>
        
      </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Meteors" class="md-header__button md-logo" aria-label="Meteors" data-md-component="logo">
      
  <img src="../../assets/logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Meteors
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              LIME
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Meteors" class="md-nav__button md-logo" aria-label="Meteors" data-md-component="logo">
      
  <img src="../../assets/logo.png" alt="logo">

    </a>
    Meteors
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    üè† Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../quickstart/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    üöÄ Quickstart
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    üìö Tutorials
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            üìö Tutorials
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../introduction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    üéì Introduction
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    LIME
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    LIME
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#table-of-contents" class="md-nav__link">
    <span class="md-ellipsis">
      Table of Contents
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1-loading-the-model" class="md-nav__link">
    <span class="md-ellipsis">
      1. Loading the Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-load-the-hyperspectral-data-from-hyperview-challenge" class="md-nav__link">
    <span class="md-ellipsis">
      2. Load the Hyperspectral data from HYPERVIEW Challenge
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-convert-data-into-hsi-image-and-preview-the-images" class="md-nav__link">
    <span class="md-ellipsis">
      3. Convert Data into HSI Image and Preview the Images
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-analyze-hsi-data-with-lime" class="md-nav__link">
    <span class="md-ellipsis">
      4. Analyze HSI Data with LIME
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-analyze-hsi-data-with-lime_1" class="md-nav__link">
    <span class="md-ellipsis">
      4. Analyze HSI data with LIME
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4. Analyze HSI data with LIME">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41-spatial-analysis" class="md-nav__link">
    <span class="md-ellipsis">
      4.1. Spatial Analysis
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4.1. Spatial Analysis">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#different-segmentation-masks" class="md-nav__link">
    <span class="md-ellipsis">
      Different segmentation masks
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Different segmentation masks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#patch-segmentation" class="md-nav__link">
    <span class="md-ellipsis">
      Patch segmentation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#custom-segmentation-mask" class="md-nav__link">
    <span class="md-ellipsis">
      Custom segmentation mask
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Custom segmentation mask">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mask-importance" class="md-nav__link">
    <span class="md-ellipsis">
      Mask importance
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42-spectral-analysis" class="md-nav__link">
    <span class="md-ellipsis">
      4.2. Spectral Analysis
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4.2. Spectral Analysis">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#band-and-indices-names" class="md-nav__link">
    <span class="md-ellipsis">
      Band and indices names
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wavelengths-ranges" class="md-nav__link">
    <span class="md-ellipsis">
      Wavelengths ranges
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#global-attributions" class="md-nav__link">
    <span class="md-ellipsis">
      Global attributions
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../reference/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    üìñ API Reference
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../changelog/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    üìù Changelog
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../how-to-guides/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ü§ù How to Contribute
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#table-of-contents" class="md-nav__link">
    <span class="md-ellipsis">
      Table of Contents
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1-loading-the-model" class="md-nav__link">
    <span class="md-ellipsis">
      1. Loading the Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-load-the-hyperspectral-data-from-hyperview-challenge" class="md-nav__link">
    <span class="md-ellipsis">
      2. Load the Hyperspectral data from HYPERVIEW Challenge
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-convert-data-into-hsi-image-and-preview-the-images" class="md-nav__link">
    <span class="md-ellipsis">
      3. Convert Data into HSI Image and Preview the Images
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-analyze-hsi-data-with-lime" class="md-nav__link">
    <span class="md-ellipsis">
      4. Analyze HSI Data with LIME
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-analyze-hsi-data-with-lime_1" class="md-nav__link">
    <span class="md-ellipsis">
      4. Analyze HSI data with LIME
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4. Analyze HSI data with LIME">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41-spatial-analysis" class="md-nav__link">
    <span class="md-ellipsis">
      4.1. Spatial Analysis
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4.1. Spatial Analysis">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#different-segmentation-masks" class="md-nav__link">
    <span class="md-ellipsis">
      Different segmentation masks
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Different segmentation masks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#patch-segmentation" class="md-nav__link">
    <span class="md-ellipsis">
      Patch segmentation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#custom-segmentation-mask" class="md-nav__link">
    <span class="md-ellipsis">
      Custom segmentation mask
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Custom segmentation mask">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mask-importance" class="md-nav__link">
    <span class="md-ellipsis">
      Mask importance
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42-spectral-analysis" class="md-nav__link">
    <span class="md-ellipsis">
      4.2. Spectral Analysis
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4.2. Spectral Analysis">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#band-and-indices-names" class="md-nav__link">
    <span class="md-ellipsis">
      Band and indices names
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wavelengths-ranges" class="md-nav__link">
    <span class="md-ellipsis">
      Wavelengths ranges
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#global-attributions" class="md-nav__link">
    <span class="md-ellipsis">
      Global attributions
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="interpreting-hyperspectral-images-with-lime-hyperview-challenge">Interpreting Hyperspectral Images with LIME: HYPERVIEW Challenge</h1>
<p>This notebook demonstrates the usability of the Local Interpretable Model-agnostic Explanations (LIME) algorithm to interpret the predictions of a hyperspectral image regression model. The dataset used in this notebook is from the <a href="https://ai4eo.eu/challenge/hyperview-challenge/">HYPERVIEW Challenge</a> organized by The European Space Agency (ESA). The goal of the challenge is to predict soil parameters based on the hyperspectral images of the ground.</p>
<p>The model used in this notebook is one of the top-performing models in the challenge. The trained model architecture is based on the Vision Transformer (ViT) and CLIP (Contrastive Language-Image Pretraining), and its fine-tuned weights are open-sourced under the Apache License in the <a href="https://huggingface.co/KPLabs/HYPERVIEW-VisionTransformer">Hugging Face Model Hub</a>. In the same place the original implementation of the CLIP model can be found. </p>
<p><strong>Note</strong>: Before running this notebook, make sure to install the required libraries used in the notebook. It should be sufficient to install the package <code>meteors</code> from PyPI, as it carry all the required dependencies, but in some cases, you might need to install additional ones. The <code>clip_model</code> module contains the code needed for additional preprocessing and model loading and can be downloaded from the <a href="https://github.com/xai4space/meteors/tree/main/examples/lime">Vignettes in the <code>meteors</code> repository</a></p>
<h2 id="table-of-contents">Table of Contents</h2>
<ul>
<li><a href="#1-loading-the-model">1. Loading the Model</a></li>
<li><a href="#2-load-the-hyperspectral-data-from-hyperview-challenge">2. Loading the Hyperspectral data from HYPERVIEW Challenge</a></li>
<li><a href="#3-convert-data-into-hsi-image-and-preview-the-images">3. Convert data into HSI image and preview the images</a></li>
<li><a href="#4-analyze-hsi-data-with-lime">4. Analyze HSI data with LIME</a></li>
<li><a href="#41-spatial-analysis">4.1. Spatial Analysis</a></li>
<li><a href="#42-spectral-analysis">4.2. Spectral Analysis</a></li>
</ul>
<pre><code class="language-python">import torch
import numpy as np
from tqdm import tqdm
from torchvision import transforms
import matplotlib.pyplot as plt
import pandas as pd

import meteors as mt

from clip_model import build_model

torch.manual_seed(0)  # set seed for reproducibility
</code></pre>
<pre><code>&lt;torch._C.Generator at 0x24d39bf3290&gt;
</code></pre>
<pre><code class="language-python">device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
</code></pre>
<h2 id="1-loading-the-model">1. Loading the Model</h2>
<p>The dataset used for training this model can be found on the official page for the <a href="https://ai4eo.eu/challenge/hyperview-challenge/">HYPERVIEW Challenge</a>.</p>
<p>The following code snippet includes additional functions to load pre-trained CLIP weights and fine-tuned weights for the model.</p>
<pre><code class="language-python">import os
import hashlib
import urllib
import warnings


BASE_MODEL_URL = &quot;https://openaipublic.azureedge.net/clip/models/b8cca3fd41ae0c99ba7e8951adf17d267cdb84cd88be6f7c2e0eca1737a03836/ViT-L-14.pt&quot;


def download(url: str, root: str, error_checksum: bool = True) -&gt; str:
    os.makedirs(root, exist_ok=True)
    filename = os.path.basename(url)

    expected_sha256 = url.split(&quot;/&quot;)[-2]
    if expected_sha256 == &quot;main&quot;:
        expected_sha256 = &quot;0cc03ba20aff35a41312de47da843a0d8541acba3c2101d9691f3ab999128d34&quot;  # CLIP sha256
    download_target = os.path.join(root, filename)

    if os.path.exists(download_target) and not os.path.isfile(download_target):
        raise RuntimeError(f&quot;{download_target} exists and is not a regular file&quot;)

    if os.path.isfile(download_target):
        # real_sha256 = hashlib.sha256(open(download_target, &quot;rb&quot;).read()).hexdigest()
        # print(&quot;INFO: Real SHA256: {}&quot;.format(real_sha256))
        # print(&quot;INFO: Expected SHA256: {}&quot;.format(expected_sha256))
        if hashlib.sha256(open(download_target, &quot;rb&quot;).read()).hexdigest() == expected_sha256:
            return download_target
        else:
            warnings.warn(f&quot;{download_target} exists, but the SHA256 checksum does not match; re-downloading the file&quot;)

    with urllib.request.urlopen(url) as source, open(download_target, &quot;wb&quot;) as output:
        with tqdm(
            total=int(source.info().get(&quot;Content-Length&quot;)), ncols=80, unit=&quot;iB&quot;, unit_scale=True, unit_divisor=1024
        ) as loop:
            while True:
                buffer = source.read(8192)
                if not buffer:
                    break

                output.write(buffer)
                loop.update(len(buffer))

    if hashlib.sha256(open(download_target, &quot;rb&quot;).read()).hexdigest() != expected_sha256:
        if error_checksum:
            raise RuntimeError(&quot;Model has been downloaded but the SHA256 checksum does not not match&quot;)
        else:
            warnings.warn(&quot;Model has been downloaded but the SHA256 checksum does not not match&quot;)

    return download_target


def load_base_clip(download_root: str, class_num: int = 1000):
    model_path = download(BASE_MODEL_URL, download_root)
    model = torch.jit.load(model_path, map_location=&quot;cpu&quot;).eval()
    model = build_model(model.state_dict(), downstream_task=4, class_num=class_num)
    model.float()
    return model
</code></pre>
<pre><code class="language-python">download_root = os.path.expanduser(&quot;~/.cache/clip&quot;)  # Change this to the directory where you want to download the model
num_classes = 4  # Number of classes in the original HYPERVIEW dataset

# Load the CLIP model with the HYPERVIEW head
model = load_base_clip(download_root=download_root, class_num=num_classes)

# Load the pre-trained weights
vit_checkpoint_path = download(
    &quot;https://huggingface.co/KPLabs/HYPERVIEW-VisionTransformer/resolve/main/VisionTransformer.pt&quot;,
    download_root,
    error_checksum=False,
)
model.load_state_dict(torch.load(vit_checkpoint_path, map_location=device))
model.eval()
model = model.to(device)
</code></pre>
<pre><code>C:\Users\tymot\AppData\Local\Temp\ipykernel_13308\1125051604.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(vit_checkpoint_path, map_location=device))
</code></pre>
<h2 id="2-load-the-hyperspectral-data-from-hyperview-challenge">2. Load the Hyperspectral data from HYPERVIEW Challenge</h2>
<p>In this notebook, we will use the sample images from the HYPERVIEW Challenge dataset. The images are stored in the <a href="https://github.com/xai4space/meteors/tree/main/examples/lime">vignettes folder for the lime example</a> in the <code>data</code> folder. The images are in the <code>.npy</code> format. The images are 3D hyperspectral images with 150 bands and various spatial dimensions. The images are stored in the raw format and contain both the hyperspectral image and the mask applied while training the model.</p>
<p>First, we need to define the loading and preprocessing functions for the data.</p>
<pre><code class="language-python">def _shape_pad(data):
    max_edge = np.max(data.shape[1:])
    shape = (max_edge, max_edge)
    # padded = np.pad(data, ((0, 0), (0, (shape[0] - data.shape[1])), (0, (shape[1] - data.shape[2]))), &quot;wrap&quot;)
    padded = np.pad(
        data,
        ((0, 0), (0, (shape[0] - data.shape[1])), (0, (shape[1] - data.shape[2]))),
        &quot;constant&quot;,
        constant_values=0.0,
    )
    return padded


def load_single_npz_image(image_path):
    with np.load(image_path) as npz:
        data = npz[&quot;data&quot;]
        mask = npz[&quot;mask&quot;]

        mask = 1 - mask.astype(int)

        mask = _shape_pad(mask)
        data = _shape_pad(data)

        mask = mask.transpose((1, 2, 0))
        data = data.transpose((1, 2, 0))
        data = data / 5419

        return data, mask


def get_eval_transform(image_shape):
    return transforms.Compose(
        [
            transforms.Resize((image_shape, image_shape)),
        ]
    )
</code></pre>
<p>Now, let's load the hyperspectral image using functions we defined earlier.</p>
<pre><code class="language-python">data, mask = load_single_npz_image(&quot;data/0.npz&quot;)
masked_data = data * mask
masked_data = torch.from_numpy(masked_data.astype(np.float32)).permute(2, 0, 1)
eval_tr = get_eval_transform(224)

image_torch = eval_tr(masked_data)
not_masked_image_torch = eval_tr(torch.from_numpy(data.astype(np.float32)).permute(2, 0, 1))

print(f&quot;Original data shape: {data.shape}&quot;)
print(f&quot;Original mask shape: {mask.shape}&quot;)
print(f&quot;Transformed data shape: {image_torch.shape}&quot;)
</code></pre>
<pre><code>Original data shape: (89, 89, 150)
Original mask shape: (89, 89, 150)
Transformed data shape: torch.Size([150, 224, 224])
</code></pre>
<p>As we can see, the image is a 3D NumPy array with a shape transformed to (150, 224, 224), where 150 is the number of bands and 224x224 is the spatial dimension of the image. We will keep the image in both masked and unmasked formats.</p>
<p>Now, we need to specify the wavelengths of the bands in the image. The wavelengths were provided in the challenge dataset, but to avoid loading additional files, we save it as a simple pythonic list</p>
<pre><code class="language-python">wavelengths = [
    462.08,
    465.27,
    468.47,
    471.67,
    474.86,
    478.06,
    481.26,
    484.45,
    487.65,
    490.85,
    494.04,
    497.24,
    500.43,
    503.63,
    506.83,
    510.03,
    513.22,
    516.42,
    519.61,
    522.81,
    526.01,
    529.2,
    532.4,
    535.6,
    538.79,
    541.99,
    545.19,
    548.38,
    551.58,
    554.78,
    557.97,
    561.17,
    564.37,
    567.56,
    570.76,
    573.96,
    577.15,
    580.35,
    583.55,
    586.74,
    589.94,
    593.14,
    596.33,
    599.53,
    602.73,
    605.92,
    609.12,
    612.32,
    615.51,
    618.71,
    621.91,
    625.1,
    628.3,
    631.5,
    634.69,
    637.89,
    641.09,
    644.28,
    647.48,
    650.67,
    653.87,
    657.07,
    660.27,
    663.46,
    666.66,
    669.85,
    673.05,
    676.25,
    679.45,
    682.64,
    685.84,
    689.03,
    692.23,
    695.43,
    698.62,
    701.82,
    705.02,
    708.21,
    711.41,
    714.61,
    717.8,
    721.0,
    724.2,
    727.39,
    730.59,
    733.79,
    736.98,
    740.18,
    743.38,
    746.57,
    749.77,
    752.97,
    756.16,
    759.36,
    762.56,
    765.75,
    768.95,
    772.15,
    775.34,
    778.54,
    781.74,
    784.93,
    788.13,
    791.33,
    794.52,
    797.72,
    800.92,
    804.11,
    807.31,
    810.51,
    813.7,
    816.9,
    820.1,
    823.29,
    826.49,
    829.68,
    832.88,
    836.08,
    839.28,
    842.47,
    845.67,
    848.86,
    852.06,
    855.26,
    858.46,
    861.65,
    864.85,
    868.04,
    871.24,
    874.44,
    877.63,
    880.83,
    884.03,
    887.22,
    890.42,
    893.62,
    896.81,
    900.01,
    903.21,
    906.4,
    909.6,
    912.8,
    915.99,
    919.19,
    922.39,
    925.58,
    928.78,
    931.98,
    935.17,
    938.37,
]
</code></pre>
<h2 id="3-convert-data-into-hsi-image-and-preview-the-images">3. Convert Data into HSI Image and Preview the Images</h2>
<p>Now, having the raw data - the tensor representing the image, its wavelengths and the image orientation, we can to combine this information into a complete hyperspectral image. To create the hyperspectral image, we will use the <code>HSI</code> data class from the <code>meteors</code> package.</p>
<p>The <code>HSI</code> (HyperSpectral Image) class takes the hyperspectral image data, the wavelength data, and the orientation of the image as input and creates the meaningful hyperspectral image, that can be easily analysed in any downstream task.</p>
<p>Additionally, we may provide the binary mask, which may cover data irrelevant for the task, as suggested by the challenge dataset providers. In our case we cover the regions where there is land whose parameters we do not want to estimate, for instance some forests, roads or rivers. If we learned the model on such unmasked data, it could faslely underestimate the predictions, since the measured soil parameters in such uncultivated land is usually lower.</p>
<pre><code class="language-python"># Create a binary mask from the image, where 1 is the masked region and 0 is the unmasked region
binary_mask = (image_torch &gt; 0.0).int()

hsi_0 = mt.HSI(
    image=not_masked_image_torch,  # The preprocessed image saved as tensor/numpy ndarray
    wavelengths=wavelengths,  # The wavelengths list of the hyperspectral image
    orientation=&quot;CWH&quot;,  # The orientation of the image tensor, here it is CWH (Channels, Width, Height)
    binary_mask=binary_mask,  # The binary mask tensor/numpy
    device=device,  # The device where the image data will be stored. We can provide it later with the .to(device) method
)
</code></pre>
<p>Now, let's view the hyperspectral image along with the masked and unmasked versions.</p>
<pre><code class="language-python">fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))

mt.visualize.visualize_hsi(hsi_0, ax1, use_mask=True)
ax1.set_title(&quot;Masked Image&quot;)

ax2.imshow(binary_mask[0, ...].T.cpu().numpy(), cmap=&quot;gray&quot;)
ax2.set_title(&quot;Binary Mask&quot;)

mt.visualize.visualize_hsi(hsi_0, ax3, use_mask=False)
ax3.set_title(&quot;Unmasked Image&quot;)

fig.suptitle(&quot;Sample image from the HYPERVIEW dataset&quot;)
plt.show()
</code></pre>
<p><img alt="png" src="../lime_files/lime_15_0.png" /></p>
<p>The <code>HSI</code> dataclass automatically provides us the clean RGB chart of the hyperspectral image and releases us from the obligation of selecting the specific wavelengths to be plotted and considering the image orientation.</p>
<p>Now, we can provide the hyperspectral image to the model and get the prediction. The model will return the predictions for the 4 classes of the hyperspectral image.</p>
<pre><code class="language-python">original_prediction = model(not_masked_image_torch.unsqueeze(0))  # original image
hsi_prediction = model(hsi_0.image.unsqueeze(0))  # Hsi image
assert torch.allclose(original_prediction, hsi_prediction, atol=1e-3)

prediction_dict = {0: &quot;Phosphorus&quot;, 1: &quot;Potassium&quot;, 2: &quot;Magnesium&quot;, 3: &quot;pH&quot;}  # The classes of the HYPERVIEW dataset

predictions = {prediction_dict[i]: float(hsi_prediction[0, i].cpu().detach().numpy()) for i in range(4)}
predictions = pd.Series(predictions)
predictions
</code></pre>
<pre><code>Phosphorus    0.210551
Potassium     0.350670
Magnesium     0.391935
pH            0.883228
dtype: float64
</code></pre>
<h2 id="4-analyze-hsi-data-with-lime">4. Analyze HSI Data with LIME</h2>
<p><a href="https://dl.acm.org/doi/abs/10.1145/2939672.2939778">LIME</a> (Local Interpretable Model-agnostic Explanations) is a model-agnostic algorithm that explains the predictions of a model by approximating the model's decision boundary around the prediction. It transforms the local area of the sample into interpretable space by creating superbands or superpixels which are the blocks that make up the sample. The algorithm generates a set of perturbed samples around the input sample and fits a linear model to the predictions of the perturbed samples. The linear model is then used to explain the prediction of the input sample.</p>
<figure>
<center><img src="assets/lime_introduction.png" width="500" align="center"/></center>
<figcaption align = "center">Illustrarion of LIME method idea. The colored areas corresponds to the descison regions of a complex classification model, which we aim to explain. The black cross indicates the sample (observation) of interest. Dots correspond to artificial data around the instance of interest.The dashed line represents a simple linear model fitted to the artificial data and explained model predictions. The simple surrogate model "explains" local behaviour of the black-box model around the instance of interest
</figcaption>
</figure>

<p>An image above with explanation of LIME idea is taken from the <em>Explanatory Model Analysis</em> book, by Przemyslaw Biecek and Tomasz Burzykowski (2021)</p>
<p>To initialize the LIME object, we need to provide the following parameters:
- <code>explainable_model</code>: The model to be explained, used to predict the hyperspectral image and the task type. In order to integrate any model with our methods, the model has to be wrapped in a class from the meteors package, providing additional info for the LIME explainer, such as the problem type.
- <code>interpretable_model</code>: The model used to approximate the decision boundary of the model. It could be any kind of easily interpretable model. The package provides implementations of Ridge and Lasso linear models ported from <code>sklearn</code> library and in this notebook we use a Lasso regression model. </p>
<p>Meteors package supports explaining the model solving 3 different machine learning problems:
- regression
- classification
- segmentation</p>
<p>Since the linear, interpretable models used in the package for creating the LIME explanations do not solve directly the segmentation problem, we apply a handy trick that converts this problem into a regression problem that can be solved by linear models. This idea is inspired from the <code>captum</code> library and involves appropriately counting pixels in the segmentation mask so that the number of pixels in each segment can be estimated by the regression model.</p>
<p>Let's initialize the LIME explainer with the CLIP model!</p>
<pre><code class="language-python">explainable_model = mt.utils.models.ExplainableModel(model, &quot;regression&quot;)  # The model to be explained
interpretable_model = mt.utils.models.SkLearnLasso(
    alpha=0.001
)  # The interpretable model with regularization strength of 0.001

lime = mt.attr.Lime(explainable_model, interpretable_model)  # The LIME explainer
</code></pre>
<h2 id="4-analyze-hsi-data-with-lime_1">4. Analyze HSI data with LIME</h2>
<p>Our implementation of LIME explainer enables to analyze HSI data based on the spatial or spectral dimension. It allows to investigate which regions or which spectral bands are the most relevant for the model. The rest of the notebook will be divided into spectral and spatial analysis of HSI data.</p>
<h3 id="41-spatial-analysis">4.1. Spatial Analysis</h3>
<p>Similar to the original implementation of LIME for images, we will create spatial superpixels which are perturbed and used to train a surrogate model for explaining. These explanations will produce a correlation map with the output for each superpixel.
Firstly, to generate attributions, we need to prepare the segmentation mask. Essentially, the segmentation mask is a torch tensor or numpy ndarray that contains information to which superpixel (region) does the specified pixel belongs. Integer values in such tensor represent the labels of superpixels. The mask should have the same shape as the image, but should be repeated along the channels dimension. The package now supports three methods to create the mask:
- Using <a href="https://scikit-image.org/docs/stable/auto_examples/segmentation/plot_segmentations.html">SLIC</a> for superpixel detection
- Using patch segmentation. Knowing that ViT uses squared non-overlapping sliding windows, we can also make superpixels based on the same technique
- Providing a custom mask</p>
<p>In this notebook we will present, how can we utilize different segmentation masks to investigate the model's performance and explore interesting areas in the images</p>
<pre><code class="language-python">segmentation_mask_slic = lime.get_segmentation_mask(hsi_0, segmentation_method=&quot;slic&quot;)
segmentation_mask_patch = lime.get_segmentation_mask(hsi_0, segmentation_method=&quot;patch&quot;, patch_size=14)
</code></pre>
<pre><code class="language-python">fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))

ax1.imshow(segmentation_mask_slic[0, ...].T)
ax1.set_title(&quot;SLIC Mask Segmentation&quot;)

mt.visualize.visualize_hsi(hsi_0, ax2, use_mask=True)
ax2.set_title(&quot;Original HSI Image&quot;)

ax3.imshow(segmentation_mask_patch[0, ...].T)
ax3.set_title(&quot;Patch Mask Segmentation&quot;)

fig.suptitle(&quot;Segmentation masks for the LIME explainer&quot;)

plt.show()
</code></pre>
<p><img alt="png" src="../lime_files/lime_22_0.png" /></p>
<p>Now, since we have our HSI sample data, segmentation mask and LIME model prepared, we will produce attribution maps for the segments. To do this, we simply need to execute one method <code>get_spatial_attributes</code>, and provide the HSI data, segmentation mask, and target class to be analyzed. If our model predicts more than one class (the model's output is multidimensional), we need to specify which target class we want to analyze. For each class, the analysis of the correlation with segments can be different.</p>
<pre><code class="language-python">spatial_attributes = lime.get_spatial_attributes(
    hsi_0,  # HSI data
    segmentation_mask_slic,  # Segmentation mask
    target=1,  # class analyzed: K - potassium
    n_samples=10,  # Number of perturbation, the more the better explainer is trained but the analsys is slower
    perturbations_per_eval=4,  # Number of perturbations evaluated at once
)
</code></pre>
<p>method <code>get_spatial_attributes</code> apart from the 3 required fields, takes as well few optional hyperparameters of explanations. Those are:
- <code>n_samples</code> - it is a number of the generated artificial samples on which the linear model is trained. The larger number of samples, the explanations produced by the linear model are usually better, since its predictions should better mimic the predictions of the explained model. However, the larger <code>n_samples</code> the longer attribution takes to be performed
- <code>perturbations_per_eval</code> - an inner batch size, this parameter may fasten the attribution, depending your machine capacity
- <code>verbose</code> - a parameter specifying whether to output a progress bar, that makes the waiting time for attribution more pleasant</p>
<p>The method has also an option to generate the segmentaion mask by itself, utilizing the static method <code>get_segmentation_mask</code> method under the hood.</p>
<p>More information about the <code>get_spatial_attributes</code> function can be found in its <a href="https://xai4space.github.io/meteors/latest/reference/#src.meteors.attr.Lime.get_spatial_attributes">reference page</a> on the documentation website.</p>
<p>The obtained <code>spatial_attributes</code> object is an object containing all the necessary data about the explanation. It consists of few fields:
- <code>hsi</code> - a HyperSpectral Image object, 
- <code>mask</code> - here used for creation superpixels 
- <code>attributes</code> - explanations produced by the explainer of the same shape as the HSI
- <code>score</code> - R2 score of the linear model used for the attribution</p>
<p>Now, let us see how the attributions look like! </p>
<p>Using the visualization capabilities provided by the <code>meteors</code> package, it is incredibely easy.</p>
<pre><code class="language-python">mt.visualize.visualize_spatial_attributes(spatial_attributes)
plt.show()
</code></pre>
<p><img alt="png" src="../lime_files/lime_28_0.png" /></p>
<p>The plot presents three components (from the left)
1. On the left, the original image provided to LIME.
2. In the middle, the attribution map for each segment.
3. On the right, the segmented mask with IDs of the segments. The number 0 represents the masked region, and because it surrounds each segment, the placement of the number is in the middle of the segment.</p>
<p>The colors in the attribution map have the following meanings:
- <strong>Red</strong>: This superpixel is negatively correlated with the input. In our case, it means that the presence of this superpixel contributed to lowering the value for the target class 1.
- <strong>White</strong>: This segment did not have a significant impact on the output.
- <strong>Green</strong>: This superpixel is positively correlated with the output. Its presence increases the value for the class <code>1</code>.</p>
<p>To validate how well the surrogate model was trained, we also provide the <code>score</code> attribute, which indicates the <code>R2</code> metric (coefficient of determination) that measures how well the surrogate model was trained.</p>
<pre><code class="language-python">spatial_attributes.score
</code></pre>
<pre><code>0.9498704075813293
</code></pre>
<p>High R2 score for the surrogate model hints that the attributions are sensible. In case the R2 score were very low, it could mean that the surrogate linear model can't tell which regions are more important for the explainable model preditions.</p>
<p>Let's analyze the attribution maps for class <code>0</code> representing phosphorus estimation. We can simply modify the <code>target</code> parameter, utilizing the same segmentation mask, and rerun the explanation process.</p>
<pre><code class="language-python">spatial_attributes = lime.get_spatial_attributes(hsi_0, segmentation_mask_slic, target=0, n_samples=10)
</code></pre>
<pre><code class="language-python">mt.visualize.visualize_spatial_attributes(spatial_attributes)
print(f&quot;R2 metric: {spatial_attributes.score:.4f}&quot;)
plt.show()
</code></pre>
<pre><code>R2 metric: 0.6901
</code></pre>
<p><img alt="png" src="../lime_files/lime_33_1.png" /></p>
<p>In our case, the green regions may correspond to areas with a higher concentration of the parameter being tested - here phosporus. </p>
<h4 id="different-segmentation-masks">Different segmentation masks</h4>
<p>The most semantically meaningful segmentation mask is the one created by slic method. It contains superpixels that are created based on the image structure, which choice is very similiar to the regions that a human would choose</p>
<p>However, <code>meteors</code> also supports creating the patch segmentation mask, but we are planning to increase support for different methods very soon.</p>
<h5 id="patch-segmentation">Patch segmentation</h5>
<p>Patch segmentation mask tests the importance of regions shaped as rectangles. It is designed to work well with the ViT architecure, but it gives less semantic meaning.</p>
<p>Let's check the <code>patch</code> segmentation mask and see how it affects the attribution maps.</p>
<pre><code class="language-python">spatial_attributes = lime.get_spatial_attributes(hsi_0, segmentation_mask_patch, target=1, n_samples=10)
</code></pre>
<pre><code class="language-python">fig, ax = mt.visualize.visualize_spatial_attributes(spatial_attributes)
print(f&quot;R2 metric: {spatial_attributes.score:.4f}&quot;)
fig.suptitle(&quot;Patch Mask Segmentation for Potassium&quot;)
plt.show()
</code></pre>
<pre><code>R2 metric: 0.8699
</code></pre>
<p><img alt="png" src="../lime_files/lime_36_1.png" /></p>
<p>As we can see the Lime explainer focused on different regions of the image, which are not consistent with the previous results. Let's try using a larger number of perturbed samples. In this way, the linear model will be trained on much more perturbed versions of the original image and will be able to mimic the predictions of the explained model better.</p>
<pre><code class="language-python">spatial_attributes = lime.get_spatial_attributes(
    hsi_0, segmentation_mask_patch, target=1, n_samples=100, perturbations_per_eval=10
)
</code></pre>
<pre><code class="language-python">fig, ax = mt.visualize.visualize_spatial_attributes(spatial_attributes)
print(f&quot;R2 metric: {spatial_attributes.score:.4f}&quot;)
fig.suptitle(&quot;LIME Explanation for Potassium with increased number of perturbed samples&quot;)
plt.show()
</code></pre>
<pre><code>R2 metric: 0.5538
</code></pre>
<p><img alt="png" src="../lime_files/lime_39_1.png" /></p>
<p>The explainer focuses again on the part of the image in the lower right corner and marks it as positively correlated with the output. It may suggests that the model indeed finds something interesting in this important region.</p>
<h4 id="custom-segmentation-mask">Custom segmentation mask</h4>
<p>Additionally, an user might want to create their own segmentation mask, or modify the one created by the package.</p>
<p>Therefore, we can inspect this lower right region more thoroughly by creating a more specific segmentation mask based on the slic one.</p>
<pre><code class="language-python">thorough_segmentation_mask_slic = segmentation_mask_slic.clone()
thorough_segmentation_mask_slic[(thorough_segmentation_mask_slic != 10) &amp; (thorough_segmentation_mask_slic != 0)] = 1

spatial_attributes = lime.get_spatial_attributes(
    hsi_0, thorough_segmentation_mask_slic, target=1, n_samples=100, perturbations_per_eval=10
)
</code></pre>
<pre><code class="language-python">mt.visualize.visualize_spatial_attributes(spatial_attributes)
print(f&quot;R2 metric: {spatial_attributes.score:.4f}&quot;)
</code></pre>
<pre><code>R2 metric: 0.9755
</code></pre>
<p><img alt="png" src="../lime_files/lime_43_1.png" /></p>
<p>it is visible, that this superpixel covers area that seems to be the most important region for the model. Why? This is another problem to be explained.</p>
<h5 id="mask-importance">Mask importance</h5>
<p>Using an another custom segmentation mask, we can inspect, if binary mask covers regions that should not be relevant for the model. Now we will use the binary mask used for covering irrelevant regions as a segmentation mask to verify our hypothesis.</p>
<p>Firstly, let's create a another HSI object, this time a plain image without any covering binary mask</p>
<pre><code class="language-python">image_without_mask = not_masked_image_torch.clone()
hsi_without_mask = mt.HSI(
    image=image_without_mask,  # The preprocessed image tensor, but without the mask
    wavelengths=wavelengths,
    orientation=&quot;CWH&quot;,
    device=device,
)
</code></pre>
<p>Now, this image with the segmentation mask that consists of only two classes, we may explore, how each region is important for the model. To do so, we repeat this process for the potassium class.</p>
<pre><code class="language-python">segmentation_mask_from_binary = binary_mask + 1
spatial_attributes = lime.get_spatial_attributes(
    hsi_without_mask, segmentation_mask_from_binary, target=1, n_samples=10
)
</code></pre>
<pre><code>C:\Users\tymot\Documents\praca\pineapple\meteors\src\meteors\attr\lime_base.py:738: UserWarning: Minimum element in feature mask is not 0, shifting indices to start at 0.
  warnings.warn("Minimum element in feature mask is not 0, shifting indices to" " start at 0.")
</code></pre>
<pre><code class="language-python">fig, ax = mt.visualize.visualize_spatial_attributes(spatial_attributes)
fig.suptitle(&quot;Importance of regions covered by the mask for Potassium class&quot;)
print(f&quot;R2 metric: {spatial_attributes.score:.4f}&quot;)
</code></pre>
<pre><code>R2 metric: 0.4493
</code></pre>
<p><img alt="png" src="../lime_files/lime_49_1.png" /></p>
<p>as we can see, the model correctly has learned that the relevant information is not covered with the mask. The region that was covered by default using the mask is strongly negatively correlated with the output, which may suggest that indeed, mask covers some regions, causing the model to output lower values for the estimated soil parameter</p>
<h3 id="42-spectral-analysis">4.2. Spectral Analysis</h3>
<p>The spectral analysis is similar to the spatial one, but instead of analyzing the spatial dimension of the hyperspectral images, we analyze the spectral dimension - the image bands. In the process we group the specific channels of the image into superbands (groups of bands) and investigate importances of such groups. The spectral or band mask is a similar torch tensor or numpy ndarray as the segmentation mask, but instead of grouping regions it groups image channels. In the similar manner it is repeated along the width and height dimensions of the image.</p>
<p>Since this kind of spectral analysis has sense only in hyperspectral imaginery, we paid special attention to this novel feature. As in the case of segmentation mask, user has several options how to create the band mask, to ensure that they had no difficulty using the analysis package and could rather focus on explaining the model. In the current package version user can:
- provide the spectral indices or indexes of the commmonly recognized bands
- specify exactly which wavelengths should compose the band mask
- specify which wavelength indices corresponding to the wavelength list from the explained HSI object should be used</p>
<p>All these band masks can be obtained using one simple method <code>get_band_mask</code>, which detailed documentation may also be found in the <a href="https://xai4space.github.io/meteors/latest/reference/#src.meteors.attr.Lime.get_band_mask">reference</a>. Now we will go through these different methods of creating the band mask and create some attributions.</p>
<h4 id="band-and-indices-names">Band and indices names</h4>
<p>This, definetely the fastest for the user, method provides a quick way to explore importance of some well known superbands. To create the band mask using this approach, all we need to do is to pass a list or a dictionary of the band names:</p>
<pre><code class="language-python">band_mask, band_names = lime.get_band_mask(hsi_0, [&quot;R&quot;, &quot;G&quot;, &quot;B&quot;])
</code></pre>
<pre><code>[32m2024-09-24 02:43:21.378[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments G and B are overlapping on wavelength 510.0299987792969[0m
[32m2024-09-24 02:43:21.381[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments G and B are overlapping on wavelength 513.219970703125[0m
[32m2024-09-24 02:43:21.382[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments G and B are overlapping on wavelength 516.4199829101562[0m
[32m2024-09-24 02:43:21.383[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments G and B are overlapping on wavelength 519.6099853515625[0m
[32m2024-09-24 02:43:21.384[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments G and B are overlapping on wavelength 522.8099975585938[0m
[32m2024-09-24 02:43:21.384[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments G and B are overlapping on wavelength 526.010009765625[0m
[32m2024-09-24 02:43:21.385[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments G and B are overlapping on wavelength 529.2000122070312[0m
</code></pre>
<p>this method outpus a tuple of two variables:
- <code>band_mask</code> - a created band mask
- <code>band_names</code> - a dictionary containing mapping from provided labels and segment indices</p>
<p>In this case we created a band mask that contains 4 superpixels - one for each of the base colours and another one including all the background.</p>
<pre><code class="language-python">band_names
</code></pre>
<pre><code>{'R': 1, 'G': 2, 'B': 3}
</code></pre>
<pre><code class="language-python">plt.scatter(wavelengths, band_mask.squeeze(1).squeeze(1))

plt.title(&quot;Band Mask for the RGB bands&quot;)

plt.show()
</code></pre>
<p><img alt="png" src="../lime_files/lime_56_0.png" /></p>
<p>The plot presents how bands are grouped. The bands with the value 0 creates the additional band group <code>not_included</code> which also will be used in the analysis.</p>
<p>Now, we can analyze the hyperspectral image based on the spectral dimension. We will use the same LIME model as in the spatial analysis (initialize with the same parameters), but we will provide the band mask instead of the segmentation mask and also band names.</p>
<pre><code class="language-python">lime = mt.attr.Lime(
    explainable_model=mt.utils.models.ExplainableModel(model, &quot;regression&quot;),
    interpretable_model=mt.utils.models.SkLearnLasso(alpha=0.001),
)
</code></pre>
<pre><code class="language-python">spectral_attributes = lime.get_spectral_attributes(
    hsi_0,  # HSI data
    band_mask=band_mask,  # Band mask
    target=1,  # class analyzed - K - potassium
    band_names=band_names,  # Band names
    n_samples=10,  # Number of perturbation, the more the better explainer is trained but the analsys is slower
)
</code></pre>
<pre><code>C:\Users\tymot\Documents\praca\pineapple\meteors\src\meteors\attr\attributes.py:148: UserWarning: Adding 'not_included' to band names because 0 ids is present in the mask and not in band names
  warnings.warn(
</code></pre>
<p>The spectral attributions are similar to spatial attributes consisting of <code>hsi</code>, <code>mask</code>, <code>attributes</code> and <code>score</code> of the linear model.</p>
<pre><code class="language-python">assert len(wavelengths) == spectral_attributes.flattened_attributes.shape[0]
plt.scatter(wavelengths, spectral_attributes.flattened_attributes)
plt.title(&quot;Spectral Attributes Map&quot;)
plt.show()
</code></pre>
<p><img alt="png" src="../lime_files/lime_61_0.png" /></p>
<p>But again as with spatial analysis it is much easier to visualize the results using the provided meteors visualization functions.</p>
<pre><code class="language-python">fig, ax = mt.visualize.visualize_spectral_attributes(spectral_attributes, show_not_included=True)
fig.suptitle(&quot;Spectral Attributes for Potassium class and RGB bands&quot;)
plt.show()
</code></pre>
<p><img alt="png" src="../lime_files/lime_63_0.png" /></p>
<p>The plot this time consists of two parts. On the left, we have the attribution value per band for the hyperspectral image it helps to identify, which particular bands are important, whilist the right plot helps to identify the most important superbands and compare magnitudes of its importance. </p>
<p>On the plot above, we may see that the red superband is much more important than the green and blue ones. How would the situation change if we compared red superband and blue and green superband together?</p>
<p>Fortunately, thanks to the method <code>get_band_mask</code> it is incredibely easy - we just need to specify bands that will produce the superband.</p>
<pre><code class="language-python">band_mask, band_names = lime.get_band_mask(hsi_0, [&quot;R&quot;, [&quot;G&quot;, &quot;B&quot;]])
</code></pre>
<p>and as before we can create the attributions for the selected superbands using LIME explainer.</p>
<pre><code class="language-python">spectral_attributes = lime.get_spectral_attributes(
    hsi_0,  # HSI data
    band_mask=band_mask,  # Band mask
    target=1,  # class analyzed - K - potassium
    band_names=band_names,  # Band names
    n_samples=10,  # Number of perturbation, the more the better explainer is trained but the analsys is slower
)
</code></pre>
<pre><code>C:\Users\tymot\Documents\praca\pineapple\meteors\src\meteors\attr\attributes.py:148: UserWarning: Adding 'not_included' to band names because 0 ids is present in the mask and not in band names
  warnings.warn(
</code></pre>
<pre><code class="language-python">fig, ax = mt.visualize.visualize_spectral_attributes(spectral_attributes, show_not_included=True)
fig.suptitle(&quot;Spectral Attributes for Potassium class and R and GB superbands&quot;)
plt.show()
</code></pre>
<p><img alt="png" src="../lime_files/lime_69_0.png" /></p>
<p>it looks that, indeed, green and blue superbands combined are more important than the red one.</p>
<p>To validate the model, we again can use the <code>score</code> attribute, which indicates the <code>R2</code> metric of how well-trained the surrogate model was.</p>
<pre><code class="language-python">spectral_attributes.score
</code></pre>
<pre><code>0.6528714895248413
</code></pre>
<p>All the band names are sourced from the <a href="https://github.com/awesome-spectral-indices/awesome-spectral-indices?tab=readme-ov-file#expressions">Awesome Spectral Indices</a> repository and handled using the <code>spyndex</code> library. Therefore, we can explore all the bands or try out some more exotic combinations using the predefined band indices <a href="https://github.com/awesome-spectral-indices/awesome-spectral-indices?tab=readme-ov-file#spectral-indices-by-application-domain">here</a> </p>
<p>We will use now one of the indices taken from the library, a Bare Soil Index, which is a combination of couple of base bands. It can be defined as
$$
BI = \frac{(S1 + R) - (N + B)}{(S1 + R) + (N + B)}
$$</p>
<p>where S1 corresponds to SWIR 1 band, R and B to red and blue respectively and N to NIR band. It is used, as the name suggests, to detect bare soil in the hyperspectral imaginery and possibly can be used as well to detect soil parameters.</p>
<pre><code class="language-python">band_mask, band_names = lime.get_band_mask(hsi_0, [[&quot;G&quot;, &quot;B&quot;], &quot;R&quot;, &quot;BI&quot;])
</code></pre>
<pre><code>[32m2024-09-24 02:43:35.036[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_get_band_wavelengths_indices_from_band_names[0m:[36m634[0m - [33m[1mBand S1 is not present in the given wavelengths. Band ranges from 1550 nm to 1750 nm and the HSI wavelengths range from 462.08 nm to 938.37 nm. The given band will be skipped[0m
[32m2024-09-24 02:43:35.038[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments ('G', 'B') and BI are overlapping on wavelength 462.0799865722656[0m
[32m2024-09-24 02:43:35.039[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments ('G', 'B') and BI are overlapping on wavelength 465.2699890136719[0m
[32m2024-09-24 02:43:35.040[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments ('G', 'B') and BI are overlapping on wavelength 468.4700012207031[0m
[32m2024-09-24 02:43:35.040[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments ('G', 'B') and BI are overlapping on wavelength 471.6700134277344[0m
[32m2024-09-24 02:43:35.041[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments ('G', 'B') and BI are overlapping on wavelength 474.8599853515625[0m
[32m2024-09-24 02:43:35.042[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments ('G', 'B') and BI are overlapping on wavelength 478.05999755859375[0m
[32m2024-09-24 02:43:35.043[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments ('G', 'B') and BI are overlapping on wavelength 481.260009765625[0m
[32m2024-09-24 02:43:35.043[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments ('G', 'B') and BI are overlapping on wavelength 484.45001220703125[0m
[32m2024-09-24 02:43:35.044[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments ('G', 'B') and BI are overlapping on wavelength 487.6499938964844[0m
[32m2024-09-24 02:43:35.045[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments ('G', 'B') and BI are overlapping on wavelength 490.8500061035156[0m
[32m2024-09-24 02:43:35.046[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments ('G', 'B') and BI are overlapping on wavelength 494.0400085449219[0m
[32m2024-09-24 02:43:35.047[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments ('G', 'B') and BI are overlapping on wavelength 497.239990234375[0m
[32m2024-09-24 02:43:35.047[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments ('G', 'B') and BI are overlapping on wavelength 500.42999267578125[0m
[32m2024-09-24 02:43:35.048[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments ('G', 'B') and BI are overlapping on wavelength 503.6300048828125[0m
[32m2024-09-24 02:43:35.048[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments ('G', 'B') and BI are overlapping on wavelength 506.8299865722656[0m
[32m2024-09-24 02:43:35.049[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments ('G', 'B') and BI are overlapping on wavelength 510.0299987792969[0m
[32m2024-09-24 02:43:35.050[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments ('G', 'B') and BI are overlapping on wavelength 513.219970703125[0m
[32m2024-09-24 02:43:35.050[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments ('G', 'B') and BI are overlapping on wavelength 516.4199829101562[0m
[32m2024-09-24 02:43:35.051[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments ('G', 'B') and BI are overlapping on wavelength 519.6099853515625[0m
[32m2024-09-24 02:43:35.051[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments ('G', 'B') and BI are overlapping on wavelength 522.8099975585938[0m
[32m2024-09-24 02:43:35.052[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments ('G', 'B') and BI are overlapping on wavelength 526.010009765625[0m
[32m2024-09-24 02:43:35.053[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments ('G', 'B') and BI are overlapping on wavelength 529.2000122070312[0m
[32m2024-09-24 02:43:35.054[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments R and BI are overlapping on wavelength 621.9099731445312[0m
[32m2024-09-24 02:43:35.054[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments R and BI are overlapping on wavelength 625.0999755859375[0m
[32m2024-09-24 02:43:35.055[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments R and BI are overlapping on wavelength 628.2999877929688[0m
[32m2024-09-24 02:43:35.056[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments R and BI are overlapping on wavelength 631.5[0m
[32m2024-09-24 02:43:35.057[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments R and BI are overlapping on wavelength 634.6900024414062[0m
[32m2024-09-24 02:43:35.057[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments R and BI are overlapping on wavelength 637.8900146484375[0m
[32m2024-09-24 02:43:35.058[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments R and BI are overlapping on wavelength 641.0900268554688[0m
[32m2024-09-24 02:43:35.058[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments R and BI are overlapping on wavelength 644.280029296875[0m
[32m2024-09-24 02:43:35.059[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments R and BI are overlapping on wavelength 647.47998046875[0m
[32m2024-09-24 02:43:35.060[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments R and BI are overlapping on wavelength 650.6699829101562[0m
[32m2024-09-24 02:43:35.061[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments R and BI are overlapping on wavelength 653.8699951171875[0m
[32m2024-09-24 02:43:35.062[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments R and BI are overlapping on wavelength 657.0700073242188[0m
[32m2024-09-24 02:43:35.062[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments R and BI are overlapping on wavelength 660.27001953125[0m
[32m2024-09-24 02:43:35.063[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments R and BI are overlapping on wavelength 663.4600219726562[0m
[32m2024-09-24 02:43:35.064[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments R and BI are overlapping on wavelength 666.6599731445312[0m
[32m2024-09-24 02:43:35.065[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments R and BI are overlapping on wavelength 669.8499755859375[0m
[32m2024-09-24 02:43:35.065[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments R and BI are overlapping on wavelength 673.0499877929688[0m
[32m2024-09-24 02:43:35.066[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments R and BI are overlapping on wavelength 676.25[0m
[32m2024-09-24 02:43:35.067[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments R and BI are overlapping on wavelength 679.4500122070312[0m
[32m2024-09-24 02:43:35.068[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments R and BI are overlapping on wavelength 682.6400146484375[0m
[32m2024-09-24 02:43:35.068[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments R and BI are overlapping on wavelength 685.8400268554688[0m
[32m2024-09-24 02:43:35.069[0m | [33m[1mWARNING [0m | [36mmeteors.attr.lime[0m:[36m_check_overlapping_segments[0m:[36m800[0m - [33m[1mSegments R and BI are overlapping on wavelength 689.030029296875[0m
</code></pre>
<p>now, using the same methods as before, we can attribute the new superbands using the LIME explainer and visualize the output</p>
<pre><code class="language-python">band_names
</code></pre>
<pre><code>{('G', 'B'): 1, 'R': 2, 'BI': 3}
</code></pre>
<pre><code class="language-python">spectral_attributes = lime.get_spectral_attributes(
    hsi_0,  # HSI data
    band_mask=band_mask,  # Band mask
    target=1,  # class analyzed - K - potassium
    band_names=band_names,  # Band names
    n_samples=10,  # Number of perturbation, the more the better explainer is trained but the analsys is slower
)
</code></pre>
<pre><code>C:\Users\tymot\Documents\praca\pineapple\meteors\src\meteors\attr\attributes.py:148: UserWarning: Adding 'not_included' to band names because 0 ids is present in the mask and not in band names
  warnings.warn(
</code></pre>
<pre><code class="language-python">mt.visualize.visualize_spectral_attributes(spectral_attributes, show_not_included=True)
print(f&quot;R2 metric: {spectral_attributes.score:.4f}&quot;)
plt.show()
</code></pre>
<pre><code>R2 metric: 0.7993
</code></pre>
<p><img alt="png" src="../lime_files/lime_78_1.png" /></p>
<p>it seems that the superband BI is actually irrelevant for the model in this specific case. It looks that the model does not base its predictions for the current image on this specific bands</p>
<p>In this way, we may investigate if the model uses the bands that were commonly used for the similar tasks in the literature, which could help us debbuging the model! 
Now we will use some bands, that should really be important to the model.</p>
<h4 id="wavelengths-ranges">Wavelengths ranges</h4>
<p>In some cases, we do not want to use any well known band combinations. In our team, we had access to knowledge of domain experts who gave us the exact wavelength values that are used to detect potassium, phosphorus, magnessium and pH in the soil. Now we can utilize this knowledge and create our own superbands. </p>
<p>Now we will try out the values for the potassium. Unfortunately, not all the wavelengths provided are exactly mentioned in our wavelengths list, thus we need to find the closest corresponding indices to the values given by the experts.</p>
<pre><code class="language-python">potassium_superband_indices = [0, 1, 4, 10, 43, 46, 47]  # supposedly important bands for potassium
# or differently
potassium_superband_wavelengths = [wavelengths[i] for i in potassium_superband_indices]
</code></pre>
<pre><code class="language-python">potassium_superband_wavelengths
</code></pre>
<pre><code>[462.08, 465.27, 474.86, 494.04, 599.53, 609.12, 612.32]
</code></pre>
<pre><code class="language-python">band_dict = {&quot;potassium&quot;: potassium_superband_indices, &quot;another_superpixel&quot;: [i for i in range(20, 30)]}
band_dict
</code></pre>
<pre><code>{'potassium': [0, 1, 4, 10, 43, 46, 47],
 'another_superpixel': [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]}
</code></pre>
<pre><code class="language-python">band_mask, band_names = lime.get_band_mask(hsi_0, band_indices=band_dict)
</code></pre>
<pre><code class="language-python">band_names  # the mapping from names to segment labels
</code></pre>
<pre><code>{'potassium': 1, 'another_superpixel': 2}
</code></pre>
<pre><code class="language-python">spectral_attributes = lime.get_spectral_attributes(
    hsi_0,
    band_mask=band_mask,
    target=1,
    band_names=band_names,
    n_samples=100,
)
</code></pre>
<pre><code>C:\Users\tymot\Documents\praca\pineapple\meteors\src\meteors\attr\attributes.py:148: UserWarning: Adding 'not_included' to band names because 0 ids is present in the mask and not in band names
  warnings.warn(
</code></pre>
<pre><code class="language-python">mt.visualize.visualize_spectral_attributes(spectral_attributes, show_not_included=True)
print(f&quot;R2 metric: {spectral_attributes.score:.4f}&quot;)
plt.show()
</code></pre>
<pre><code>R2 metric: 0.4769
</code></pre>
<p><img alt="png" src="../lime_files/lime_87_1.png" /></p>
<p>As it turns out, in this case, the bands given us by the experts are not necessarily important for the model. We need to remember, that this analysis is performed for solely one image. Perhaps, this is just one outlier and in different cases model might actually use the specified bands. For such cases, we can utilize the <em>global explanations</em> which are attributions aggregeated for multiple input images. </p>
<h3 id="global-attributions">Global attributions</h3>
<p>An interesting capability unique to spectral analysis is the ability to aggregate results across multiple samples, allowing us to transition from local interpretation to global interpretation. This is usually not possible for spatial analysis, as the images differ significantly when it comes to the covered land. Aggregating spatial information is challenging due to the lack of straightforward method for determining which parts of different images are similar, as the covered land can vary significantly. On the contrary, spectral analysis benefits from consistent bands accross images, allowing for specification of common superbands.</p>
<p>To give an idea how to perform such analysis, we need a second sample of the hyperspectral image.</p>
<pre><code class="language-python">data, mask = load_single_npz_image(&quot;data/1.npz&quot;)
masked_data = data * mask
masked_data = torch.from_numpy(masked_data.astype(np.float32)).permute(2, 0, 1)
eval_tr = get_eval_transform(224)

image_torch_1 = eval_tr(masked_data)
not_masked_image_torch_1 = eval_tr(torch.from_numpy(data.astype(np.float32)).permute(2, 0, 1))
</code></pre>
<pre><code class="language-python">binary_mask_1 = (image_torch_1 &gt; 0.0).int()

hsi_1 = mt.HSI(
    image=not_masked_image_torch_1, wavelengths=wavelengths, orientation=&quot;CWH&quot;, binary_mask=binary_mask_1, device=device
)

ax = mt.visualize.visualize_hsi(hsi_1, use_mask=True)
ax.set_title(&quot;Another sample image from the HYPERVIEW dataset&quot;)
</code></pre>
<pre><code>Text(0.5, 1.0, 'Another sample image from the HYPERVIEW dataset')
</code></pre>
<p><img alt="png" src="../lime_files/lime_90_1.png" /></p>
<p>Now, once the image is properly loaded and preprocessed, let's get the attributions for the second sample, using the same band mask as before</p>
<pre><code class="language-python">spectral_attributes_1 = lime.get_spectral_attributes(
    hsi_1, band_mask=band_mask, target=1, band_names=band_names, n_samples=100
)
</code></pre>
<pre><code>C:\Users\tymot\Documents\praca\pineapple\meteors\src\meteors\attr\attributes.py:148: UserWarning: Adding 'not_included' to band names because 0 ids is present in the mask and not in band names
  warnings.warn(
</code></pre>
<pre><code class="language-python">mt.visualize.visualize_spectral_attributes(spectral_attributes, show_not_included=True)
print(f&quot;R2 metric: {spectral_attributes.score:.4f}&quot;)
plt.show()
</code></pre>
<pre><code>R2 metric: 0.4769
</code></pre>
<p><img alt="png" src="../lime_files/lime_93_1.png" /></p>
<p>To get the global interpretation we will provide the list of attributions to the meteors visualizer to create the global interpretation visualization.</p>
<pre><code class="language-python">mt.visualize.visualize_spectral_attributes([spectral_attributes, spectral_attributes_1], show_not_included=True)
plt.tight_layout()
plt.show()
</code></pre>
<p><img alt="png" src="../lime_files/lime_95_0.png" /></p>
<p>As it turns out, the model does not necessarily use the specified bands in the prediction of the potassium class. This is probably insufficient to conclude anything using only attributions from 2 images, especially because the <code>score</code> of the explanations was low, but our model suprisingly does not use the expected wavelengths.</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": [], "search": "../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"provider": "mike"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.56dfad97.min.js"></script>
      
    
  </body>
</html>