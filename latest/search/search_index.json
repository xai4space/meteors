{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\u2604\ufe0f\ud83d\udef0\ufe0f Meteors","text":""},{"location":"#introduction","title":"\ud83d\udef0\ufe0f Introduction","text":"<p>Meteors is an open-source package for creating explanations of hyperspectral and multispectral images. Developed primarily for Pytorch models, Meteors was inspired by the Captum library. Our goal is to provide not only the ability to create explanations for hyperspectral images but also to visualize them in a user-friendly way.</p> <p>Please note that this package is still in the development phase, and we welcome any feedback and suggestions to help improve the library.</p> <p>Meteors emerged from a research grant project between the Warsaw University of Technology research group MI2.ai and Kp Labs, financially supported by the European Space Agency (ESA).</p>"},{"location":"#target-audience","title":"\ud83c\udfaf Target Audience","text":"<p>Meteors is designed for:</p> <ul> <li>Researchers, data scientists, and developers who work with hyperspectral and multispectral images and want to understand the decisions made by their models.</li> <li>Engineers who build models for production and want to troubleshoot through improved model interpretability.</li> <li>Developers seeking to deliver better explanations to end users on why they're seeing specific content.</li> </ul>"},{"location":"#installation","title":"\ud83d\udce6 Installation","text":"<p>Requirements</p> <ul> <li>Python &gt;= 3.9</li> <li>PyTorch &gt;= 1.10</li> <li>Captum &gt;= 0.7.0</li> </ul> <p>Install with <code>pip</code>:</p> <pre><code>pip install meteors\n</code></pre> <p>With conda: Coming soon</p>"},{"location":"#documentation","title":"\ud83d\udcda Documentation","text":"<p>Please refer to the documentation for more information on how to use Meteors.</p>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>As an open-source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation.</p> <p>We use rye as our project and package management tool. To start developing, follow these steps:</p> <pre><code>curl -sSf https://rye.astral.sh/get | bash # Install Rye\nrye pin &lt;python version &gt;=3.9&gt; # Pin the python version\nrye sync # Sync the environment\n</code></pre> <p>Before pushing your changes, please run the tests and the linter:</p> <pre><code>rye test\nrye run pre-commit run --all-files\n</code></pre> <p>For more information on how to contribute, please refer to our Contributing Guide.</p> <p>Thank you for considering contributing to Meteors!</p>"},{"location":"#contributors","title":"\ud83d\udcab Contributors","text":""},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v004-2024-09-24","title":"v0.0.4 (2024-09-24)","text":"<ul> <li>feat: HyperNoiseTunnel and captum attribution methods (#51)</li> </ul>"},{"location":"changelog/#v003-2024-09-23","title":"v0.0.3 (2024-09-23)","text":""},{"location":"changelog/#bug-fixes","title":"\ud83e\ude7a Bug Fixes","text":"<ul> <li>github action release workflow to pypi (#83)</li> </ul>"},{"location":"changelog/#meteors-002-2024-08-11","title":"meteors 0.0.2 (2024-08-11)","text":"<ul> <li>No release</li> <li>Refined package structure - simple modules for models and visualisation, installation using toml file</li> <li>Spectral attributions using LIME</li> <li>CUDA compatibility of LIME</li> </ul>"},{"location":"changelog/#meteors-001-2024-06-02","title":"meteors 0.0.1 (2024-06-02)","text":"<ul> <li>No release</li> <li>Prepared a simple draft of package along with some ideas and sample files for implementation of LIME for hyperspectral images.</li> <li>Segmentation mask for LIME using slic</li> <li>Spatial attributions using LIME</li> </ul>"},{"location":"how-to-guides/","title":"\ud83e\udd1d How to Contribute","text":"<p>Thank you for your interest in contributing to Meteors! We welcome contributions from the community to help improve and expand the library. This guide will walk you through the process of contributing to the project.</p>"},{"location":"how-to-guides/#types-of-contributions","title":"\ud83d\udccb Types of Contributions","text":"<p>There are several ways you can contribute to Meteors:</p> <ul> <li>\ud83d\udc1b Reporting bugs</li> <li>\ud83d\udca1 Suggesting new features or enhancements</li> <li>\ud83d\udcd6 Improving documentation</li> <li>\ud83d\udcbb Writing code (fixing bugs, implementing new features)</li> <li>\ud83e\uddea Adding or improving test cases</li> <li>\ud83d\udce3 Spreading the word about Meteors</li> </ul>"},{"location":"how-to-guides/#getting-started","title":"\ud83c\udf3f Getting Started","text":"<p>To start contributing to Meteors, follow these steps:</p> <ol> <li>\ud83c\udf74 Fork the Meteors repository on GitHub.</li> <li>\ud83d\udce5 Clone your forked repository to your local machine.</li> <li>\ud83d\udd00 Create a new branch for your contribution.</li> <li>\ud83d\udce6 Set rye <code>rye pin &lt;python_version&gt; &amp;&amp; rye sync</code>.</li> <li>\ud83d\udee0\ufe0f Make your changes or additions.</li> <li>\u2705 Test your changes to ensure they work as expected <code>rye test</code>.</li> <li>\ud83e\uddf9 Lint your code <code>rye run pre-commit run --all-files</code>.</li> <li>\ud83d\udcdd Commit your changes with a clear and descriptive commit message.</li> <li>\ud83d\udce4 Push your changes to your forked repository.</li> <li>\ud83d\udd0c Submit a pull request to the main Meteors repository.</li> <li>\ud83d\udcdd Fill out the pull request template with the necessary information.</li> </ol>"},{"location":"how-to-guides/#code-guidelines","title":"\ud83d\udcd0 Code Guidelines","text":"<p>When contributing code to Meteors, please follow these guidelines:</p> <ul> <li>\ud83d\udcda Document your code using docstrings and comments.</li> <li>\u2705 Write unit tests for new features or bug fixes.</li> <li>\ud83e\uddf9 Ensure your code is clean, readable, and well-structured.</li> <li>\ud83d\udea8 Run the existing tests and make sure they pass before submitting a pull request.</li> <li>\ud83d\udc0d Run linter to clean the code.</li> </ul>"},{"location":"how-to-guides/#code-of-conduct","title":"\ud83d\udcdc Code of Conduct","text":"<p>Please note that by contributing to Meteors, you are expected to adhere to our Code of Conduct. Be respectful, inclusive, and considerate in your interactions with others.</p>"},{"location":"how-to-guides/#recognition","title":"\ud83d\ude4c Recognition","text":"<p>We appreciate all contributions to Meteors, and we make sure to recognize our contributors. Your name will be added to the list of contributors in the project's README file.</p> <p>If you have any questions or need further assistance, feel free to reach out to the maintainers or open an issue on the GitHub repository.</p> <p>Thank you for your contribution to Meteors! \ud83c\udf89\ud83d\ude80</p>"},{"location":"quickstart/","title":"\ud83d\ude80 Quickstart","text":"<p>Welcome to the Quickstart Guide for Meteors! This guide will walk you through the basic steps to get started with using Meteors for explaining your hyperspectral and multispectral image models.</p>"},{"location":"quickstart/#prerequisites","title":"\ud83d\udccb Prerequisites","text":"<p>Before you begin, make sure you have the following installed:</p> <ul> <li>Python &gt;= 3.9</li> <li>PyTorch &gt;= 1.10</li> <li>Captum &gt;= 0.7.0</li> </ul>"},{"location":"quickstart/#installation","title":"\ud83d\udce5 Installation","text":"<p>To install Meteors, simply run the following command:</p> <pre><code>pip install meteors\n</code></pre> <p>For conda users, we will provide a conda installation method in the near future. We promise!\ud83e\udd1e</p>"},{"location":"quickstart/#basic-hyperspectral-or-multispectral-data-object","title":"\ud83c\udf1f Basic Hyperspectral or Multispectral Data Object","text":"<p>Meteors provide an easy-to-use object for handling hyperspectral and multispectral images. The <code>HSI</code> object is a simple way to process and manipulate your data.</p> <pre><code>from meteors import HSI\n</code></pre> <p>Remember, when providing data to the model, make sure it is in the final format that the model expects, without the batch dimension. The <code>HSI</code> object will handle the rest. We also recommend providing the image data channel orientation, height, width, and the number of channels in the format <code>(CHW)</code>. For example:</p> <ul> <li>Number of channels: C</li> <li>Height: H</li> <li>Width: W</li> </ul>"},{"location":"quickstart/#explanation-methods","title":"\ud83d\udd0d Explanation Methods","text":"<p>Meteors provides several explanation methods for hyperspectral and multispectral images, including:</p> <ul> <li>LIME: Local Interpretable Model-agnostic Explanations</li> <li>More methods coming soon!</li> </ul> <p>To use a specific explanation method in tutorials we provide for each method, example code.</p>"},{"location":"quickstart/#visualization-options","title":"\ud83c\udfa8 Visualization Options","text":"<p>Meteors offers various visualization options to help you understand and interpret the explanations in package <code>meteors.visualize</code>.</p> <pre><code>from meteors.visualize import visualize_spectral_attributes, visualize_spatial_attributes\n</code></pre>"},{"location":"quickstart/#tutorials","title":"\ud83d\udcda Tutorials","text":"<p>We have several tutorials to help get you off the ground with Meteors. The tutorials are Jupyter notebooks and cover the basics along with demonstrating usage of Meteors .</p> <p>View the tutorials page here.</p>"},{"location":"quickstart/#api-reference","title":"\ud83d\udcd6 API Reference","text":"<p>For an in-depth reference of the various Meteors internals, see our API Reference.</p>"},{"location":"quickstart/#contributing","title":"\ud83d\ude4c Contributing","text":"<p>We welcome contributions to Meteors! Please refer to our Contributing Guide for more information on how to get involved.</p>"},{"location":"reference/","title":"API Reference","text":"<p>The architecture of the package can be seen on the UML diagram: </p>"},{"location":"reference/#hyperspectral-image","title":"HyperSpectral Image","text":""},{"location":"reference/#src.meteors.hsi.HSI","title":"<code>HSI</code>","text":"<p>A dataclass for hyperspectral image data, including the image, wavelengths, and binary mask.</p> <p>Attributes:</p> Name Type Description <code>image</code> <code>Tensor</code> <p>The hyperspectral image data as a PyTorch tensor.</p> <code>wavelengths</code> <code>Tensor</code> <p>The wavelengths present in the image.</p> <code>orientation</code> <code>tuple[str, str, str]</code> <p>The orientation of the image data.</p> <code>device</code> <code>device</code> <p>The device to be used for inference.</p> <code>binary_mask</code> <code>Tensor</code> <p>A binary mask used to cover unimportant parts of the image.</p> Source code in <code>src/meteors/hsi.py</code> <pre><code>class HSI(BaseModel):\n    \"\"\"A dataclass for hyperspectral image data, including the image, wavelengths, and binary mask.\n\n    Attributes:\n        image (torch.Tensor): The hyperspectral image data as a PyTorch tensor.\n        wavelengths (torch.Tensor): The wavelengths present in the image.\n        orientation (tuple[str, str, str]): The orientation of the image data.\n        device (torch.device): The device to be used for inference.\n        binary_mask (torch.Tensor): A binary mask used to cover unimportant parts of the image.\n    \"\"\"\n\n    image: Annotated[  # Should always be a first field\n        torch.Tensor,\n        PlainValidator(ensure_image_tensor),\n        Field(description=\"Hyperspectral image. Converted to torch tensor.\"),\n    ]\n    wavelengths: Annotated[\n        torch.Tensor,\n        PlainValidator(ensure_wavelengths_tensor),\n        Field(description=\"Wavelengths present in the image. Defaults to None.\"),\n    ]\n    orientation: Annotated[\n        tuple[str, str, str],\n        PlainValidator(validate_orientation),\n        Field(\n            description=(\n                'Orientation of the image - sequence of three one-letter strings in any order: \"C\", \"H\", \"W\" '\n                'meaning respectively channels, height and width of the image. Defaults to (\"C\", \"H\", \"W\").'\n            ),\n        ),\n    ] = (\"C\", \"H\", \"W\")\n    device: Annotated[\n        torch.device,\n        PlainValidator(resolve_inference_device_hsi),\n        Field(\n            validate_default=True,\n            exclude=True,\n            description=\"Device to be used for inference. If None, the device of the input image will be used. Defaults to None.\",\n        ),\n    ] = None\n    binary_mask: Annotated[\n        torch.Tensor,\n        PlainValidator(process_and_validate_binary_mask),\n        Field(\n            validate_default=True,\n            description=(\n                \"Binary mask used to cover not important parts of the base image, masked parts have values equals to 0. \"\n                \"Converted to torch tensor. Defaults to None.\"\n            ),\n        ),\n    ] = None\n\n    @property\n    def spectral_axis(self) -&gt; int:\n        \"\"\"Returns the index of the spectral (wavelength) axis based on the current data orientation.\n\n        In hyperspectral imaging, the spectral axis represents the dimension along which\n        different spectral bands or wavelengths are arranged. This property dynamically\n        determines the index of this axis based on the current orientation of the data.\n\n        Returns:\n            int: The index of the spectral axis in the current data structure.\n                - 0 for 'CHW' or 'CWH' orientations (Channel/Wavelength first)\n                - 2 for 'HWC' or 'WHC' orientations (Channel/Wavelength last)\n                - 1 for 'HCW' or 'WCH' orientations (Channel/Wavelength in the middle)\n\n        Note:\n            The orientation is typically represented as a string where:\n            - 'C' represents the spectral/wavelength dimension\n            - 'H' represents the height (rows) of the image\n            - 'W' represents the width (columns) of the image\n\n        Examples:\n            &gt;&gt;&gt; hsi_image = HSI()\n            &gt;&gt;&gt; hsi_image.orientation = \"CHW\"\n            &gt;&gt;&gt; hsi_image.spectral_axis\n            0\n            &gt;&gt;&gt; hsi_image.orientation = \"HWC\"\n            &gt;&gt;&gt; hsi_image.spectral_axis\n            2\n        \"\"\"\n        return get_channel_axis(self.orientation)\n\n    @property\n    def spatial_binary_mask(self) -&gt; torch.Tensor:\n        \"\"\"Returns a 2D spatial representation of the binary mask.\n\n        This property extracts a single 2D slice from the 3D binary mask, assuming that\n        the mask is identical across all spectral bands. It handles different data\n        orientations by first ensuring the spectral dimension is the last dimension\n        before extracting the 2D spatial mask.\n\n        Returns:\n            torch.Tensor: A 2D tensor representing the spatial binary mask.\n                The shape will be (H, W) where H is height and W is width of the image.\n\n        Note:\n            - This assumes that the binary mask is consistent across all spectral bands.\n            - The returned mask is always 2D, regardless of the original data orientation.\n\n        Examples:\n            &gt;&gt;&gt; # If self.binary_mask has shape (100, 100, 5) with spectral_axis=2:\n            &gt;&gt;&gt; hsi_image = HSI(binary_mask=torch.rand(100, 100, 5), orientation=(\"H\", \"W\", \"C\"))\n            &gt;&gt;&gt; hsi_image.spatial_binary_mask.shape\n            torch.Size([100, 100])\n            &gt;&gt;&gt; If self.binary_mask has shape (5, 100, 100) with spectral_axis=0:\n            &gt;&gt;&gt; hsi_image = HSI(binary_mask=torch.rand(5, 100, 100), orientation=(\"C\", \"H\", \"W\"))\n            &gt;&gt;&gt; hsi_image.spatial_binary_mask.shape\n            torch.Size([100, 100])\n        \"\"\"\n        mask = self.binary_mask if self.binary_mask is not None else torch.ones_like(self.image)\n        return mask.select(dim=self.spectral_axis, index=0)\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @model_validator(mode=\"after\")\n    def validate_image_data(self) -&gt; Self:\n        \"\"\"Validates the image data by checking the shape of the wavelengths, image, and spectral_axis.\n\n        Returns:\n            Self: The instance of the class.\n        \"\"\"\n        validate_shapes(self.wavelengths, self.image, self.spectral_axis)\n        return self\n\n    def to(self, device: str | torch.device) -&gt; Self:\n        \"\"\"Moves the image and binary mask (if available) to the specified device.\n\n        Args:\n            device (str or torch.device): The device to move the image and binary mask to.\n\n        Returns:\n            Self: The updated HSI object.\n\n        Examples:\n            &gt;&gt;&gt; # Create an HSI object\n            &gt;&gt;&gt; hsi_image = HSI(image=torch.rand(10, 10, 10), wavelengths=np.arange(10))\n            &gt;&gt;&gt; # Move the image to cpu\n            &gt;&gt;&gt; hsi_image = hsi_image.to(\"cpu\")\n            &gt;&gt;&gt; hsi_image.device\n            device(type='cpu')\n            &gt;&gt;&gt; # Move the image to cuda\n            &gt;&gt;&gt; hsi_image = hsi_image.to(\"cuda\")\n            &gt;&gt;&gt; hsi_image.device\n            device(type='cuda', index=0)\n        \"\"\"\n        self.image = self.image.to(device)\n        self.binary_mask = self.binary_mask.to(device)\n        self.device = self.image.device\n        return self\n\n    def get_image(self, apply_mask: bool = True) -&gt; torch.Tensor:\n        \"\"\"Returns the hyperspectral image data with optional masking applied.\n\n        Args:\n            apply_mask (bool, optional): Whether to apply the binary mask to the image.\n                Defaults to True.\n        Returns:\n            torch.Tensor: The hyperspectral image data.\n\n        Notes:\n            - If apply_mask is True, the binary mask will be applied to the image based on the `binary_mask` attribute.\n\n        Examples:\n            &gt;&gt;&gt; hsi_image = HSI(image=torch.rand(10, 100, 100), wavelengths=np.linspace(400, 1000, 10))\n            &gt;&gt;&gt; image = hsi_image.get_image()\n            &gt;&gt;&gt; image.shape\n            torch.Size([10, 100, 100])\n            &gt;&gt;&gt; image = hsi_image.get_image(apply_mask=False)\n            &gt;&gt;&gt; image.shape\n            torch.Size([10, 100, 100])\n        \"\"\"\n        if apply_mask and self.binary_mask is not None:\n            return self.image * self.binary_mask\n        return self.image\n\n    def get_rgb_image(\n        self, apply_mask: bool = True, apply_min_cutoff: bool = False, output_channel_axis: int | None = None\n    ) -&gt; torch.Tensor:\n        \"\"\"Extracts an RGB representation from the hyperspectral image data.\n\n        This method creates a 3-channel RGB image by selecting appropriate bands\n        corresponding to red, green, and blue wavelengths from the hyperspectral data.\n\n        Args:\n            apply_mask (bool, optional): Whether to apply the binary mask to the image.\n                Defaults to True.\n            apply_min_cutoff (bool, optional): Whether to apply a minimum intensity\n                cutoff to the image. Defaults to False.\n            output_channel_axis (int | None, optional): The axis where the RGB channels\n                should be placed in the output tensor. If None, uses the current spectral\n                axis of the hyperspectral data. Defaults to None.\n\n        Returns:\n            torch.Tensor: The RGB representation of the hyperspectral image.\n                Shape will be either (H, W, 3), (3, H, W), or (H, 3, W) depending on\n                the specified output_channel_axis, where H is height and W is width.\n\n        Notes:\n            - The RGB bands are extracted using predefined wavelength ranges for R, G, and B.\n            - Each band is normalized independently before combining into the RGB image.\n            - If apply_mask is True, masked areas will be set to zero in the output.\n            - If apply_min_cutoff is True, a minimum intensity threshold is applied to each band.\n\n        Examples:\n            &gt;&gt;&gt; hsi_image = HSI(image=torch.rand(10, 100, 100), wavelengths=np.linspace(400, 1000, 10))\n            &gt;&gt;&gt; rgb_image = hsi_image.get_rgb_image()\n            &gt;&gt;&gt; rgb_image.shape\n            torch.Size([100, 100, 3])\n\n            &gt;&gt;&gt; rgb_image = hsi_image.get_rgb_image(output_channel_axis=0)\n            &gt;&gt;&gt; rgb_image.shape\n            torch.Size([3, 100, 100])\n\n            &gt;&gt;&gt; rgb_image = hsi_image.get_rgb_image(apply_mask=False, apply_min_cutoff=True)\n            &gt;&gt;&gt; rgb_image.shape\n            torch.Size([100, 100, 3])\n        \"\"\"\n        if output_channel_axis is None:\n            output_channel_axis = self.spectral_axis\n\n        rgb_img = torch.stack(\n            [\n                self.extract_band_by_name(\n                    band, apply_mask=apply_mask, apply_min_cutoff=apply_min_cutoff, normalize=True\n                )\n                for band in [\"R\", \"G\", \"B\"]\n            ],\n            dim=self.spectral_axis,\n        )\n\n        return (\n            rgb_img\n            if output_channel_axis == self.spectral_axis\n            else torch.moveaxis(rgb_img, self.spectral_axis, output_channel_axis)\n        )\n\n    def _extract_central_slice_from_band(\n        self,\n        band_wavelengths: torch.Tensor,\n        apply_mask: bool = True,\n        apply_min_cutoff: bool = False,\n        normalize: bool = True,\n    ) -&gt; torch.Tensor:\n        \"\"\"Extracts and processes the central wavelength band from a given range in the hyperspectral image.\n\n        This method selects the central band from a specified range of wavelengths,\n        applies optional processing steps (masking, normalization, and minimum cutoff),\n        and returns the resulting 2D image slice.\n\n        Args:\n            band_wavelengths (torch.Tensor): The selected wavelengths that define the whole band\n                from which the central slice will be extracted.\n                All of the passed wavelengths must be present in the image.\n            apply_mask (bool, optional): Whether to apply the binary mask to the extracted band.\n                Defaults to True.\n            apply_min_cutoff (bool, optional): Whether to apply a minimum intensity cutoff.\n                If True, sets the minimum non-zero value to zero after normalization.\n                Defaults to False.\n            normalize (bool, optional): Whether to normalize the band values to [0, 1] range.\n                Defaults to True.\n\n        Returns:\n            torch.Tensor: A 2D tensor representing the processed central wavelength band.\n                Shape will be (H, W), where H is height and W is width of the image.\n\n        Notes:\n            - The central wavelength is determined as the middle index of the provided wavelengths list.\n            - If normalization is applied, it's done before masking and cutoff operations.\n            - The binary mask, if applied, is expected to have the same spatial dimensions as the image.\n\n        Examples:\n            &gt;&gt;&gt; hsi_image = HSI(image=torch.rand(13, 100, 100), wavelengths=np.linspace(400, 1000, 13))\n            &gt;&gt;&gt; band_wavelengths = torch.tensor([500, 600, 650, 700])\n            &gt;&gt;&gt; central_slice = hsi_image._extract_central_slice_from_band(band_wavelengths)\n            &gt;&gt;&gt; central_slice.shape\n            torch.Size([100, 100])\n\n            &gt;&gt;&gt; # Extract a slice without normalization or masking\n            &gt;&gt;&gt; raw_band = hsi_image._extract_central_slice_from_band(band_wavelengths, apply_mask=False, normalize=False)\n        \"\"\"\n        # check if all wavelengths from the `band_wavelengths` are present in the image\n        if not all(wave in self.wavelengths for wave in band_wavelengths):\n            raise ValueError(\"All of the passed wavelengths must be present in the image\")\n\n        # sort the `band_wavelengths` to ensure the central band is selected\n        band_wavelengths = torch.sort(band_wavelengths).values\n\n        start_index = np.where(self.wavelengths == band_wavelengths[0])[0][0]\n        relative_center_band_index = len(band_wavelengths) // 2\n        central_band_index = start_index + relative_center_band_index\n\n        # Ensure the spectral dimension is the last\n        image = self.image if self.spectral_axis == 2 else torch.moveaxis(self.image, self.spectral_axis, 2)\n\n        slice = image[..., central_band_index]\n\n        if normalize:\n            if apply_min_cutoff:\n                slice_min = slice[slice != 0].min()\n            else:\n                slice_min = slice.min()\n\n            slice_max = slice.max()\n            if slice_max &gt; slice_min:  # Avoid division by zero\n                slice = (slice - slice_min) / (slice_max - slice_min)\n\n            if apply_min_cutoff:\n                slice[slice == slice.min()] = 0  # Set minimum values to zero\n\n        if apply_mask:\n            mask = (\n                self.binary_mask if self.spectral_axis == 2 else torch.moveaxis(self.binary_mask, self.spectral_axis, 2)\n            )\n            slice = slice * mask[..., central_band_index]\n\n        return slice\n\n    def extract_band_by_name(\n        self,\n        band_name: str,\n        selection_method: str = \"center\",\n        apply_mask: bool = True,\n        apply_min_cutoff: bool = False,\n        normalize: bool = True,\n    ) -&gt; torch.Tensor:\n        \"\"\"Extracts a single spectral band from the hyperspectral image based on a standardized band name.\n\n        This method uses the spyndex library to map standardized band names to wavelength ranges,\n        then extracts the corresponding band from the hyperspectral data.\n\n        Args:\n            band_name (str): The standardized name of the band to extract (e.g., \"Red\", \"NIR\", \"SWIR1\").\n            selection_method (str, optional): The method to use for selecting the band within the wavelength range.\n                Currently, only \"center\" is supported, which selects the central wavelength.\n                Defaults to \"center\".\n            apply_mask (bool, optional): Whether to apply the binary mask to the extracted band.\n                Defaults to True.\n            apply_min_cutoff (bool, optional): Whether to apply a minimum intensity cutoff after normalization.\n                If True, sets the minimum non-zero value to zero. Defaults to False.\n            normalize (bool, optional): Whether to normalize the band values to the [0, 1] range.\n                Defaults to True.\n\n        Returns:\n            torch.Tensor: A 2D tensor representing the extracted and processed spectral band.\n                Shape will be (H, W), where H is height and W is width of the image.\n\n        Raises:\n            ValueError: If the specified band name is not found in the spyndex library.\n            NotImplementedError: If a selection method other than \"center\" is specified.\n\n        Notes:\n            - The spyndex library is used to map band names to wavelength ranges.\n            - Currently, only the \"center\" selection method is implemented, which chooses\n            the central wavelength within the specified range.\n            - Processing steps are applied in the order: normalization, cutoff, masking.\n\n        Examples:\n            &gt;&gt;&gt; hsi_image = HSI(image=torch.rand(200, 100, 100), wavelengths=np.linspace(400, 2500, 200))\n            &gt;&gt;&gt; red_band = hsi_image.extract_band_by_name(\"Red\")\n            &gt;&gt;&gt; red_band.shape\n            torch.Size([100, 100])\n\n            &gt;&gt;&gt; # Extract NIR band without normalization or masking\n            &gt;&gt;&gt; nir_band = hsi_image.extract_band_by_name(\"NIR\", apply_mask=False, normalize=False)\n        \"\"\"\n        band_info = spyndex.bands.get(band_name)\n        if band_info is None:\n            raise ValueError(f\"Band name '{band_name}' not found in the spyndex library\")\n\n        min_wave, max_wave = band_info.min_wavelength, band_info.max_wavelength\n        selected_wavelengths = self.wavelengths[(self.wavelengths &gt;= min_wave) &amp; (self.wavelengths &lt;= max_wave)]\n\n        if selection_method == \"center\":\n            return self._extract_central_slice_from_band(\n                selected_wavelengths, apply_mask=apply_mask, apply_min_cutoff=apply_min_cutoff, normalize=normalize\n            )\n        else:\n            raise NotImplementedError(\n                f\"Selection method '{selection_method}' is not supported. Only 'center' is currently available.\"\n            )\n\n    def change_orientation(self, target_orientation: tuple[str, str, str] | list[str] | str, inplace=False) -&gt; Self:\n        \"\"\"Changes the orientation of the hsi data to the target orientation.\n\n        Args:\n            target_orientation (tuple[str, str, str], list[str], str): The target orientation for the hsi data.\n                This should be a tuple of three one-letter strings in any order: \"C\", \"H\", \"W\".\n            inplace (bool, optional): Whether to modify the hsi data in place or return a new object.\n\n        Returns:\n            Self: The updated HSI object with the new orientation.\n\n        Raises:\n            ValueError: If the target orientation is not a valid tuple of three one-letter strings.\n        \"\"\"\n        target_orientation = validate_orientation(target_orientation)\n\n        if inplace:\n            hsi = self\n        else:\n            hsi = self.model_copy()\n\n        if target_orientation == self.orientation:\n            return hsi\n\n        permute_dims = [hsi.orientation.index(dim) for dim in target_orientation]\n\n        # permute the image\n        hsi.image = hsi.image.permute(permute_dims)\n\n        # permute the binary mask\n        if hsi.binary_mask is not None:\n            hsi.binary_mask = hsi.binary_mask.permute(permute_dims)\n\n        hsi.orientation = target_orientation\n\n        return hsi\n</code></pre>"},{"location":"reference/#src.meteors.hsi.HSI.spatial_binary_mask","title":"<code>spatial_binary_mask: torch.Tensor</code>  <code>property</code>","text":"<p>Returns a 2D spatial representation of the binary mask.</p> <p>This property extracts a single 2D slice from the 3D binary mask, assuming that the mask is identical across all spectral bands. It handles different data orientations by first ensuring the spectral dimension is the last dimension before extracting the 2D spatial mask.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A 2D tensor representing the spatial binary mask. The shape will be (H, W) where H is height and W is width of the image.</p> Note <ul> <li>This assumes that the binary mask is consistent across all spectral bands.</li> <li>The returned mask is always 2D, regardless of the original data orientation.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # If self.binary_mask has shape (100, 100, 5) with spectral_axis=2:\n&gt;&gt;&gt; hsi_image = HSI(binary_mask=torch.rand(100, 100, 5), orientation=(\"H\", \"W\", \"C\"))\n&gt;&gt;&gt; hsi_image.spatial_binary_mask.shape\ntorch.Size([100, 100])\n&gt;&gt;&gt; If self.binary_mask has shape (5, 100, 100) with spectral_axis=0:\n&gt;&gt;&gt; hsi_image = HSI(binary_mask=torch.rand(5, 100, 100), orientation=(\"C\", \"H\", \"W\"))\n&gt;&gt;&gt; hsi_image.spatial_binary_mask.shape\ntorch.Size([100, 100])\n</code></pre>"},{"location":"reference/#src.meteors.hsi.HSI.spectral_axis","title":"<code>spectral_axis: int</code>  <code>property</code>","text":"<p>Returns the index of the spectral (wavelength) axis based on the current data orientation.</p> <p>In hyperspectral imaging, the spectral axis represents the dimension along which different spectral bands or wavelengths are arranged. This property dynamically determines the index of this axis based on the current orientation of the data.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The index of the spectral axis in the current data structure. - 0 for 'CHW' or 'CWH' orientations (Channel/Wavelength first) - 2 for 'HWC' or 'WHC' orientations (Channel/Wavelength last) - 1 for 'HCW' or 'WCH' orientations (Channel/Wavelength in the middle)</p> Note <p>The orientation is typically represented as a string where: - 'C' represents the spectral/wavelength dimension - 'H' represents the height (rows) of the image - 'W' represents the width (columns) of the image</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hsi_image = HSI()\n&gt;&gt;&gt; hsi_image.orientation = \"CHW\"\n&gt;&gt;&gt; hsi_image.spectral_axis\n0\n&gt;&gt;&gt; hsi_image.orientation = \"HWC\"\n&gt;&gt;&gt; hsi_image.spectral_axis\n2\n</code></pre>"},{"location":"reference/#src.meteors.hsi.HSI.change_orientation","title":"<code>change_orientation(target_orientation, inplace=False)</code>","text":"<p>Changes the orientation of the hsi data to the target orientation.</p> <p>Parameters:</p> Name Type Description Default <code>target_orientation</code> <code>(tuple[str, str, str], list[str], str)</code> <p>The target orientation for the hsi data. This should be a tuple of three one-letter strings in any order: \"C\", \"H\", \"W\".</p> required <code>inplace</code> <code>bool</code> <p>Whether to modify the hsi data in place or return a new object.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>The updated HSI object with the new orientation.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the target orientation is not a valid tuple of three one-letter strings.</p> Source code in <code>src/meteors/hsi.py</code> <pre><code>def change_orientation(self, target_orientation: tuple[str, str, str] | list[str] | str, inplace=False) -&gt; Self:\n    \"\"\"Changes the orientation of the hsi data to the target orientation.\n\n    Args:\n        target_orientation (tuple[str, str, str], list[str], str): The target orientation for the hsi data.\n            This should be a tuple of three one-letter strings in any order: \"C\", \"H\", \"W\".\n        inplace (bool, optional): Whether to modify the hsi data in place or return a new object.\n\n    Returns:\n        Self: The updated HSI object with the new orientation.\n\n    Raises:\n        ValueError: If the target orientation is not a valid tuple of three one-letter strings.\n    \"\"\"\n    target_orientation = validate_orientation(target_orientation)\n\n    if inplace:\n        hsi = self\n    else:\n        hsi = self.model_copy()\n\n    if target_orientation == self.orientation:\n        return hsi\n\n    permute_dims = [hsi.orientation.index(dim) for dim in target_orientation]\n\n    # permute the image\n    hsi.image = hsi.image.permute(permute_dims)\n\n    # permute the binary mask\n    if hsi.binary_mask is not None:\n        hsi.binary_mask = hsi.binary_mask.permute(permute_dims)\n\n    hsi.orientation = target_orientation\n\n    return hsi\n</code></pre>"},{"location":"reference/#src.meteors.hsi.HSI.extract_band_by_name","title":"<code>extract_band_by_name(band_name, selection_method='center', apply_mask=True, apply_min_cutoff=False, normalize=True)</code>","text":"<p>Extracts a single spectral band from the hyperspectral image based on a standardized band name.</p> <p>This method uses the spyndex library to map standardized band names to wavelength ranges, then extracts the corresponding band from the hyperspectral data.</p> <p>Parameters:</p> Name Type Description Default <code>band_name</code> <code>str</code> <p>The standardized name of the band to extract (e.g., \"Red\", \"NIR\", \"SWIR1\").</p> required <code>selection_method</code> <code>str</code> <p>The method to use for selecting the band within the wavelength range. Currently, only \"center\" is supported, which selects the central wavelength. Defaults to \"center\".</p> <code>'center'</code> <code>apply_mask</code> <code>bool</code> <p>Whether to apply the binary mask to the extracted band. Defaults to True.</p> <code>True</code> <code>apply_min_cutoff</code> <code>bool</code> <p>Whether to apply a minimum intensity cutoff after normalization. If True, sets the minimum non-zero value to zero. Defaults to False.</p> <code>False</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize the band values to the [0, 1] range. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A 2D tensor representing the extracted and processed spectral band. Shape will be (H, W), where H is height and W is width of the image.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified band name is not found in the spyndex library.</p> <code>NotImplementedError</code> <p>If a selection method other than \"center\" is specified.</p> Notes <ul> <li>The spyndex library is used to map band names to wavelength ranges.</li> <li>Currently, only the \"center\" selection method is implemented, which chooses the central wavelength within the specified range.</li> <li>Processing steps are applied in the order: normalization, cutoff, masking.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hsi_image = HSI(image=torch.rand(200, 100, 100), wavelengths=np.linspace(400, 2500, 200))\n&gt;&gt;&gt; red_band = hsi_image.extract_band_by_name(\"Red\")\n&gt;&gt;&gt; red_band.shape\ntorch.Size([100, 100])\n</code></pre> <pre><code>&gt;&gt;&gt; # Extract NIR band without normalization or masking\n&gt;&gt;&gt; nir_band = hsi_image.extract_band_by_name(\"NIR\", apply_mask=False, normalize=False)\n</code></pre> Source code in <code>src/meteors/hsi.py</code> <pre><code>def extract_band_by_name(\n    self,\n    band_name: str,\n    selection_method: str = \"center\",\n    apply_mask: bool = True,\n    apply_min_cutoff: bool = False,\n    normalize: bool = True,\n) -&gt; torch.Tensor:\n    \"\"\"Extracts a single spectral band from the hyperspectral image based on a standardized band name.\n\n    This method uses the spyndex library to map standardized band names to wavelength ranges,\n    then extracts the corresponding band from the hyperspectral data.\n\n    Args:\n        band_name (str): The standardized name of the band to extract (e.g., \"Red\", \"NIR\", \"SWIR1\").\n        selection_method (str, optional): The method to use for selecting the band within the wavelength range.\n            Currently, only \"center\" is supported, which selects the central wavelength.\n            Defaults to \"center\".\n        apply_mask (bool, optional): Whether to apply the binary mask to the extracted band.\n            Defaults to True.\n        apply_min_cutoff (bool, optional): Whether to apply a minimum intensity cutoff after normalization.\n            If True, sets the minimum non-zero value to zero. Defaults to False.\n        normalize (bool, optional): Whether to normalize the band values to the [0, 1] range.\n            Defaults to True.\n\n    Returns:\n        torch.Tensor: A 2D tensor representing the extracted and processed spectral band.\n            Shape will be (H, W), where H is height and W is width of the image.\n\n    Raises:\n        ValueError: If the specified band name is not found in the spyndex library.\n        NotImplementedError: If a selection method other than \"center\" is specified.\n\n    Notes:\n        - The spyndex library is used to map band names to wavelength ranges.\n        - Currently, only the \"center\" selection method is implemented, which chooses\n        the central wavelength within the specified range.\n        - Processing steps are applied in the order: normalization, cutoff, masking.\n\n    Examples:\n        &gt;&gt;&gt; hsi_image = HSI(image=torch.rand(200, 100, 100), wavelengths=np.linspace(400, 2500, 200))\n        &gt;&gt;&gt; red_band = hsi_image.extract_band_by_name(\"Red\")\n        &gt;&gt;&gt; red_band.shape\n        torch.Size([100, 100])\n\n        &gt;&gt;&gt; # Extract NIR band without normalization or masking\n        &gt;&gt;&gt; nir_band = hsi_image.extract_band_by_name(\"NIR\", apply_mask=False, normalize=False)\n    \"\"\"\n    band_info = spyndex.bands.get(band_name)\n    if band_info is None:\n        raise ValueError(f\"Band name '{band_name}' not found in the spyndex library\")\n\n    min_wave, max_wave = band_info.min_wavelength, band_info.max_wavelength\n    selected_wavelengths = self.wavelengths[(self.wavelengths &gt;= min_wave) &amp; (self.wavelengths &lt;= max_wave)]\n\n    if selection_method == \"center\":\n        return self._extract_central_slice_from_band(\n            selected_wavelengths, apply_mask=apply_mask, apply_min_cutoff=apply_min_cutoff, normalize=normalize\n        )\n    else:\n        raise NotImplementedError(\n            f\"Selection method '{selection_method}' is not supported. Only 'center' is currently available.\"\n        )\n</code></pre>"},{"location":"reference/#src.meteors.hsi.HSI.get_image","title":"<code>get_image(apply_mask=True)</code>","text":"<p>Returns the hyperspectral image data with optional masking applied.</p> <p>Parameters:</p> Name Type Description Default <code>apply_mask</code> <code>bool</code> <p>Whether to apply the binary mask to the image. Defaults to True.</p> <code>True</code> <p>Returns:     torch.Tensor: The hyperspectral image data.</p> Notes <ul> <li>If apply_mask is True, the binary mask will be applied to the image based on the <code>binary_mask</code> attribute.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hsi_image = HSI(image=torch.rand(10, 100, 100), wavelengths=np.linspace(400, 1000, 10))\n&gt;&gt;&gt; image = hsi_image.get_image()\n&gt;&gt;&gt; image.shape\ntorch.Size([10, 100, 100])\n&gt;&gt;&gt; image = hsi_image.get_image(apply_mask=False)\n&gt;&gt;&gt; image.shape\ntorch.Size([10, 100, 100])\n</code></pre> Source code in <code>src/meteors/hsi.py</code> <pre><code>def get_image(self, apply_mask: bool = True) -&gt; torch.Tensor:\n    \"\"\"Returns the hyperspectral image data with optional masking applied.\n\n    Args:\n        apply_mask (bool, optional): Whether to apply the binary mask to the image.\n            Defaults to True.\n    Returns:\n        torch.Tensor: The hyperspectral image data.\n\n    Notes:\n        - If apply_mask is True, the binary mask will be applied to the image based on the `binary_mask` attribute.\n\n    Examples:\n        &gt;&gt;&gt; hsi_image = HSI(image=torch.rand(10, 100, 100), wavelengths=np.linspace(400, 1000, 10))\n        &gt;&gt;&gt; image = hsi_image.get_image()\n        &gt;&gt;&gt; image.shape\n        torch.Size([10, 100, 100])\n        &gt;&gt;&gt; image = hsi_image.get_image(apply_mask=False)\n        &gt;&gt;&gt; image.shape\n        torch.Size([10, 100, 100])\n    \"\"\"\n    if apply_mask and self.binary_mask is not None:\n        return self.image * self.binary_mask\n    return self.image\n</code></pre>"},{"location":"reference/#src.meteors.hsi.HSI.get_rgb_image","title":"<code>get_rgb_image(apply_mask=True, apply_min_cutoff=False, output_channel_axis=None)</code>","text":"<p>Extracts an RGB representation from the hyperspectral image data.</p> <p>This method creates a 3-channel RGB image by selecting appropriate bands corresponding to red, green, and blue wavelengths from the hyperspectral data.</p> <p>Parameters:</p> Name Type Description Default <code>apply_mask</code> <code>bool</code> <p>Whether to apply the binary mask to the image. Defaults to True.</p> <code>True</code> <code>apply_min_cutoff</code> <code>bool</code> <p>Whether to apply a minimum intensity cutoff to the image. Defaults to False.</p> <code>False</code> <code>output_channel_axis</code> <code>int | None</code> <p>The axis where the RGB channels should be placed in the output tensor. If None, uses the current spectral axis of the hyperspectral data. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The RGB representation of the hyperspectral image. Shape will be either (H, W, 3), (3, H, W), or (H, 3, W) depending on the specified output_channel_axis, where H is height and W is width.</p> Notes <ul> <li>The RGB bands are extracted using predefined wavelength ranges for R, G, and B.</li> <li>Each band is normalized independently before combining into the RGB image.</li> <li>If apply_mask is True, masked areas will be set to zero in the output.</li> <li>If apply_min_cutoff is True, a minimum intensity threshold is applied to each band.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hsi_image = HSI(image=torch.rand(10, 100, 100), wavelengths=np.linspace(400, 1000, 10))\n&gt;&gt;&gt; rgb_image = hsi_image.get_rgb_image()\n&gt;&gt;&gt; rgb_image.shape\ntorch.Size([100, 100, 3])\n</code></pre> <pre><code>&gt;&gt;&gt; rgb_image = hsi_image.get_rgb_image(output_channel_axis=0)\n&gt;&gt;&gt; rgb_image.shape\ntorch.Size([3, 100, 100])\n</code></pre> <pre><code>&gt;&gt;&gt; rgb_image = hsi_image.get_rgb_image(apply_mask=False, apply_min_cutoff=True)\n&gt;&gt;&gt; rgb_image.shape\ntorch.Size([100, 100, 3])\n</code></pre> Source code in <code>src/meteors/hsi.py</code> <pre><code>def get_rgb_image(\n    self, apply_mask: bool = True, apply_min_cutoff: bool = False, output_channel_axis: int | None = None\n) -&gt; torch.Tensor:\n    \"\"\"Extracts an RGB representation from the hyperspectral image data.\n\n    This method creates a 3-channel RGB image by selecting appropriate bands\n    corresponding to red, green, and blue wavelengths from the hyperspectral data.\n\n    Args:\n        apply_mask (bool, optional): Whether to apply the binary mask to the image.\n            Defaults to True.\n        apply_min_cutoff (bool, optional): Whether to apply a minimum intensity\n            cutoff to the image. Defaults to False.\n        output_channel_axis (int | None, optional): The axis where the RGB channels\n            should be placed in the output tensor. If None, uses the current spectral\n            axis of the hyperspectral data. Defaults to None.\n\n    Returns:\n        torch.Tensor: The RGB representation of the hyperspectral image.\n            Shape will be either (H, W, 3), (3, H, W), or (H, 3, W) depending on\n            the specified output_channel_axis, where H is height and W is width.\n\n    Notes:\n        - The RGB bands are extracted using predefined wavelength ranges for R, G, and B.\n        - Each band is normalized independently before combining into the RGB image.\n        - If apply_mask is True, masked areas will be set to zero in the output.\n        - If apply_min_cutoff is True, a minimum intensity threshold is applied to each band.\n\n    Examples:\n        &gt;&gt;&gt; hsi_image = HSI(image=torch.rand(10, 100, 100), wavelengths=np.linspace(400, 1000, 10))\n        &gt;&gt;&gt; rgb_image = hsi_image.get_rgb_image()\n        &gt;&gt;&gt; rgb_image.shape\n        torch.Size([100, 100, 3])\n\n        &gt;&gt;&gt; rgb_image = hsi_image.get_rgb_image(output_channel_axis=0)\n        &gt;&gt;&gt; rgb_image.shape\n        torch.Size([3, 100, 100])\n\n        &gt;&gt;&gt; rgb_image = hsi_image.get_rgb_image(apply_mask=False, apply_min_cutoff=True)\n        &gt;&gt;&gt; rgb_image.shape\n        torch.Size([100, 100, 3])\n    \"\"\"\n    if output_channel_axis is None:\n        output_channel_axis = self.spectral_axis\n\n    rgb_img = torch.stack(\n        [\n            self.extract_band_by_name(\n                band, apply_mask=apply_mask, apply_min_cutoff=apply_min_cutoff, normalize=True\n            )\n            for band in [\"R\", \"G\", \"B\"]\n        ],\n        dim=self.spectral_axis,\n    )\n\n    return (\n        rgb_img\n        if output_channel_axis == self.spectral_axis\n        else torch.moveaxis(rgb_img, self.spectral_axis, output_channel_axis)\n    )\n</code></pre>"},{"location":"reference/#src.meteors.hsi.HSI.to","title":"<code>to(device)</code>","text":"<p>Moves the image and binary mask (if available) to the specified device.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str or device</code> <p>The device to move the image and binary mask to.</p> required <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>The updated HSI object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Create an HSI object\n&gt;&gt;&gt; hsi_image = HSI(image=torch.rand(10, 10, 10), wavelengths=np.arange(10))\n&gt;&gt;&gt; # Move the image to cpu\n&gt;&gt;&gt; hsi_image = hsi_image.to(\"cpu\")\n&gt;&gt;&gt; hsi_image.device\ndevice(type='cpu')\n&gt;&gt;&gt; # Move the image to cuda\n&gt;&gt;&gt; hsi_image = hsi_image.to(\"cuda\")\n&gt;&gt;&gt; hsi_image.device\ndevice(type='cuda', index=0)\n</code></pre> Source code in <code>src/meteors/hsi.py</code> <pre><code>def to(self, device: str | torch.device) -&gt; Self:\n    \"\"\"Moves the image and binary mask (if available) to the specified device.\n\n    Args:\n        device (str or torch.device): The device to move the image and binary mask to.\n\n    Returns:\n        Self: The updated HSI object.\n\n    Examples:\n        &gt;&gt;&gt; # Create an HSI object\n        &gt;&gt;&gt; hsi_image = HSI(image=torch.rand(10, 10, 10), wavelengths=np.arange(10))\n        &gt;&gt;&gt; # Move the image to cpu\n        &gt;&gt;&gt; hsi_image = hsi_image.to(\"cpu\")\n        &gt;&gt;&gt; hsi_image.device\n        device(type='cpu')\n        &gt;&gt;&gt; # Move the image to cuda\n        &gt;&gt;&gt; hsi_image = hsi_image.to(\"cuda\")\n        &gt;&gt;&gt; hsi_image.device\n        device(type='cuda', index=0)\n    \"\"\"\n    self.image = self.image.to(device)\n    self.binary_mask = self.binary_mask.to(device)\n    self.device = self.image.device\n    return self\n</code></pre>"},{"location":"reference/#visualizations","title":"Visualizations","text":""},{"location":"reference/#src.meteors.visualize.hsi_visualize.visualize_hsi","title":"<code>visualize_hsi(hsi_or_attributes, ax=None, use_mask=True)</code>","text":"<p>Visualizes a Hyperspectral image object on the given axes. It uses either the object from HSI class or a field from the HSIAttributes class.</p> <p>Parameters:</p> Name Type Description Default <code>hsi_or_attributes</code> <code>HSI | HSIAttributes</code> <p>The hyperspectral image, or the attributes to be visualized.</p> required <code>ax</code> <code>Axes | None</code> <p>The axes on which the image will be plotted. If None, the current axes will be used.</p> <code>None</code> <code>use_mask</code> <code>bool</code> <p>Whether to use the image mask if provided for the visualization.</p> <code>True</code> <p>Returns:</p> Type Description <code>Axes</code> <p>matplotlib.figure.Figure | None: If use_pyplot is False, returns the figure and axes objects. If use_pyplot is True, returns None.</p> Source code in <code>src/meteors/visualize/hsi_visualize.py</code> <pre><code>def visualize_hsi(hsi_or_attributes: HSI | HSIAttributes, ax: Axes | None = None, use_mask: bool = True) -&gt; Axes:\n    \"\"\"Visualizes a Hyperspectral image object on the given axes. It uses either the object from HSI class or a field\n    from the HSIAttributes class.\n\n    Parameters:\n        hsi_or_attributes (HSI | HSIAttributes): The hyperspectral image, or the attributes to be visualized.\n        ax (matplotlib.axes.Axes | None): The axes on which the image will be plotted.\n            If None, the current axes will be used.\n        use_mask (bool): Whether to use the image mask if provided for the visualization.\n\n\n    Returns:\n        matplotlib.figure.Figure | None:\n            If use_pyplot is False, returns the figure and axes objects.\n            If use_pyplot is True, returns None.\n    \"\"\"\n    if isinstance(hsi_or_attributes, HSIAttributes):\n        hsi = hsi_or_attributes.hsi\n    else:\n        hsi = hsi_or_attributes\n\n    hsi = hsi.change_orientation(\"HWC\", inplace=False)\n\n    rgb = hsi.get_rgb_image(output_channel_axis=2, apply_mask=use_mask).cpu().numpy()\n    ax = ax or plt.gca()\n    ax.imshow(rgb)\n\n    return ax\n</code></pre>"},{"location":"reference/#src.meteors.visualize.attr_visualize.visualize_attributes","title":"<code>visualize_attributes(image_attributes, use_pyplot=False)</code>","text":"<p>Visualizes the attributes of an image on the given axes.</p> <p>Parameters:</p> Name Type Description Default <code>image_attributes</code> <code>HSIAttributes</code> <p>The image attributes to be visualized.</p> required <code>use_pyplot</code> <code>bool</code> <p>If True, uses pyplot to display the image. If False, returns the figure and axes objects.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[Figure, Axes] | None</code> <p>matplotlib.figure.Figure | None: If use_pyplot is False, returns the figure and axes objects. If use_pyplot is True, returns None . If all the attributions are zero, returns None.</p> Source code in <code>src/meteors/visualize/attr_visualize.py</code> <pre><code>def visualize_attributes(image_attributes: HSIAttributes, use_pyplot: bool = False) -&gt; tuple[Figure, Axes] | None:\n    \"\"\"Visualizes the attributes of an image on the given axes.\n\n    Parameters:\n        image_attributes (HSIAttributes): The image attributes to be visualized.\n        use_pyplot (bool): If True, uses pyplot to display the image. If False, returns the figure and axes objects.\n\n    Returns:\n        matplotlib.figure.Figure | None:\n            If use_pyplot is False, returns the figure and axes objects.\n            If use_pyplot is True, returns None .\n            If all the attributions are zero, returns None.\n    \"\"\"\n\n    if image_attributes.hsi.orientation != (\"C\", \"H\", \"W\"):\n        raise ValueError(f\"HSI orientation {image_attributes.hsi.orientation} is not supported yet\")\n\n    rotated_attributes_dataclass = image_attributes.change_orientation(\"HWC\", inplace=False)\n    rotated_attributes = rotated_attributes_dataclass.attributes.detach().cpu().numpy()\n    if np.all(rotated_attributes == 0):\n        warnings.warn(\"All the attributions are zero. There is nothing to visualize.\")\n        return None\n\n    fig, ax = plt.subplots(2, 3, figsize=(15, 5))\n    ax[0, 0].set_title(\"Attribution Heatmap\")\n    ax[0, 0].grid(False)\n    ax[0, 0].axis(\"off\")\n\n    fig.suptitle(f\"HSI Attributes of: {image_attributes.attribution_method}\")\n\n    _ = viz.visualize_image_attr(\n        rotated_attributes,\n        method=\"heat_map\",\n        sign=\"all\",\n        plt_fig_axis=(fig, ax[0, 0]),\n        show_colorbar=True,\n        use_pyplot=False,\n    )\n\n    ax[0, 1].set_title(\"Attribution Module Values\")\n    ax[0, 1].grid(False)\n    ax[0, 1].axis(\"off\")\n\n    # Attributions module values\n    _ = viz.visualize_image_attr(\n        rotated_attributes,\n        method=\"heat_map\",\n        sign=\"absolute_value\",\n        plt_fig_axis=(fig, ax[0, 1]),\n        show_colorbar=True,\n        use_pyplot=False,\n    )\n\n    sign_attr = np.sign(rotated_attributes).sum(axis=-1) / rotated_attributes.shape[-1]\n    sns.heatmap(sign_attr, cmap=\"PiYG\", vmin=-1, vmax=1, square=True, ax=ax[0, 2])\n    ax[0, 2].set(title=\"Attribution Sign Values\")\n    ax[0, 2].grid(False)\n    ax[0, 2].set_xticks([])\n    ax[0, 2].set_yticks([])\n\n    attr_all = rotated_attributes.sum(axis=(0, 1))\n    ax[1, 0].scatter(image_attributes.hsi.wavelengths, attr_all, c=\"r\")\n    ax[1, 0].set_title(\"Spectral Attribution\")\n    ax[1, 0].set_xlabel(\"Wavelength\")\n    ax[1, 0].set_ylabel(\"Attribution\")\n    ax[1, 0].grid(True)\n\n    attr_abs = np.abs(rotated_attributes).sum(axis=(0, 1))\n    ax[1, 1].scatter(image_attributes.hsi.wavelengths, attr_abs, c=\"b\")\n    ax[1, 1].set_title(\"Spectral Attribution Absolute Values\")\n    ax[1, 1].set_xlabel(\"Wavelength\")\n    ax[1, 1].set_ylabel(\"Attribution Absolute Value\")\n    ax[1, 1].grid(True)\n\n    # Sign values\n    sign_attr = np.sign(rotated_attributes).sum(axis=(0, 1)) / rotated_attributes.shape[0] / rotated_attributes.shape[1]\n    ax[1, 2].scatter(image_attributes.hsi.wavelengths, sign_attr, c=\"g\")\n    ax[1, 2].set_title(\"Spectral Attribution Sign Values\")\n    ax[1, 2].set_xlabel(\"Wavelength\")\n    ax[1, 2].set_ylabel(\"Attribution Sign Proportion\")\n    ax[1, 2].grid(True)\n    ax[1, 2].set_yticks([-1, 0, 1])\n\n    plt.tight_layout()\n\n    if use_pyplot:\n        plt.show()  # pragma: no cover\n        return None  # pragma: no cover\n\n    return fig, ax\n</code></pre>"},{"location":"reference/#src.meteors.visualize.lime_visualize.visualize_spectral_attributes_by_waveband","title":"<code>visualize_spectral_attributes_by_waveband(spectral_attributes, ax, color_palette=None, show_not_included=True, show_legend=True)</code>","text":"<p>Visualizes spectral attributes by waveband.</p> <p>Parameters:</p> Name Type Description Default <code>spectral_attributes</code> <code>HSISpectralAttributes | list[HSISpectralAttributes]</code> <p>The spectral attributes to visualize.</p> required <code>ax</code> <code>Axes | None</code> <p>The matplotlib axes to plot the visualization on. If None, a new axes will be created.</p> required <code>color_palette</code> <code>list[str] | None</code> <p>The color palette to use for plotting. If None, a default color palette will be used.</p> <code>None</code> <code>show_not_included</code> <code>bool</code> <p>Whether to show the \"not_included\" band in the visualization. Default is True.</p> <code>True</code> <code>show_legend</code> <code>bool</code> <p>Whether to show the legend in the visualization.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Axes</code> <code>Axes</code> <p>The matplotlib axes object containing the visualization.</p> Source code in <code>src/meteors/visualize/lime_visualize.py</code> <pre><code>def visualize_spectral_attributes_by_waveband(\n    spectral_attributes: HSISpectralAttributes | list[HSISpectralAttributes],\n    ax: Axes | None,\n    color_palette: list[str] | None = None,\n    show_not_included: bool = True,\n    show_legend: bool = True,\n) -&gt; Axes:\n    \"\"\"Visualizes spectral attributes by waveband.\n\n    Args:\n        spectral_attributes (HSISpectralAttributes | list[HSISpectralAttributes]):\n            The spectral attributes to visualize.\n        ax (Axes | None): The matplotlib axes to plot the visualization on.\n            If None, a new axes will be created.\n        color_palette (list[str] | None): The color palette to use for plotting.\n            If None, a default color palette will be used.\n        show_not_included (bool): Whether to show the \"not_included\" band in the visualization.\n            Default is True.\n        show_legend (bool): Whether to show the legend in the visualization.\n\n    Returns:\n        Axes: The matplotlib axes object containing the visualization.\n    \"\"\"\n    if isinstance(spectral_attributes, HSISpectralAttributes):\n        spectral_attributes = [spectral_attributes]\n    if not (\n        isinstance(spectral_attributes, list)\n        and all(isinstance(attr, HSISpectralAttributes) for attr in spectral_attributes)\n    ):\n        raise ValueError(\n            \"spectral_attributes must be an HSISpectralAttributes object or a list of HSISpectralAttributes objects.\"\n        )\n\n    aggregate_results = False if len(spectral_attributes) == 1 else True\n    band_names = dict(spectral_attributes[0].band_names)\n    wavelengths = spectral_attributes[0].hsi.wavelengths\n    validate_consistent_band_and_wavelengths(band_names, wavelengths, spectral_attributes)\n\n    ax = setup_visualization(ax, \"Attributions by Waveband\", \"Wavelength (nm)\", \"Correlation with Output\")\n\n    if not show_not_included and band_names.get(\"not_included\") is not None:\n        band_names.pop(\"not_included\")\n\n    band_names = _merge_band_names_segments(band_names)  # type: ignore\n\n    if color_palette is None:\n        color_palette = sns.color_palette(\"hsv\", len(band_names.keys()))\n\n    band_mask = spectral_attributes[0].band_mask.cpu()\n    attribution_map = torch.stack([attr.flattened_attributes.cpu() for attr in spectral_attributes])\n\n    for idx, (band_name, segment_id) in enumerate(band_names.items()):\n        current_wavelengths = wavelengths[band_mask == segment_id]\n        current_attribution_map = attribution_map[:, band_mask == segment_id]\n\n        current_mean = current_attribution_map.numpy().mean(axis=0)\n        if aggregate_results:\n            lolims = current_attribution_map.numpy().min(axis=0)\n            uplims = current_attribution_map.numpy().max(axis=0)\n\n            ax.errorbar(\n                current_wavelengths.numpy(),\n                current_mean,\n                yerr=[current_mean - lolims, uplims - current_mean],\n                label=band_name,\n                color=color_palette[idx],\n                linestyle=\"--\",\n                marker=\"o\",\n                markersize=5,\n            )\n        else:\n            ax.scatter(\n                current_wavelengths.numpy(),\n                current_mean,\n                label=band_name,\n                color=color_palette[idx],\n            )\n\n    if show_legend:\n        ax.legend(title=\"SuperBand\")\n\n    return ax\n</code></pre>"},{"location":"reference/#src.meteors.visualize.lime_visualize.visualize_spectral_attributes_by_magnitude","title":"<code>visualize_spectral_attributes_by_magnitude(spectral_attributes, ax, color_palette=None, annotate_bars=True, show_not_included=True)</code>","text":"<p>Visualizes the spectral attributes by magnitude.</p> <p>Parameters:</p> Name Type Description Default <code>spectral_attributes</code> <code>HSISpectralAttributes | list[HSISpectralAttributes]</code> <p>The spectral attributes to visualize.</p> required <code>ax</code> <code>Axes | None</code> <p>The matplotlib Axes object to plot the visualization on. If None, a new Axes object will be created.</p> required <code>color_palette</code> <code>list[str] | None</code> <p>The color palette to use for the visualization. If None, a default color palette will be used.</p> <code>None</code> <code>annotate_bars</code> <code>bool</code> <p>Whether to annotate the bars with their magnitudes. Defaults to True.</p> <code>True</code> <code>show_not_included</code> <code>bool</code> <p>Whether to show the 'not_included' band in the visualization. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Axes</code> <code>Axes</code> <p>The matplotlib Axes object containing the visualization.</p> Source code in <code>src/meteors/visualize/lime_visualize.py</code> <pre><code>def visualize_spectral_attributes_by_magnitude(\n    spectral_attributes: HSISpectralAttributes | list[HSISpectralAttributes],\n    ax: Axes | None,\n    color_palette: list[str] | None = None,\n    annotate_bars: bool = True,\n    show_not_included: bool = True,\n) -&gt; Axes:\n    \"\"\"Visualizes the spectral attributes by magnitude.\n\n    Args:\n        spectral_attributes (HSISpectralAttributes | list[HSISpectralAttributes]):\n            The spectral attributes to visualize.\n        ax (Axes | None): The matplotlib Axes object to plot the visualization on.\n            If None, a new Axes object will be created.\n        color_palette (list[str] | None): The color palette to use for the visualization.\n            If None, a default color palette will be used.\n        annotate_bars (bool): Whether to annotate the bars with their magnitudes.\n            Defaults to True.\n        show_not_included (bool): Whether to show the 'not_included' band in the visualization.\n            Defaults to True.\n\n    Returns:\n        Axes: The matplotlib Axes object containing the visualization.\n    \"\"\"\n    if isinstance(spectral_attributes, HSISpectralAttributes):\n        spectral_attributes = [spectral_attributes]\n    if not (\n        isinstance(spectral_attributes, list)\n        and all(isinstance(attr, HSISpectralAttributes) for attr in spectral_attributes)\n    ):\n        raise ValueError(\n            \"spectral_attributes must be an HSISpectralAttributes object or a list of HSISpectralAttributes objects.\"\n        )\n\n    aggregate_results = False if len(spectral_attributes) == 1 else True\n    band_names = dict(spectral_attributes[0].band_names)\n    wavelengths = spectral_attributes[0].hsi.wavelengths\n    validate_consistent_band_and_wavelengths(band_names, wavelengths, spectral_attributes)\n\n    ax = setup_visualization(ax, \"Attributions by Magnitude\", \"Group\", \"Average Attribution Magnitude\")\n    ax.tick_params(axis=\"x\", rotation=45)\n\n    band_names = _merge_band_names_segments(band_names)  # type: ignore\n    labels = list(band_names.keys())\n\n    if not show_not_included and band_names.get(\"not_included\") is not None:\n        band_names.pop(\"not_included\")\n        labels = list(band_names.keys())\n\n    if color_palette is None:\n        color_palette = sns.color_palette(\"hsv\", len(band_names.keys()))\n\n    band_mask = spectral_attributes[0].band_mask.cpu()\n    attribution_map = torch.stack([attr.flattened_attributes.cpu() for attr in spectral_attributes])\n    avg_magnitudes = calculate_average_magnitudes(band_names, band_mask, attribution_map)\n\n    if aggregate_results:\n        boxplot = ax.boxplot(avg_magnitudes, labels=labels, patch_artist=True)\n        for patch, color in zip(boxplot[\"boxes\"], color_palette):\n            patch.set_facecolor(color)\n\n    else:\n        bars = ax.bar(labels, avg_magnitudes, color=color_palette)\n        if annotate_bars:\n            for bar in bars:\n                height = bar.get_height()\n                ax.annotate(\n                    f\"{height:.2f}\",\n                    xy=(bar.get_x() + bar.get_width() / 2, height),\n                    xytext=(0, 3),  # 3 points vertical offset\n                    textcoords=\"offset points\",\n                    ha=\"center\",\n                    va=\"bottom\",\n                )\n    return ax\n</code></pre>"},{"location":"reference/#src.meteors.visualize.lime_visualize.visualize_spectral_attributes","title":"<code>visualize_spectral_attributes(spectral_attributes, use_pyplot=False, color_palette=None, show_not_included=True)</code>","text":"<p>Visualizes the spectral attributes of an hsi object or a list of hsi objects.</p> <p>Parameters:</p> Name Type Description Default <code>spectral_attributes</code> <code>HSISpectralAttributes | list[HSISpectralAttributes]</code> <p>The spectral attributes of the image object to visualize.</p> required <code>use_pyplot</code> <code>bool</code> <p>If True, displays the visualization using pyplot. If False, returns the figure and axes objects. Defaults to False.</p> <code>False</code> <code>color_palette</code> <code>list[str] | None</code> <p>The color palette to use for visualizing different spectral bands. If None, a default color palette is used. Defaults to None.</p> <code>None</code> <code>show_not_included</code> <code>bool</code> <p>If True, includes the spectral bands that are not included in the visualization. If False, only includes the spectral bands that are included in the visualization. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple[Figure, Axes] | None</code> <p>tuple[matplotlib.figure.Figure, matplotlib.axes.Axes] | None: If use_pyplot is False, returns the figure and axes objects. If use_pyplot is True, returns None.</p> Source code in <code>src/meteors/visualize/lime_visualize.py</code> <pre><code>def visualize_spectral_attributes(\n    spectral_attributes: HSISpectralAttributes | list[HSISpectralAttributes],\n    use_pyplot: bool = False,\n    color_palette: list[str] | None = None,\n    show_not_included: bool = True,\n) -&gt; tuple[Figure, Axes] | None:\n    \"\"\"Visualizes the spectral attributes of an hsi object or a list of hsi objects.\n\n    Args:\n        spectral_attributes (HSISpectralAttributes | list[HSISpectralAttributes]):\n            The spectral attributes of the image object to visualize.\n        use_pyplot (bool, optional):\n            If True, displays the visualization using pyplot.\n            If False, returns the figure and axes objects. Defaults to False.\n        color_palette (list[str] | None, optional):\n            The color palette to use for visualizing different spectral bands.\n            If None, a default color palette is used.\n            Defaults to None.\n        show_not_included (bool, optional):\n            If True, includes the spectral bands that are not included in the visualization.\n            If False, only includes the spectral bands that are included in the visualization.\n            Defaults to True.\n\n    Returns:\n        tuple[matplotlib.figure.Figure, matplotlib.axes.Axes] | None:\n            If use_pyplot is False, returns the figure and axes objects.\n            If use_pyplot is True, returns None.\n    \"\"\"\n    agg = True if isinstance(spectral_attributes, list) else False\n    band_names = spectral_attributes[0].band_names if agg else spectral_attributes.band_names  # type: ignore\n\n    color_palette = color_palette or sns.color_palette(\"hsv\", len(band_names.keys()))\n\n    fig, ax = plt.subplots(1, 3 if agg else 2, figsize=(15, 5))\n    fig.suptitle(\"Spectral Attributes Visualization\")\n\n    visualize_spectral_attributes_by_waveband(\n        spectral_attributes,\n        ax[0],\n        color_palette=color_palette,\n        show_not_included=show_not_included,\n        show_legend=False,\n    )\n\n    visualize_spectral_attributes_by_magnitude(\n        spectral_attributes,\n        ax[1],\n        color_palette=color_palette,\n        show_not_included=show_not_included,\n    )\n\n    if agg:\n        scores = [attr.score for attr in spectral_attributes]  # type: ignore\n        mean_score = sum(scores) / len(scores)  # type: ignore\n        ax[2].hist(scores, bins=50, color=\"steelblue\", alpha=0.7)\n        ax[2].axvline(mean_score, color=\"darkred\", linestyle=\"dashed\")\n\n        ax[2].set_title(\"Distribution of Score Values\")\n        ax[2].set_xlabel(\"Score\")\n        ax[2].set_ylabel(\"Frequency\")\n\n    if use_pyplot:\n        plt.show()  # pragma: no cover\n        return None  # pragma: no cover\n\n    return fig, ax\n</code></pre>"},{"location":"reference/#src.meteors.visualize.lime_visualize.visualize_spatial_attributes","title":"<code>visualize_spatial_attributes(spatial_attributes, use_pyplot=False)</code>","text":"<p>Visualizes the spatial attributes of an hsi using Lime attribution.</p> <p>Parameters:</p> Name Type Description Default <code>spatial_attributes</code> <code>HSISpatialAttributes</code> <p>The spatial attributes of the image object to visualize.</p> required <code>use_pyplot</code> <code>bool</code> <p>Whether to use pyplot for visualization. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[Figure, Axes] | None</code> <p>tuple[matplotlib.figure.Figure, matplotlib.axes.Axes] | None: If use_pyplot is False, returns the figure and axes objects. If use_pyplot is True, returns None.</p> Source code in <code>src/meteors/visualize/lime_visualize.py</code> <pre><code>def visualize_spatial_attributes(\n    spatial_attributes: HSISpatialAttributes, use_pyplot: bool = False\n) -&gt; tuple[Figure, Axes] | None:\n    \"\"\"Visualizes the spatial attributes of an hsi using Lime attribution.\n\n    Args:\n        spatial_attributes (HSISpatialAttributes):\n            The spatial attributes of the image object to visualize.\n        use_pyplot (bool, optional):\n            Whether to use pyplot for visualization. Defaults to False.\n\n    Returns:\n        tuple[matplotlib.figure.Figure, matplotlib.axes.Axes] | None:\n            If use_pyplot is False, returns the figure and axes objects.\n            If use_pyplot is True, returns None.\n    \"\"\"\n    mask_enabled = spatial_attributes.segmentation_mask is not None\n    fig, ax = plt.subplots(1, 3 if mask_enabled else 2, figsize=(15, 5))\n    fig.suptitle(\"Spatial Attributes Visualization\")\n    spatial_attributes = spatial_attributes.change_orientation(\"HWC\", inplace=False)\n\n    if mask_enabled:\n        mask = spatial_attributes.segmentation_mask.cpu()\n\n        group_names = mask.unique().tolist()\n        colors = sns.color_palette(\"hsv\", len(group_names))\n        color_map = dict(zip(group_names, colors))\n\n        for unique in group_names:\n            segment_indices = torch.argwhere(mask == unique)\n\n            y_center, x_center = segment_indices.numpy().mean(axis=0).astype(int)\n            ax[1].text(x_center, y_center, str(unique), color=color_map[unique], fontsize=8, ha=\"center\", va=\"center\")\n            ax[2].text(x_center, y_center, str(unique), color=color_map[unique], fontsize=8, ha=\"center\", va=\"center\")\n\n        ax[2].imshow(mask.numpy() / mask.max(), cmap=\"gray\")\n        ax[2].set_title(\"Mask\")\n        ax[2].grid(False)\n        ax[2].axis(\"off\")\n\n    ax[0].imshow(spatial_attributes.hsi.get_rgb_image(output_channel_axis=2).cpu())\n    ax[0].set_title(\"Original image\")\n    ax[0].grid(False)\n    ax[0].axis(\"off\")\n\n    attrs = spatial_attributes.attributes.cpu().numpy()\n    if np.all(attrs == 0):\n        logger.warning(\"All spatial attributes are zero.\")\n        cmap = LinearSegmentedColormap.from_list(\"RdWhGn\", [\"red\", \"white\", \"green\"])\n        heat_map = ax[1].imshow(attrs.sum(axis=-1), cmap=cmap, vmin=-1, vmax=1)\n\n        axis_separator = make_axes_locatable(ax[1])\n        colorbar_axis = axis_separator.append_axes(\"bottom\", size=\"5%\", pad=0.1)\n        fig.colorbar(heat_map, orientation=\"horizontal\", cax=colorbar_axis)\n    else:\n        viz.visualize_image_attr(\n            attrs,\n            method=\"heat_map\",\n            sign=\"all\",\n            plt_fig_axis=(fig, ax[1]),\n            show_colorbar=True,\n            use_pyplot=False,\n        )\n    ax[1].set_title(\"Attribution Map\")\n    ax[1].axis(\"off\")\n\n    if use_pyplot:\n        plt.show()  # pragma: no cover\n        return None  # pragma: no cover\n    else:\n        return fig, ax\n</code></pre>"},{"location":"reference/#attribution-methods","title":"Attribution Methods","text":""},{"location":"reference/#src.meteors.attr.attributes.HSIAttributes","title":"<code>HSIAttributes</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents an object that contains Hyperspectral image attributes and explanations.</p> <p>Attributes:</p> Name Type Description <code>hsi</code> <code>HSI</code> <p>Hyperspectral image object for which the explanations were created.</p> <code>attributes</code> <code>Tensor</code> <p>Attributions (explanations) for the hsi.</p> <code>score</code> <code>float</code> <p>The score provided by the interpretable model. Can be None if method don't provide one.</p> <code>device</code> <code>device</code> <p>Device to be used for inference. If None, the device of the input hsi will be used. Defaults to None.</p> <code>attribution_method</code> <code>str | None</code> <p>The method used to generate the explanation. Defaults to None.</p> Source code in <code>src/meteors/attr/attributes.py</code> <pre><code>class HSIAttributes(BaseModel):\n    \"\"\"Represents an object that contains Hyperspectral image attributes and explanations.\n\n    Attributes:\n        hsi (HSI): Hyperspectral image object for which the explanations were created.\n        attributes (torch.Tensor): Attributions (explanations) for the hsi.\n        score (float): The score provided by the interpretable model. Can be None if method don't provide one.\n        device (torch.device): Device to be used for inference. If None, the device of the input hsi will be used.\n            Defaults to None.\n        attribution_method (str | None): The method used to generate the explanation. Defaults to None.\n    \"\"\"\n\n    hsi: Annotated[\n        HSI,\n        Field(\n            description=\"Hyperspectral image object for which the explanations were created.\",\n        ),\n    ]\n    attributes: Annotated[\n        torch.Tensor,\n        BeforeValidator(validate_and_convert_attributes),\n        Field(\n            description=\"Attributions (explanations) for the hsi.\",\n        ),\n    ]\n    attribution_method: Annotated[\n        str | None,\n        AfterValidator(validate_attribution_method),\n        Field(\n            description=\"The method used to generate the explanation.\",\n        ),\n    ] = None\n    score: Annotated[\n        float | None,\n        Field(\n            validate_default=True,\n            description=\"The score provided by the interpretable model. Can be None if method don't provide one.\",\n        ),\n    ] = None\n    mask: Annotated[\n        torch.Tensor | None,\n        BeforeValidator(validate_and_convert_mask),\n        Field(\n            description=\"`superpixel` or `superband` mask used for the explanation.\",\n        ),\n    ] = None\n    device: Annotated[\n        torch.device,\n        BeforeValidator(resolve_inference_device_attributes),\n        Field(\n            validate_default=True,\n            exclude=True,\n            description=(\n                \"Device to be used for inference. If None, the device of the input hsi will be used. \"\n                \"Defaults to None.\"\n            ),\n        ),\n    ] = None\n\n    @property\n    def flattened_attributes(self) -&gt; torch.Tensor:\n        \"\"\"Returns a flattened tensor of attributes.\n\n        This method should be implemented in the subclass.\n\n        Returns:\n            torch.Tensor: A flattened tensor of attributes.\n        \"\"\"\n        raise NotImplementedError(\"This method should be implemented in the subclass\")\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @property\n    def orientation(self) -&gt; tuple[str, str, str]:\n        \"\"\"Returns the orientation of the hsi.\n\n        Returns:\n            tuple[str, str, str]: The orientation of the hsi corresponding to the attributes.\n        \"\"\"\n        return self.hsi.orientation\n\n    def _validate_hsi_attributions_and_mask(self) -&gt; None:\n        \"\"\"Validates the hsi attributions and performs necessary operations to ensure compatibility with the device.\n\n        Raises:\n            ValueError: If the shapes of the attributes and hsi tensors do not match.\n        \"\"\"\n        validate_shapes(self.attributes, self.hsi)\n\n        self.attributes = self.attributes.to(self.device)\n        if self.device != self.hsi.device:\n            self.hsi.to(self.device)\n\n        if self.mask is not None:\n            validate_shapes(self.mask, self.hsi)\n            self.mask = self.mask.to(self.device)\n\n    @model_validator(mode=\"after\")\n    def validate_hsi_attributions(self) -&gt; Self:\n        \"\"\"Validates the hsi attributions.\n\n        This method performs validation on the hsi attributions to ensure they are correct.\n\n        Returns:\n            Self: The current instance of the class.\n        \"\"\"\n        self._validate_hsi_attributions_and_mask()\n        return self\n\n    def to(self, device: str | torch.device) -&gt; Self:\n        \"\"\"Move the hsi and attributes tensors to the specified device.\n\n        Args:\n            device (str or torch.device): The device to move the tensors to.\n\n        Returns:\n            Self: The modified object with tensors moved to the specified device.\n\n        Examples:\n            &gt;&gt;&gt; attrs = HSIAttributes(hsi, attributes, score=0.5)\n            &gt;&gt;&gt; attrs.to(\"cpu\")\n            &gt;&gt;&gt; attrs.hsi.device\n            device(type='cpu')\n            &gt;&gt;&gt; attrs.attributes.device\n            device(type='cpu')\n            &gt;&gt;&gt; attrs.to(\"cuda\")\n            &gt;&gt;&gt; attrs.hsi.device\n            device(type='cuda')\n            &gt;&gt;&gt; attrs.attributes.device\n            device(type='cuda')\n        \"\"\"\n        self.hsi = self.hsi.to(device)\n        self.attributes = self.attributes.to(device)\n        self.device = self.hsi.device\n        return self\n\n    def change_orientation(self, target_orientation: tuple[str, str, str] | list[str] | str, inplace=False) -&gt; Self:\n        \"\"\"Changes the orientation of the image data along with the attributions to the target orientation.\n\n        Args:\n            target_orientation (tuple[str, str, str] | list[str] | str): The target orientation for the attribution data.\n                This should be a tuple of three one-letter strings in any order: \"C\", \"H\", \"W\".\n            inplace (bool, optional): Whether to modify the data in place or return a new object.\n\n        Returns:\n            Self: The updated Image object with the new orientation.\n\n        Raises:\n            ValueError: If the target orientation is not a valid tuple of three one-letter strings.\n        \"\"\"\n        current_orientation = self.orientation\n        hsi = self.hsi.change_orientation(target_orientation, inplace=inplace)\n        if inplace:\n            attrs = self\n        else:\n            attrs = self.model_copy()\n            attrs.hsi = hsi\n\n        # now change the orientation of the attributes\n        if current_orientation == target_orientation:\n            return attrs\n\n        permute_dims = [current_orientation.index(dim) for dim in target_orientation]\n\n        attrs.attributes = attrs.attributes.permute(permute_dims)\n\n        if attrs.mask is not None:\n            attrs.mask = attrs.mask.permute(permute_dims)\n        return attrs\n</code></pre>"},{"location":"reference/#src.meteors.attr.attributes.HSIAttributes.flattened_attributes","title":"<code>flattened_attributes: torch.Tensor</code>  <code>property</code>","text":"<p>Returns a flattened tensor of attributes.</p> <p>This method should be implemented in the subclass.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A flattened tensor of attributes.</p>"},{"location":"reference/#src.meteors.attr.attributes.HSIAttributes.orientation","title":"<code>orientation: tuple[str, str, str]</code>  <code>property</code>","text":"<p>Returns the orientation of the hsi.</p> <p>Returns:</p> Type Description <code>tuple[str, str, str]</code> <p>tuple[str, str, str]: The orientation of the hsi corresponding to the attributes.</p>"},{"location":"reference/#src.meteors.attr.attributes.HSIAttributes.change_orientation","title":"<code>change_orientation(target_orientation, inplace=False)</code>","text":"<p>Changes the orientation of the image data along with the attributions to the target orientation.</p> <p>Parameters:</p> Name Type Description Default <code>target_orientation</code> <code>tuple[str, str, str] | list[str] | str</code> <p>The target orientation for the attribution data. This should be a tuple of three one-letter strings in any order: \"C\", \"H\", \"W\".</p> required <code>inplace</code> <code>bool</code> <p>Whether to modify the data in place or return a new object.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>The updated Image object with the new orientation.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the target orientation is not a valid tuple of three one-letter strings.</p> Source code in <code>src/meteors/attr/attributes.py</code> <pre><code>def change_orientation(self, target_orientation: tuple[str, str, str] | list[str] | str, inplace=False) -&gt; Self:\n    \"\"\"Changes the orientation of the image data along with the attributions to the target orientation.\n\n    Args:\n        target_orientation (tuple[str, str, str] | list[str] | str): The target orientation for the attribution data.\n            This should be a tuple of three one-letter strings in any order: \"C\", \"H\", \"W\".\n        inplace (bool, optional): Whether to modify the data in place or return a new object.\n\n    Returns:\n        Self: The updated Image object with the new orientation.\n\n    Raises:\n        ValueError: If the target orientation is not a valid tuple of three one-letter strings.\n    \"\"\"\n    current_orientation = self.orientation\n    hsi = self.hsi.change_orientation(target_orientation, inplace=inplace)\n    if inplace:\n        attrs = self\n    else:\n        attrs = self.model_copy()\n        attrs.hsi = hsi\n\n    # now change the orientation of the attributes\n    if current_orientation == target_orientation:\n        return attrs\n\n    permute_dims = [current_orientation.index(dim) for dim in target_orientation]\n\n    attrs.attributes = attrs.attributes.permute(permute_dims)\n\n    if attrs.mask is not None:\n        attrs.mask = attrs.mask.permute(permute_dims)\n    return attrs\n</code></pre>"},{"location":"reference/#src.meteors.attr.attributes.HSIAttributes.to","title":"<code>to(device)</code>","text":"<p>Move the hsi and attributes tensors to the specified device.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str or device</code> <p>The device to move the tensors to.</p> required <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>The modified object with tensors moved to the specified device.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; attrs = HSIAttributes(hsi, attributes, score=0.5)\n&gt;&gt;&gt; attrs.to(\"cpu\")\n&gt;&gt;&gt; attrs.hsi.device\ndevice(type='cpu')\n&gt;&gt;&gt; attrs.attributes.device\ndevice(type='cpu')\n&gt;&gt;&gt; attrs.to(\"cuda\")\n&gt;&gt;&gt; attrs.hsi.device\ndevice(type='cuda')\n&gt;&gt;&gt; attrs.attributes.device\ndevice(type='cuda')\n</code></pre> Source code in <code>src/meteors/attr/attributes.py</code> <pre><code>def to(self, device: str | torch.device) -&gt; Self:\n    \"\"\"Move the hsi and attributes tensors to the specified device.\n\n    Args:\n        device (str or torch.device): The device to move the tensors to.\n\n    Returns:\n        Self: The modified object with tensors moved to the specified device.\n\n    Examples:\n        &gt;&gt;&gt; attrs = HSIAttributes(hsi, attributes, score=0.5)\n        &gt;&gt;&gt; attrs.to(\"cpu\")\n        &gt;&gt;&gt; attrs.hsi.device\n        device(type='cpu')\n        &gt;&gt;&gt; attrs.attributes.device\n        device(type='cpu')\n        &gt;&gt;&gt; attrs.to(\"cuda\")\n        &gt;&gt;&gt; attrs.hsi.device\n        device(type='cuda')\n        &gt;&gt;&gt; attrs.attributes.device\n        device(type='cuda')\n    \"\"\"\n    self.hsi = self.hsi.to(device)\n    self.attributes = self.attributes.to(device)\n    self.device = self.hsi.device\n    return self\n</code></pre>"},{"location":"reference/#src.meteors.attr.attributes.HSISpatialAttributes","title":"<code>HSISpatialAttributes</code>","text":"<p>               Bases: <code>HSIAttributes</code></p> <p>Represents spatial attributes of an hsi used for explanation.</p> <p>Attributes:</p> Name Type Description <code>hsi</code> <code>HSI</code> <p>Hyperspectral image object for which the explanations were created.</p> <code>attributes</code> <code>Tensor</code> <p>Attributions (explanations) for the hsi.</p> <code>score</code> <code>float</code> <p>The score provided by the interpretable model. Can be None if method don't provide one.</p> <code>device</code> <code>device</code> <p>Device to be used for inference. If None, the device of the input hsi will be used. Defaults to None.</p> <code>attribution_method</code> <code>str | None</code> <p>The method used to generate the explanation. Defaults to None.</p> <code>segmentation_mask</code> <code>Tensor</code> <p>Spatial (Segmentation) mask used for the explanation.</p> <code>flattened_attributes</code> <code>Tensor</code> <p>Spatial 2D attribution map.</p> Source code in <code>src/meteors/attr/attributes.py</code> <pre><code>class HSISpatialAttributes(HSIAttributes):\n    \"\"\"Represents spatial attributes of an hsi used for explanation.\n\n    Attributes:\n        hsi (HSI): Hyperspectral image object for which the explanations were created.\n        attributes (torch.Tensor): Attributions (explanations) for the hsi.\n        score (float): The score provided by the interpretable model. Can be None if method don't provide one.\n        device (torch.device): Device to be used for inference. If None, the device of the input hsi will be used.\n            Defaults to None.\n        attribution_method (str | None): The method used to generate the explanation. Defaults to None.\n        segmentation_mask (torch.Tensor): Spatial (Segmentation) mask used for the explanation.\n        flattened_attributes (torch.Tensor): Spatial 2D attribution map.\n    \"\"\"\n\n    @property\n    def segmentation_mask(self) -&gt; torch.Tensor:\n        \"\"\"Returns the 2D spatial segmentation mask that has the same size as the hsi image.\n\n        Returns:\n            torch.Tensor: The segmentation mask tensor.\n        \"\"\"\n        if self.mask is None:\n            raise ValueError(\"Segmentation mask is not provided\")\n        return self.mask.select(dim=self.hsi.spectral_axis, index=0)\n\n    @property\n    def flattened_attributes(self) -&gt; torch.Tensor:\n        \"\"\"Returns a flattened tensor of attributes, with removed repeated dimensions.\n\n        In the case of spatial attributes, the flattened attributes are 2D spatial attributes of shape (rows, columns) and the spectral dimension is removed.\n\n        Examples:\n            &gt;&gt;&gt; segmentation_mask = torch.zeros((3, 2, 2))\n            &gt;&gt;&gt; attrs = HSISpatialAttributes(hsi, attributes, score=0.5, segmentation_mask=segmentation_mask)\n            &gt;&gt;&gt; attrs.flattened_attributes\n                tensor([[0., 0.],\n                        [0., 0.]])\n\n        Returns:\n            torch.Tensor: A flattened tensor of attributes.\n        \"\"\"\n        return self.attributes.select(dim=self.hsi.spectral_axis, index=0)\n\n    def _validate_hsi_attributions_and_mask(self) -&gt; None:\n        \"\"\"Validates the hsi attributions and performs necessary operations to ensure compatibility with the device.\n\n        Raises:\n            ValueError: If the shapes of the attributes and hsi tensors do not match.\n            ValueError: If the segmentation mask is not provided.\n        \"\"\"\n        super()._validate_hsi_attributions_and_mask()\n        if self.mask is None:\n            raise ValueError(\"Segmentation mask is not provided\")\n</code></pre>"},{"location":"reference/#src.meteors.attr.attributes.HSISpatialAttributes.flattened_attributes","title":"<code>flattened_attributes: torch.Tensor</code>  <code>property</code>","text":"<p>Returns a flattened tensor of attributes, with removed repeated dimensions.</p> <p>In the case of spatial attributes, the flattened attributes are 2D spatial attributes of shape (rows, columns) and the spectral dimension is removed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; segmentation_mask = torch.zeros((3, 2, 2))\n&gt;&gt;&gt; attrs = HSISpatialAttributes(hsi, attributes, score=0.5, segmentation_mask=segmentation_mask)\n&gt;&gt;&gt; attrs.flattened_attributes\n    tensor([[0., 0.],\n            [0., 0.]])\n</code></pre> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A flattened tensor of attributes.</p>"},{"location":"reference/#src.meteors.attr.attributes.HSISpatialAttributes.segmentation_mask","title":"<code>segmentation_mask: torch.Tensor</code>  <code>property</code>","text":"<p>Returns the 2D spatial segmentation mask that has the same size as the hsi image.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The segmentation mask tensor.</p>"},{"location":"reference/#src.meteors.attr.attributes.HSISpectralAttributes","title":"<code>HSISpectralAttributes</code>","text":"<p>               Bases: <code>HSIAttributes</code></p> <p>Represents an hsi with spectral attributes used for explanation.</p> <p>Attributes:</p> Name Type Description <code>hsi</code> <code>HSI</code> <p>Hyperspectral hsi object for which the explanations were created.</p> <code>attributes</code> <code>Tensor</code> <p>Attributions (explanations) for the hsi.</p> <code>score</code> <code>float</code> <p>R^2 score of interpretable model used for the explanation.</p> <code>device</code> <code>device</code> <p>Device to be used for inference. If None, the device of the input hsi will be used. Defaults to None.</p> <code>attribution_method</code> <code>str | None</code> <p>The method used to generate the explanation. Defaults to None.</p> <code>band_mask</code> <code>Tensor</code> <p>Band mask used for the explanation.</p> <code>band_names</code> <code>dict[str | tuple[str, ...], int]</code> <p>Dictionary that translates the band names into the band segment ids.</p> <code>flattened_attributes</code> <code>Tensor</code> <p>Spectral 1D attribution map.</p> Source code in <code>src/meteors/attr/attributes.py</code> <pre><code>class HSISpectralAttributes(HSIAttributes):\n    \"\"\"Represents an hsi with spectral attributes used for explanation.\n\n    Attributes:\n        hsi (HSI): Hyperspectral hsi object for which the explanations were created.\n        attributes (torch.Tensor): Attributions (explanations) for the hsi.\n        score (float): R^2 score of interpretable model used for the explanation.\n        device (torch.device): Device to be used for inference. If None, the device of the input hsi will be used.\n            Defaults to None.\n        attribution_method (str | None): The method used to generate the explanation. Defaults to None.\n        band_mask (torch.Tensor): Band mask used for the explanation.\n        band_names (dict[str | tuple[str, ...], int]): Dictionary that translates the band names into the band segment ids.\n        flattened_attributes (torch.Tensor): Spectral 1D attribution map.\n    \"\"\"\n\n    band_names: Annotated[\n        dict[str | tuple[str, ...], int],\n        Field(\n            description=\"Dictionary that translates the band names into the band segment ids.\",\n        ),\n    ]\n\n    @property\n    def band_mask(self) -&gt; torch.Tensor:\n        \"\"\"Returns a 1D band mask - a band mask with removed repeated dimensions (num_bands, ),\n        where num_bands is the number of bands in the hsi image.\n\n        The method selects the appropriate dimensions from the `band_mask` tensor\n        based on the `axis_to_select` and returns a flattened version of the selected\n        tensor.\n\n        Returns:\n            torch.Tensor: The flattened band mask tensor.\n\n        Examples:\n            &gt;&gt;&gt; band_names = {\"R\": 0, \"G\": 1, \"B\": 2}\n            &gt;&gt;&gt; attrs = HSISpectralAttributes(hsi, attributes, score=0.5, mask=band_mask)\n            &gt;&gt;&gt; attrs.flattened_band_mask\n            torch.tensor([0, 1, 2])\n        \"\"\"\n        if self.mask is None:\n            raise ValueError(\"Band mask is not provided\")\n        axis_to_select = [i for i in range(self.hsi.image.ndim) if i != self.hsi.spectral_axis]\n        return self.mask.select(dim=axis_to_select[0], index=0).select(dim=axis_to_select[1] - 1, index=0)\n\n    @property\n    def flattened_attributes(self) -&gt; torch.Tensor:\n        \"\"\"Returns a flattened tensor of attributes with removed repeated dimensions.\n\n        In the case of spectral attributes, the flattened attributes are 1D tensor of shape (num_bands, ), where num_bands is the number of bands in the hsi image.\n\n        Returns:\n            torch.Tensor: A flattened tensor of attributes.\n        \"\"\"\n        axis = [i for i in range(self.attributes.ndim) if i != self.hsi.spectral_axis]\n        return self.attributes.select(dim=axis[0], index=0).select(dim=axis[1] - 1, index=0)\n\n    def _validate_hsi_attributions_and_mask(self) -&gt; None:\n        \"\"\"Validates the hsi attributions and performs necessary operations to ensure compatibility with the device.\n\n        Raises:\n            ValueError: If the shapes of the attributes and hsi tensors do not match.\n            ValueError: If the band mask is not provided.\n        \"\"\"\n        super()._validate_hsi_attributions_and_mask()\n        if self.mask is None:\n            raise ValueError(\"Band mask is not provided\")\n\n        self.band_names = align_band_names_with_mask(self.band_names, self.mask)\n</code></pre>"},{"location":"reference/#src.meteors.attr.attributes.HSISpectralAttributes.band_mask","title":"<code>band_mask: torch.Tensor</code>  <code>property</code>","text":"<p>Returns a 1D band mask - a band mask with removed repeated dimensions (num_bands, ), where num_bands is the number of bands in the hsi image.</p> <p>The method selects the appropriate dimensions from the <code>band_mask</code> tensor based on the <code>axis_to_select</code> and returns a flattened version of the selected tensor.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The flattened band mask tensor.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; band_names = {\"R\": 0, \"G\": 1, \"B\": 2}\n&gt;&gt;&gt; attrs = HSISpectralAttributes(hsi, attributes, score=0.5, mask=band_mask)\n&gt;&gt;&gt; attrs.flattened_band_mask\ntorch.tensor([0, 1, 2])\n</code></pre>"},{"location":"reference/#src.meteors.attr.attributes.HSISpectralAttributes.flattened_attributes","title":"<code>flattened_attributes: torch.Tensor</code>  <code>property</code>","text":"<p>Returns a flattened tensor of attributes with removed repeated dimensions.</p> <p>In the case of spectral attributes, the flattened attributes are 1D tensor of shape (num_bands, ), where num_bands is the number of bands in the hsi image.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A flattened tensor of attributes.</p>"},{"location":"reference/#src.meteors.attr.lime.Lime","title":"<code>Lime</code>","text":"<p>               Bases: <code>Explainer</code></p> <p>Lime class is a subclass of Explainer and represents the Lime explainer. Lime is an interpretable model-agnostic explanation method that explains the predictions of a black-box model by approximating it with a simpler interpretable model. The Lime method is based on the <code>captum</code> implementation and is an implementation of an idea coming from the original paper on Lime, where more details about this method can be found.</p> <p>Parameters:</p> Name Type Description Default <code>explainable_model</code> <code>ExplainableModel</code> <p>The explainable model to be explained.</p> required <code>interpretable_model</code> <code>InterpretableModel</code> <p>The interpretable model used to approximate the black-box model. Defaults to <code>SkLearnLasso</code> with alpha parameter set to 0.08.</p> <code>SkLearnLasso(alpha=0.08)</code> <code>similarity_func</code> <code>Callable[[Tensor], Tensor] | None</code> <p>The similarity function used by Lime. Defaults to None.</p> <code>None</code> <code>perturb_func</code> <code>Callable[[Tensor], Tensor] | None</code> <p>The perturbation function used by Lime. Defaults to None.</p> <code>None</code> Source code in <code>src/meteors/attr/lime.py</code> <pre><code>class Lime(Explainer):\n    \"\"\"Lime class is a subclass of Explainer and represents the Lime explainer. Lime is an interpretable model-agnostic\n    explanation method that explains the predictions of a black-box model by approximating it with a simpler\n    interpretable model. The Lime method is based on the [`captum` implementation](https://captum.ai/api/lime.html)\n    and is an implementation of an idea coming from the [original paper on Lime](https://arxiv.org/abs/1602.04938),\n    where more details about this method can be found.\n\n    Args:\n        explainable_model (ExplainableModel): The explainable model to be explained.\n        interpretable_model (InterpretableModel): The interpretable model used to approximate the black-box model.\n            Defaults to `SkLearnLasso` with alpha parameter set to 0.08.\n        similarity_func (Callable[[torch.Tensor], torch.Tensor] | None, optional): The similarity function used by Lime.\n            Defaults to None.\n        perturb_func (Callable[[torch.Tensor], torch.Tensor] | None, optional): The perturbation function used by Lime.\n            Defaults to None.\n    \"\"\"\n\n    def __init__(\n        self,\n        explainable_model: ExplainableModel,\n        interpretable_model: InterpretableModel = SkLearnLasso(alpha=0.08),\n        similarity_func: Callable[[torch.Tensor], torch.Tensor] | None = None,\n        perturb_func: Callable[[torch.Tensor], torch.Tensor] | None = None,\n    ):\n        super().__init__(explainable_model)\n        self.interpretable_model = interpretable_model\n        self._attribution_method: LimeBase = self._construct_lime(\n            self.explainable_model.forward_func, interpretable_model, similarity_func, perturb_func\n        )\n\n    @staticmethod\n    def _construct_lime(\n        forward_func: Callable[[torch.Tensor], torch.Tensor],\n        interpretable_model: InterpretableModel,\n        similarity_func: Callable | None,\n        perturb_func: Callable[[torch.Tensor], torch.Tensor] | None,\n    ) -&gt; LimeBase:\n        \"\"\"Constructs the LimeBase object.\n\n        Args:\n            forward_func (Callable[[torch.Tensor], torch.Tensor]): The forward function of the explainable model.\n            interpretable_model (InterpretableModel): The interpretable model used to approximate the black-box model.\n            similarity_func (Callable | None): The similarity function used by Lime.\n            perturb_func (Callable[[torch.Tensor], torch.Tensor] | None): The perturbation function used by Lime.\n\n        Returns:\n            LimeBase: The constructed LimeBase object.\n        \"\"\"\n        return LimeBase(\n            forward_func=forward_func,\n            interpretable_model=interpretable_model,\n            similarity_func=similarity_func,\n            perturb_func=perturb_func,\n        )\n\n    @staticmethod\n    def get_segmentation_mask(\n        hsi: HSI,\n        segmentation_method: Literal[\"patch\", \"slic\"] = \"slic\",\n        **segmentation_method_params: Any,\n    ) -&gt; torch.Tensor:\n        \"\"\"Generates a segmentation mask for the given hsi using the specified segmentation method.\n\n        Args:\n            hsi (HSI): The input hyperspectral image for which the segmentation mask needs to be generated.\n            segmentation_method (Literal[\"patch\", \"slic\"], optional): The segmentation method to be used.\n                Defaults to \"slic\".\n            **segmentation_method_params (Any): Additional parameters specific to the chosen segmentation method.\n\n        Returns:\n            torch.Tensor: The segmentation mask as a tensor.\n\n        Raises:\n            ValueError: If the input hsi is not an instance of the HSI class.\n            ValueError: If an unsupported segmentation method is specified.\n\n        Examples:\n            &gt;&gt;&gt; hsi = meteors.HSI(image=torch.ones((3, 240, 240)), wavelengths=[462.08, 465.27, 468.47])\n            &gt;&gt;&gt; segmentation_mask = mt_lime.Lime.get_segmentation_mask(hsi, segmentation_method=\"slic\")\n            &gt;&gt;&gt; segmentation_mask.shape\n            torch.Size([1, 240, 240])\n            &gt;&gt;&gt; segmentation_mask = meteors.attr.Lime.get_segmentation_mask(hsi, segmentation_method=\"patch\", patch_size=2)\n            &gt;&gt;&gt; segmentation_mask.shape\n            torch.Size([1, 240, 240])\n            &gt;&gt;&gt; segmentation_mask[0, :2, :2]\n            torch.tensor([[1, 1],\n                          [1, 1]])\n            &gt;&gt;&gt; segmentation_mask[0, 2:4, :2]\n            torch.tensor([[2, 2],\n                          [2, 2]])\n        \"\"\"\n        if not isinstance(hsi, HSI):\n            raise ValueError(\"hsi should be an instance of HSI class\")\n\n        if segmentation_method == \"slic\":\n            return Lime._get_slick_segmentation_mask(hsi, **segmentation_method_params)\n        elif segmentation_method == \"patch\":\n            return Lime._get_patch_segmentation_mask(hsi, **segmentation_method_params)\n        else:\n            raise ValueError(f\"Unsupported segmentation method: {segmentation_method}\")\n\n    @staticmethod\n    def get_band_mask(\n        hsi: HSI,\n        band_names: None | list[str | list[str]] | dict[tuple[str, ...] | str, int] = None,\n        band_indices: None | dict[str | tuple[str, ...], ListOfWavelengthsIndices] = None,\n        band_wavelengths: None | dict[str | tuple[str, ...], ListOfWavelengths] = None,\n        device: str | torch.device | None = None,\n        repeat_dimensions: bool = False,\n    ) -&gt; tuple[torch.Tensor, dict[tuple[str, ...] | str, int]]:\n        \"\"\"Generates a band mask based on the provided hsi and band information.\n\n        Remember you need to provide either band_names, band_indices, or band_wavelengths to create the band mask.\n        If you provide more than one, the band mask will be created using only one using the following priority:\n        band_names &gt; band_wavelengths &gt; band_indices.\n\n        Args:\n            hsi (HSI): The input hyperspectral image.\n            band_names (None | list[str | list[str]] | dict[tuple[str, ...] | str, int], optional):\n                The names of the spectral bands to include in the mask. Defaults to None.\n            band_indices (None | dict[str | tuple[str, ...], list[tuple[int, int]] | tuple[int, int] | list[int]], optional):\n                The indices or ranges of indices of the spectral bands to include in the mask. Defaults to None.\n            band_wavelengths (None | dict[str | tuple[str, ...], list[tuple[float, float]] | tuple[float, float], list[float], float], optional):\n                The wavelengths or ranges of wavelengths of the spectral bands to include in the mask. Defaults to None.\n            device (str | torch.device | None, optional):\n                The device to use for computation. Defaults to None.\n            repeat_dimensions (bool, optional):\n                Whether to repeat the dimensions of the mask to match the input hsi shape. Defaults to False.\n\n        Returns:\n            tuple[torch.Tensor, dict[tuple[str, ...] | str, int]]: A tuple containing the band mask tensor and a dictionary\n            mapping band names to segment IDs.\n\n        Raises:\n            ValueError: If the input hsi is not an instance of the HSI class.\n            ValueError: If no band names, indices, or wavelengths are provided.\n\n        Examples:\n            &gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((len(wavelengths), 10, 10)), wavelengths=wavelengths)\n            &gt;&gt;&gt; band_names = [\"R\", \"G\"]\n            &gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_names=band_names)\n            &gt;&gt;&gt; dict_labels_to_segment_ids\n            {\"R\": 1, \"G\": 2}\n            &gt;&gt;&gt; band_indices = {\"RGB\": [0, 1, 2]}\n            &gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_indices=band_indices)\n            &gt;&gt;&gt; dict_labels_to_segment_ids\n            {\"RGB\": 1}\n            &gt;&gt;&gt; band_wavelengths = {\"RGB\": [(462.08, 465.27), (465.27, 468.47), (468.47, 471.68)]}\n            &gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_wavelengths=band_wavelengths)\n            &gt;&gt;&gt; dict_labels_to_segment_ids\n            {\"RGB\": 1}\n        \"\"\"\n        if not isinstance(hsi, HSI):\n            raise ValueError(\"hsi should be an instance of HSI class\")\n\n        assert (\n            band_names is not None or band_indices is not None or band_wavelengths is not None\n        ), \"No band names, indices, or wavelengths are provided.\"\n\n        # validate types\n        dict_labels_to_segment_ids = None\n        if band_names is not None:\n            logger.debug(\"Getting band mask from band names of spectral bands\")\n            if band_wavelengths is not None or band_indices is not None:\n                ignored_params = [\n                    param\n                    for param in [\"band_wavelengths\", \"band_indices\"]\n                    if param in locals() and locals()[param] is not None\n                ]\n                ignored_params_str = \" and \".join(ignored_params)\n                logger.info(\n                    f\"Only the band names will be used to create the band mask. The additional parameters {ignored_params_str} will be ignored.\"\n                )\n            try:\n                validate_band_names(band_names)\n                band_groups, dict_labels_to_segment_ids = Lime._get_band_wavelengths_indices_from_band_names(\n                    hsi.wavelengths, band_names\n                )\n            except Exception as e:\n                raise ValueError(f\"Incorrect band names provided: {e}\") from e\n        elif band_wavelengths is not None:\n            logger.debug(\"Getting band mask from band groups given by ranges of wavelengths\")\n            if band_indices is not None:\n                logger.info(\n                    \"Only the band wavelengths will be used to create the band mask. The band_indices will be ignored.\"\n                )\n            validate_band_format(band_wavelengths, variable_name=\"band_wavelengths\")\n            try:\n                band_groups = Lime._get_band_indices_from_band_wavelengths(\n                    hsi.wavelengths,\n                    band_wavelengths,\n                )\n            except Exception as e:\n                raise ValueError(\n                    f\"Incorrect band ranges wavelengths provided, please check if provided wavelengths are correct: {e}\"\n                ) from e\n        elif band_indices is not None:\n            logger.debug(\"Getting band mask from band groups given by ranges of indices\")\n            validate_band_format(band_indices, variable_name=\"band_indices\")\n            try:\n                band_groups = Lime._get_band_indices_from_input_band_indices(hsi.wavelengths, band_indices)\n            except Exception as e:\n                raise ValueError(\n                    f\"Incorrect band ranges indices provided, please check if provided indices are correct: {e}\"\n                ) from e\n\n        return Lime._create_tensor_band_mask(\n            hsi,\n            band_groups,\n            dict_labels_to_segment_ids=dict_labels_to_segment_ids,\n            device=device,\n            repeat_dimensions=repeat_dimensions,\n            return_dict_labels_to_segment_ids=True,\n        )\n\n    @staticmethod\n    def _make_band_names_indexable(segment_name: list[str] | tuple[str, ...] | str) -&gt; tuple[str, ...] | str:\n        \"\"\"Converts a list of strings into a tuple of strings if necessary to make it indexable.\n\n        Args:\n            segment_name (list[str] | tuple[str, ...] | str): The segment name to be converted.\n\n        Returns:\n            tuple[str, ...] | str: The converted segment name.\n\n        Raises:\n            ValueError: If the segment_name is not of type list or string.\n        \"\"\"\n        if (\n            isinstance(segment_name, tuple) and all(isinstance(subitem, str) for subitem in segment_name)\n        ) or isinstance(segment_name, str):\n            return segment_name\n        elif isinstance(segment_name, list) and all(isinstance(subitem, str) for subitem in segment_name):\n            return tuple(segment_name)\n        raise ValueError(f\"Incorrect segment {segment_name} type. Should be either a list or string\")\n\n    @staticmethod\n    # @lru_cache(maxsize=32) Can't use with lists as they are not hashable\n    def _extract_bands_from_spyndex(segment_name: list[str] | tuple[str, ...] | str) -&gt; tuple[str, ...] | str:\n        \"\"\"Extracts bands from the given segment name.\n\n        Args:\n            segment_name (list[str] | tuple[str, ...] | str): The name of the segment.\n                Users may pass either band names or indices names, as in the spyndex library.\n\n        Returns:\n            tuple[str, ...] | str: A tuple of band names if multiple bands are extracted,\n                or a single band name if only one band is extracted.\n\n        Raises:\n            ValueError: If the provided band name is invalid.\n                The band name must be either in `spyndex.indices` or `spyndex.bands`.\n        \"\"\"\n        if isinstance(segment_name, str):\n            segment_name = (segment_name,)\n        elif isinstance(segment_name, list):\n            segment_name = tuple(segment_name)\n\n        band_names_segment: list[str] = []\n        for band_name in segment_name:\n            if band_name in spyndex.indices:\n                band_names_segment += list(spyndex.indices[band_name].bands)\n            elif band_name in spyndex.bands:\n                band_names_segment.append(band_name)\n            else:\n                raise ValueError(\n                    f\"Invalid band name {band_name}, band name must be either in `spyndex.indices` or `spyndex.bands`\"\n                )\n\n        return tuple(set(band_names_segment)) if len(band_names_segment) &gt; 1 else band_names_segment[0]\n\n    @staticmethod\n    def _get_indices_from_wavelength_indices_range(\n        wavelengths: torch.Tensor, ranges: list[tuple[int, int]] | tuple[int, int]\n    ) -&gt; list[int]:\n        \"\"\"Converts wavelength indices ranges to list indices.\n\n        Args:\n            wavelengths (torch.Tensor): The tensor containing the wavelengths.\n            ranges (list[tuple[int, int]] | tuple[int, int]): The wavelength indices ranges.\n\n        Returns:\n            list[int]: The indices of bands corresponding to the wavelength indices ranges.\n        \"\"\"\n        validated_ranges_list = validate_segment_format(ranges)\n        validated_ranges_list = adjust_and_validate_segment_ranges(wavelengths, validated_ranges_list)\n\n        return list(\n            set(\n                chain.from_iterable(\n                    [list(range(int(validated_range[0]), int(validated_range[1]))) for validated_range in ranges]  # type: ignore\n                )\n            )\n        )\n\n    @staticmethod\n    def _get_band_wavelengths_indices_from_band_names(\n        wavelengths: torch.Tensor,\n        band_names: list[str | list[str]] | dict[tuple[str, ...] | str, int],\n    ) -&gt; tuple[dict[tuple[str, ...] | str, list[int]], dict[tuple[str, ...] | str, int]]:\n        \"\"\"Extracts band wavelengths indices from the given band names.\n\n        This function takes a list or dictionary of band names or segments and extracts the list of wavelengths indices\n        associated with each segment. It returns a tuple containing a dictionary with mapping segment labels into\n        wavelength indices and a dictionary mapping segment labels into segment ids.\n\n        Args:\n            wavelengths (torch.Tensor): The tensor containing the wavelengths.\n            band_names (list[str | list[str]] | dict[tuple[str, ...] | str, int]):\n                A list or dictionary with band names or segments.\n\n        Returns:\n            tuple[dict[tuple[str, ...] | str, list[int]], dict[tuple[str, ...] | str, int]]:\n                A tuple containing the dictionary with mapping segment labels into wavelength indices and the mapping\n                from segment labels into segment ids.\n        \"\"\"\n        if isinstance(band_names, str):\n            band_names = [band_names]\n        if isinstance(band_names, list):\n            logger.debug(\"band_names is a list of segments, creating a dictionary of segments\")\n            band_names_hashed = [Lime._make_band_names_indexable(segment) for segment in band_names]\n            dict_labels_to_segment_ids = {segment: idx + 1 for idx, segment in enumerate(band_names_hashed)}\n            segments_list = band_names_hashed\n        elif isinstance(band_names, dict):\n            dict_labels_to_segment_ids = band_names.copy()\n            segments_list = tuple(band_names.keys())  # type: ignore\n        else:\n            raise ValueError(\"Incorrect band_names type. It should be a dict or a list\")\n        segments_list_after_mapping = [Lime._extract_bands_from_spyndex(segment) for segment in segments_list]\n        band_indices: dict[tuple[str, ...] | str, list[int]] = {}\n        for original_segment, segment in zip(segments_list, segments_list_after_mapping):\n            segment_indices_ranges: list[tuple[int, int]] = []\n            if isinstance(segment, str):\n                segment = (segment,)\n            for band_name in segment:\n                min_wavelength = spyndex.bands[band_name].min_wavelength\n                max_wavelength = spyndex.bands[band_name].max_wavelength\n\n                if min_wavelength &gt; wavelengths.max() or max_wavelength &lt; wavelengths.min():\n                    logger.warning(\n                        f\"Band {band_name} is not present in the given wavelengths. \"\n                        f\"Band ranges from {min_wavelength} nm to {max_wavelength} nm and the HSI wavelengths \"\n                        f\"range from {wavelengths.min():.2f} nm to {wavelengths.max():.2f} nm. The given band will be skipped\"\n                    )\n                else:\n                    segment_indices_ranges += Lime._convert_wavelengths_to_indices(\n                        wavelengths,\n                        (spyndex.bands[band_name].min_wavelength, spyndex.bands[band_name].max_wavelength),\n                    )\n\n            segment_list = Lime._get_indices_from_wavelength_indices_range(wavelengths, segment_indices_ranges)\n            band_indices[original_segment] = segment_list\n        return band_indices, dict_labels_to_segment_ids\n\n    @staticmethod\n    def _convert_wavelengths_to_indices(\n        wavelengths: torch.Tensor, ranges: list[tuple[float, float]] | tuple[float, float]\n    ) -&gt; list[tuple[int, int]]:\n        \"\"\"Converts wavelength ranges to index ranges.\n\n        Args:\n            wavelengths (torch.Tensor): The tensor containing the wavelengths.\n            ranges (list[tuple[float, float]] | tuple[float, float]): The wavelength ranges.\n\n        Returns:\n            list[tuple[int, int]]: The index ranges corresponding to the wavelength ranges.\n        \"\"\"\n        indices = []\n        if isinstance(ranges, tuple):\n            ranges = [ranges]\n\n        for start, end in ranges:\n            start_idx = torch.searchsorted(wavelengths, start, side=\"left\")\n            end_idx = torch.searchsorted(wavelengths, end, side=\"right\")\n            indices.append((start_idx.item(), end_idx.item()))\n        return indices\n\n    @staticmethod\n    def _get_band_indices_from_band_wavelengths(\n        wavelengths: torch.Tensor,\n        band_wavelengths: dict[str | tuple[str, ...], ListOfWavelengths],\n    ) -&gt; dict[str | tuple[str, ...], list[int]]:\n        \"\"\"Converts the ranges or list of wavelengths into indices.\n\n        Args:\n            wavelengths (torch.Tensor): The tensor containing the wavelengths.\n            band_wavelengths (dict): A dictionary mapping segment labels to wavelength list or ranges.\n\n        Returns:\n            dict: A dictionary mapping segment labels to index ranges.\n\n        Raises:\n            ValueError: If band_wavelengths is not a dictionary.\n        \"\"\"\n        if not isinstance(band_wavelengths, dict):\n            raise ValueError(\"band_wavelengths should be a dictionary\")\n\n        band_indices: dict[str | tuple[str, ...], list[int]] = {}\n        for segment_label, segment in band_wavelengths.items():\n            try:\n                dtype = torch_dtype_to_python_dtype(wavelengths.dtype)\n                if isinstance(segment, (float, int)):\n                    segment = [dtype(segment)]  # type: ignore\n                if isinstance(segment, list) and all(isinstance(x, (float, int)) for x in segment):\n                    segment_dtype = change_dtype_of_list(segment, dtype)\n                    indices = Lime._convert_wavelengths_list_to_indices(wavelengths, segment_dtype)  # type: ignore\n                else:\n                    if isinstance(segment, list):\n                        segment_dtype = [\n                            tuple(change_dtype_of_list(list(ranges), dtype))  # type: ignore\n                            for ranges in segment\n                        ]\n                    else:\n                        segment_dtype = tuple(change_dtype_of_list(segment, dtype))\n\n                    valid_segment_range = validate_segment_format(segment_dtype, dtype)\n                    range_indices = Lime._convert_wavelengths_to_indices(wavelengths, valid_segment_range)  # type: ignore\n                    valid_indices_format = validate_segment_format(range_indices)\n                    valid_range_indices = adjust_and_validate_segment_ranges(wavelengths, valid_indices_format)\n                    indices = Lime._get_indices_from_wavelength_indices_range(wavelengths, valid_range_indices)\n            except Exception as e:\n                raise ValueError(f\"Problem with segment {segment_label}: {e}\") from e\n\n            band_indices[segment_label] = indices\n\n        return band_indices\n\n    @staticmethod\n    def _convert_wavelengths_list_to_indices(wavelengths: torch.Tensor, ranges: list[float]) -&gt; list[int]:\n        \"\"\"Converts a list of wavelengths into indices.\n\n        Args:\n            wavelengths (torch.Tensor): The tensor containing the wavelengths.\n            ranges (list[float]): The list of wavelengths.\n\n        Returns:\n            list[int]: The indices corresponding to the wavelengths.\n        \"\"\"\n        indices = []\n        for wavelength in ranges:\n            index = (wavelengths == wavelength).nonzero(as_tuple=False)\n            number_of_elements = torch.numel(index)\n            if number_of_elements == 1:\n                indices.append(index.item())\n            elif number_of_elements == 0:\n                raise ValueError(f\"Couldn't find wavelength of value {wavelength} in list of wavelength\")\n            else:\n                raise ValueError(f\"Wavelength of value {wavelength} was present more than once in list of wavelength\")\n        return indices\n\n    @staticmethod\n    def _get_band_indices_from_input_band_indices(\n        wavelengths: torch.Tensor,\n        input_band_indices: dict[str | tuple[str, ...], ListOfWavelengthsIndices],\n    ) -&gt; dict[str | tuple[str, ...], list[int]]:\n        \"\"\"Get band indices from band list or ranges indices.\n\n        Args:\n            wavelengths (torch.Tensor): The tensor containing the wavelengths.\n            band_indices (dict[str | tuple[str, ...], ListOfWavelengthsIndices]):\n                A dictionary mapping segment labels to a list of wavelength indices.\n\n        Returns:\n            dict[str | tuple[str, ...], list[int]]: A dictionary mapping segment labels to a list of band indices.\n\n        Raises:\n            ValueError: If `band_indices` is not a dictionary.\n        \"\"\"\n        if not isinstance(input_band_indices, dict):\n            raise ValueError(\"band_indices should be a dictionary\")\n\n        band_indices: dict[str | tuple[str, ...], list[int]] = {}\n        for segment_label, indices in input_band_indices.items():\n            try:\n                if isinstance(indices, int):\n                    indices = [indices]  # type: ignore\n                if isinstance(indices, list) and all(isinstance(x, int) for x in indices):\n                    indices: list[int] = indices  # type: ignore\n                else:\n                    valid_indices_format = validate_segment_format(indices)  # type: ignore\n                    valid_range_indices = adjust_and_validate_segment_ranges(wavelengths, valid_indices_format)\n                    indices = Lime._get_indices_from_wavelength_indices_range(wavelengths, valid_range_indices)  # type: ignore\n\n                band_indices[segment_label] = indices  # type: ignore\n            except Exception as e:\n                raise ValueError(f\"Problem with segment {segment_label}\") from e\n\n        return band_indices\n\n    @staticmethod\n    def _check_overlapping_segments(hsi: HSI, dict_labels_to_indices: dict[str | tuple[str, ...], list[int]]) -&gt; None:\n        \"\"\"Check for overlapping segments in the given hsi.\n\n        Args:\n            hsi (HSI): The hsi object containing the wavelengths.\n            dict_labels_to_indices (dict[str | tuple[str, ...], list[int]]):\n                A dictionary mapping segment labels to indices.\n\n        Returns:\n            None\n        \"\"\"\n        overlapping_segments: dict[int, str | tuple[str, ...]] = {}\n        for segment_label, indices in dict_labels_to_indices.items():\n            for idx in indices:\n                if hsi.wavelengths[idx].item() in overlapping_segments.keys():\n                    logger.warning(\n                        (\n                            f\"Segments {overlapping_segments[hsi.wavelengths[idx].item()]} \"\n                            f\"and {segment_label} are overlapping on wavelength {hsi.wavelengths[idx].item()}\"\n                        )\n                    )\n                overlapping_segments[hsi.wavelengths[idx].item()] = segment_label\n\n    @staticmethod\n    def _validate_and_create_dict_labels_to_segment_ids(\n        dict_labels_to_segment_ids: dict[str | tuple[str, ...], int] | None,\n        segment_labels: list[str | tuple[str, ...]],\n    ) -&gt; dict[str | tuple[str, ...], int]:\n        \"\"\"Validates and creates a dictionary mapping segment labels to segment IDs.\n\n        Args:\n            dict_labels_to_segment_ids (dict[str | tuple[str, ...], int] | None):\n                The existing mapping from segment labels to segment IDs, or None if it doesn't exist.\n            segment_labels (list[str | tuple[str, ...]]): The list of segment labels.\n\n        Returns:\n            dict[str | tuple[str, ...], int]: A tuple containing the validated dictionary mapping segment\n            labels to segment IDs and a boolean flag indicating whether the segment labels are hashed.\n\n        Raises:\n            ValueError: If the length of `dict_labels_to_segment_ids` doesn't match the length of `segment_labels`.\n            ValueError: If a segment label is not present in `dict_labels_to_segment_ids`.\n            ValueError: If there are non-unique segment IDs in `dict_labels_to_segment_ids`.\n        \"\"\"\n        if dict_labels_to_segment_ids is None:\n            logger.debug(\"Creating mapping from segment labels into ids\")\n            return {segment: idx + 1 for idx, segment in enumerate(segment_labels)}\n\n        logger.debug(\"Using existing mapping from segment labels into segment ids\")\n\n        if len(dict_labels_to_segment_ids) != len(segment_labels):\n            raise ValueError(\n                (\n                    f\"Incorrect dict_labels_to_segment_ids - length mismatch. Expected: \"\n                    f\"{len(segment_labels)}, Actual: {len(dict_labels_to_segment_ids)}\"\n                )\n            )\n\n        unique_segment_ids = set(dict_labels_to_segment_ids.values())\n        if len(unique_segment_ids) != len(segment_labels):\n            raise ValueError(\"Non unique segment ids in the dict_labels_to_segment_ids\")\n\n        logger.debug(\"Passed mapping is correct\")\n        return dict_labels_to_segment_ids\n\n    @staticmethod\n    def _create_single_dim_band_mask(\n        hsi: HSI,\n        dict_labels_to_indices: dict[str | tuple[str, ...], list[int]],\n        dict_labels_to_segment_ids: dict[str | tuple[str, ...], int],\n        device: torch.device,\n    ) -&gt; torch.Tensor:\n        \"\"\"Create a one-dimensional band mask based on the given image, labels, and segment IDs.\n\n        Args:\n            hsi (HSI): The input hsi.\n            dict_labels_to_indices (dict[str | tuple[str, ...], list[int]]):\n                A dictionary mapping labels or label tuples to lists of indices.\n            dict_labels_to_segment_ids (dict[str | tuple[str, ...], int]):\n                A dictionary mapping labels or label tuples to segment IDs.\n            device (torch.device): The device to use for the tensor.\n\n        Returns:\n            torch.Tensor: The one-dimensional band mask tensor.\n\n        Raises:\n            ValueError: If the indices for a segment are out of bounds for the one-dimensional band mask.\n        \"\"\"\n        band_mask_single_dim = torch.zeros(len(hsi.wavelengths), dtype=torch.int64, device=device)\n\n        segment_labels = list(dict_labels_to_segment_ids.keys())\n\n        for segment_label in segment_labels[::-1]:\n            segment_indices = dict_labels_to_indices[segment_label]\n            segment_id = dict_labels_to_segment_ids[segment_label]\n            are_indices_valid = all(0 &lt;= idx &lt; band_mask_single_dim.shape[0] for idx in segment_indices)\n            if not are_indices_valid:\n                raise ValueError(\n                    (\n                        f\"Indices for segment {segment_label} are out of bounds for the one-dimensional band mask\"\n                        f\"of shape {band_mask_single_dim.shape}\"\n                    )\n                )\n            band_mask_single_dim[segment_indices] = segment_id\n\n        return band_mask_single_dim\n\n    @staticmethod\n    def _create_tensor_band_mask(\n        hsi: HSI,\n        dict_labels_to_indices: dict[str | tuple[str, ...], list[int]],\n        dict_labels_to_segment_ids: dict[str | tuple[str, ...], int] | None = None,\n        device: str | torch.device | None = None,\n        repeat_dimensions: bool = False,\n        return_dict_labels_to_segment_ids: bool = True,\n    ) -&gt; torch.Tensor | tuple[torch.Tensor, dict[tuple[str, ...] | str, int]]:\n        \"\"\"Create a tensor band mask from dictionaries. The band mask is created based on the given hsi, labels, and\n        segment IDs. The band mask is a tensor with the same shape as the input hsi and contains segment IDs, where each\n        segment is represented by a unique ID. The band mask will be used to attribute the hsi using the LIME method.\n\n        Args:\n            hsi (HSI): The input hsi.\n            dict_labels_to_indices (dict[str | tuple[str, ...], list[int]]): A dictionary mapping labels to indices.\n            dict_labels_to_segment_ids (dict[str | tuple[str, ...], int] | None, optional):\n                A dictionary mapping labels to segment IDs. Defaults to None.\n            device (str | torch.device | None, optional): The device to use. Defaults to None.\n            repeat_dimensions (bool, optional): Whether to repeat dimensions. Defaults to False.\n            return_dict_labels_to_segment_ids (bool, optional):\n                Whether to return the dictionary mapping labels to segment IDs. Defaults to True.\n\n        Returns:\n            torch.Tensor | tuple[torch.Tensor, dict[tuple[str, ...] | str, int]]:\n                The tensor band mask or a tuple containing the tensor band mask\n                and the dictionary mapping labels to segment IDs.\n        \"\"\"\n        if device is None:\n            device = hsi.device\n        segment_labels = list(dict_labels_to_indices.keys())\n\n        logger.debug(f\"Creating a band mask on the device {device} using {len(segment_labels)} segments\")\n\n        # Check for overlapping segments\n        Lime._check_overlapping_segments(hsi, dict_labels_to_indices)\n\n        # Create or validate dict_labels_to_segment_ids\n        dict_labels_to_segment_ids = Lime._validate_and_create_dict_labels_to_segment_ids(\n            dict_labels_to_segment_ids, segment_labels\n        )\n\n        # Create single-dimensional band mask\n        band_mask_single_dim = Lime._create_single_dim_band_mask(\n            hsi, dict_labels_to_indices, dict_labels_to_segment_ids, device\n        )\n\n        # Expand band mask to match image dimensions\n        band_mask = expand_spectral_mask(hsi, band_mask_single_dim, repeat_dimensions)\n\n        if return_dict_labels_to_segment_ids:\n            return band_mask, dict_labels_to_segment_ids\n        return band_mask\n\n    def attribute(\n        self,\n        hsi: HSI,\n        target: int | None = None,\n        attribution_type: Literal[\"spatial\", \"spectral\"] | None = None,\n        **kwargs,\n    ) -&gt; HSISpectralAttributes | HSISpatialAttributes:\n        \"\"\"A wrapper function to attribute the image using the LIME method. It executes either the\n        `get_spatial_attributes` or `get_spectral_attributes` method based on the provided `attribution_type`. For more\n        detailed description of the methods, please refer to the respective method documentation.\n\n        Additional, nondefault parameters, should be passed as keyword arguments to avoid misalignment of the arguments.\n\n        Args:\n            attribution_type (Literal[\"spatial\", \"spectral\"] | None): An attribution type to be executed. By default None.\n            hsi (HSI): an image on which the explanation is performed.\n            target (int | None, optional): Target output index for the explanation. Defaults to None.\n\n        Returns:\n            HSISpectralAttributes | HSISpatialAttributes: An object containing the image, the attributions and additional information. In case the `attribution_type` is `spatial`, the object is of type `HSISpatialAttributes`, otherwise it is of type `HSISpectralAttributes`.\n        \"\"\"\n        if attribution_type == \"spatial\":\n            return self.get_spatial_attributes(hsi, target=target, **kwargs)\n        elif attribution_type == \"spectral\":\n            return self.get_spectral_attributes(hsi, target=target, **kwargs)\n        raise ValueError(f\"Unsupported attribution type: {attribution_type}. Use 'spatial' or 'spectral'\")\n\n    def get_spatial_attributes(\n        self,\n        hsi: HSI,\n        segmentation_mask: np.ndarray | torch.Tensor | None = None,\n        target: int | None = None,\n        n_samples: int = 10,\n        perturbations_per_eval: int = 4,\n        verbose: bool = False,\n        postprocessing_segmentation_output: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None = None,\n        segmentation_method: Literal[\"slic\", \"patch\"] = \"slic\",\n        **segmentation_method_params: Any,\n    ) -&gt; HSISpatialAttributes:\n        \"\"\"\n        Get spatial attributes of an hsi image using the LIME method. Based on the provided hsi and segmentation mask\n        LIME method attributes the `superpixels` provided by the segmentation mask. Please refer to the original paper\n        `https://arxiv.org/abs/1602.04938` for more details or to Christoph Molnar's book\n        `https://christophm.github.io/interpretable-ml-book/lime.html`.\n\n        This function attributes the hyperspectral image using the LIME (Local Interpretable Model-Agnostic Explanations)\n        method for spatial data. It returns an `HSISpatialAttributes` object that contains the hyperspectral image,,\n        the attributions, the segmentation mask, and the score of the interpretable model used for the explanation.\n\n        Args:\n            hsi (HSI): An HSI object for which the attribution is performed.\n            segmentation_mask (np.ndarray | torch.Tensor | None, optional):\n                A segmentation mask according to which the attribution should be performed.\n                The segmentation mask should have a 3D shape, which can be broadcastable to the shape of the input image.\n                The only dimension on which the image and the mask shapes can differ is the spectral dimension, marked with letter `C` in the `image.orientation` parameter.\n                If None, a new segmentation mask is created using the `segmentation_method`.\n                    Additional parameters for the segmentation method may be passed as kwargs. Defaults to None.\n            target (int, optional): If the model creates more than one output, it analyzes the given target.\n                Defaults to None.\n            n_samples (int, optional): The number of samples to generate/analyze in LIME. The more the better but slower. Defaults to 10.\n            perturbations_per_eval (int, optional): The number of perturbations to evaluate at once (Simply the inner batch size).\n                Defaults to 4.\n            verbose (bool, optional): Whether to show the progress bar. Defaults to False.\n                postprocessing_segmentation_output (Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None): A\n                segmentation postprocessing function for segmentation problem type. This is required for segmentation problem type as\n                lime surrogate model needs to be optimized on the 1d output, and the model should be able to modify the model output with\n                inner lime active region mask as input and return the 1d output (for example number of pixel for each class) and not class mask.\n                   Defaults to None.\n            segmentation_method (Literal[\"slic\", \"patch\"], optional):\n                Segmentation method used only if `segmentation_mask` is None. Defaults to \"slic\".\n            **segmentation_method_params (Any): Additional parameters for the segmentation method.\n\n        Returns:\n            HSISpatialAttributes: A `HSISpatialAttributes` object that contains the image, the attributions,\n                the segmentation mask, and the score of the interpretable model used for the explanation.\n\n        Raises:\n            ValueError: If the Lime object is not initialized or is not an instance of LimeBase.\n            AssertionError: If explainable model type is `segmentation` and `postprocessing_segmentation_output` is not provided.\n            AssertionError: If the hsi is not an instance of the HSI class.\n\n        Examples:\n            &gt;&gt;&gt; simple_model = lambda x: torch.rand((x.shape[0], 2))\n            &gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n            &gt;&gt;&gt; segmentation_mask = torch.randint(1, 4, (1, 240, 240))\n            &gt;&gt;&gt; lime = meteors.attr.Lime(\n                    explainable_model=ExplainableModel(simple_model, \"regression\"), interpretable_model=SkLearnLasso(alpha=0.1)\n                )\n            &gt;&gt;&gt; spatial_attribution = lime.get_spatial_attributes(hsi, segmentation_mask=segmentation_mask, target=0)\n            &gt;&gt;&gt; spatial_attribution.hsi\n            HSI(shape=(4, 240, 240), dtype=torch.float32)\n            &gt;&gt;&gt; spatial_attribution.attributes.shape\n            torch.Size([4, 240, 240])\n            &gt;&gt;&gt; spatial_attribution.segmentation_mask.shape\n            torch.Size([1, 240, 240])\n            &gt;&gt;&gt; spatial_attribution.score\n            1.0\n        \"\"\"\n        if self._attribution_method is None or not isinstance(self._attribution_method, LimeBase):\n            raise ValueError(\"Lime object not initialized\")  # pragma: no cover\n\n        assert isinstance(hsi, HSI), \"hsi should be an instance of HSI class\"\n\n        if self.explainable_model.problem_type == \"segmentation\":\n            assert postprocessing_segmentation_output, (\n                \"postprocessing_segmentation_output is required for segmentation problem type, please provide \"\n                \"the `postprocessing_segmentation_output`. For a reference \"\n                \"we provided an example function to use `agg_segmentation_postprocessing` from `meteors.utils.utils` module\"\n            )\n        elif postprocessing_segmentation_output is not None:\n            logger.warning(\n                \"postprocessing_segmentation_output is provided but the problem is not segmentation, will be ignored\"\n            )\n            postprocessing_segmentation_output = None\n\n        if segmentation_mask is None:\n            segmentation_mask = self.get_segmentation_mask(hsi, segmentation_method, **segmentation_method_params)\n        segmentation_mask = ensure_torch_tensor(\n            segmentation_mask, \"Segmentation mask should be None, numpy array, or torch tensor\"\n        )\n\n        segmentation_mask = validate_mask_shape(\"segmentation\", hsi, segmentation_mask)\n\n        hsi = hsi.to(self.device)\n        segmentation_mask = segmentation_mask.to(self.device)\n\n        lime_attributes, score = self._attribution_method.attribute(\n            inputs=hsi.get_image().unsqueeze(0),\n            target=target,\n            feature_mask=segmentation_mask.unsqueeze(0),\n            n_samples=n_samples,\n            perturbations_per_eval=perturbations_per_eval,\n            model_postprocessing=postprocessing_segmentation_output,\n            show_progress=verbose,\n            return_input_shape=True,\n        )\n\n        spatial_attribution = HSISpatialAttributes(\n            hsi=hsi,\n            attributes=lime_attributes.squeeze(0),\n            mask=segmentation_mask.expand_as(hsi.image),\n            score=score,\n            attribution_method=\"Lime\",\n        )\n\n        return spatial_attribution\n\n    def get_spectral_attributes(\n        self,\n        hsi: HSI,\n        band_mask: np.ndarray | torch.Tensor | None = None,\n        target=None,\n        n_samples: int = 10,\n        perturbations_per_eval: int = 4,\n        verbose: bool = False,\n        postprocessing_segmentation_output: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None = None,\n        band_names: list[str | list[str]] | dict[tuple[str, ...] | str, int] | None = None,\n    ) -&gt; HSISpectralAttributes:\n        \"\"\"\n        Attributes the hsi image using LIME method for spectral data. Based on the provided hsi and band mask, the LIME\n        method attributes the hsi based on `superbands` (clustered bands) provided by the band mask.\n        Please refer to the original paper `https://arxiv.org/abs/1602.04938` for more details or to\n        Christoph Molnar's book `https://christophm.github.io/interpretable-ml-book/lime.html`.\n\n        The function returns a HSISpectralAttributes object that contains the image, the attributions, the band mask,\n        the band names, and the score of the interpretable model used for the explanation.\n\n        Args:\n            hsi (HSI): An HSI object for which the attribution is performed.\n            band_mask (np.ndarray | torch.Tensor | None, optional): Band mask that is used for the spectral attribution.\n                The band mask should have a 3D shape, which can be broadcastable to the shape of the input image.\n                The only dimensions on which the image and the mask shapes can differ is the height and width dimensions, marked with letters `H` and `W` in the `image.orientation` parameter.\n                If equals to None, the band mask is created within the function. Defaults to None.\n            target (int, optional): If the model creates more than one output, it analyzes the given target.\n                Defaults to None.\n            n_samples (int, optional): The number of samples to generate/analyze in LIME. The more the better but slower. Defaults to 10.\n            perturbations_per_eval (int, optional): The number of perturbations to evaluate at once (Simply the inner batch size).\n                Defaults to 4.\n            verbose (bool, optional): Specifies whether to show progress during the attribution process. Defaults to False.\n            postprocessing_segmentation_output: (Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None):\n                A segmentation postprocessing function for segmentation problem type. This is required for segmentation problem type as\n                lime surrogate model needs to be optimized on the 1d output, and the model should be able to modify the model output with\n                inner lime active region mask as input and return the 1d output (for example number of pixel for each class) and not class mask.\n                Defaults to None.\n            band_names (list[str] | dict[str | tuple[str, ...], int] | None, optional): Band names. Defaults to None.\n\n        Returns:\n            HSISpectralAttributes: A HSISpectralAttributes object containing the image, the attributions,\n                the band mask, the band names, and the score of the interpretable model used for the explanation.\n\n        Raises:\n            ValueError: If the Lime object is not initialized or is not an instance of LimeBase.\n            AssertionError: If explainable model type is `segmentation` and `postprocessing_segmentation_output` is not provided.\n            AssertionError: If the hsi is not an instance of the HSI class.\n\n        Examples:\n            &gt;&gt;&gt; simple_model = lambda x: torch.rand((x.shape[0], 2))\n            &gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n            &gt;&gt;&gt; band_mask = torch.randint(1, 4, (4, 1, 1)).repeat(1, 240, 240)\n            &gt;&gt;&gt; band_names = [\"R\", \"G\", \"B\"]\n            &gt;&gt;&gt; lime = meteors.attr.Lime(\n                    explainable_model=ExplainableModel(simple_model, \"regression\"), interpretable_model=SkLearnLasso(alpha=0.1)\n                )\n            &gt;&gt;&gt; spectral_attribution = lime.get_spectral_attributes(hsi, band_mask=band_mask, band_names=band_names, target=0)\n            &gt;&gt;&gt; spectral_attribution.hsi\n            HSI(shape=(4, 240, 240), dtype=torch.float32)\n            &gt;&gt;&gt; spectral_attribution.attributes.shape\n            torch.Size([4, 240, 240])\n            &gt;&gt;&gt; spectral_attribution.band_mask.shape\n            torch.Size([4, 240, 240])\n            &gt;&gt;&gt; spectral_attribution.band_names\n            [\"R\", \"G\", \"B\"]\n            &gt;&gt;&gt; spectral_attribution.score\n            1.0\n        \"\"\"\n\n        if self._attribution_method is None or not isinstance(self._attribution_method, LimeBase):\n            raise ValueError(\"Lime object not initialized\")  # pragma: no cover\n\n        if self.explainable_model.problem_type == \"segmentation\":\n            assert postprocessing_segmentation_output, (\n                \"postprocessing_segmentation_output is required for segmentation problem type, please provide \"\n                \"the `postprocessing_segmentation_output`. For a reference \"\n                \"we provided an example function to use `agg_segmentation_postprocessing` from `meteors.utils.utils` module\"\n            )\n        elif postprocessing_segmentation_output is not None:\n            logger.warning(\n                \"postprocessing_segmentation_output is provided but the problem is not segmentation, will be ignored\"\n            )\n            postprocessing_segmentation_output = None\n\n        assert isinstance(hsi, HSI), \"hsi should be an instance of HSI class\"\n\n        if band_mask is None:\n            band_mask, band_names = self.get_band_mask(hsi, band_names)\n        band_mask = ensure_torch_tensor(band_mask, \"Band mask should be None, numpy array, or torch tensor\")\n        if band_mask.shape != hsi.image.shape:\n            band_mask = expand_spectral_mask(hsi, band_mask, repeat_dimensions=True)\n        band_mask = band_mask.int()\n\n        if band_names is None:\n            unique_segments = torch.unique(band_mask)\n            band_names = {str(segment): idx for idx, segment in enumerate(unique_segments)}\n        else:\n            # checking consistency of names\n            # unique_segments = torch.unique(band_mask)\n            # if isinstance(band_names, dict):\n            #     assert set(unique_segments).issubset(set(band_names.values())), \"Incorrect band names\"\n            logger.debug(\n                \"Band names are provided and will be used. In the future, there should be an option to validate them.\"\n            )\n\n        band_mask = validate_mask_shape(\"band\", hsi, band_mask)\n\n        hsi = hsi.to(self.device)\n        band_mask = band_mask.to(self.device)\n\n        lime_attributes, score = self._attribution_method.attribute(\n            inputs=hsi.get_image().unsqueeze(0),\n            target=target,\n            feature_mask=band_mask.unsqueeze(0),\n            n_samples=n_samples,\n            perturbations_per_eval=perturbations_per_eval,\n            model_postprocessing=postprocessing_segmentation_output,\n            show_progress=verbose,\n            return_input_shape=True,\n        )\n\n        spectral_attribution = HSISpectralAttributes(\n            hsi=hsi,\n            attributes=lime_attributes.squeeze(0),\n            mask=band_mask.expand_as(hsi.image),\n            band_names=band_names,\n            score=score,\n            attribution_method=\"Lime\",\n        )\n\n        return spectral_attribution\n\n    @staticmethod\n    def _get_slick_segmentation_mask(\n        hsi: HSI, num_interpret_features: int = 10, *args: Any, **kwargs: Any\n    ) -&gt; torch.Tensor:\n        \"\"\"Creates a segmentation mask using the SLIC method.\n\n        Args:\n            hsi (HSI): An HSI object for which the segmentation mask is created.\n            num_interpret_features (int, optional): Number of segments. Defaults to 10.\n            *args: Additional positional arguments to be passed to the SLIC method.\n            **kwargs: Additional keyword arguments to be passed to the SLIC method.\n\n        Returns:\n            torch.Tensor: An output segmentation mask.\n        \"\"\"\n        segmentation_mask = slic(\n            hsi.get_image().cpu().detach().numpy(),\n            n_segments=num_interpret_features,\n            mask=hsi.spatial_binary_mask.cpu().detach().numpy(),\n            channel_axis=hsi.spectral_axis,\n            *args,\n            **kwargs,\n        )\n\n        if segmentation_mask.min() == 1:\n            segmentation_mask -= 1\n\n        segmentation_mask = torch.from_numpy(segmentation_mask)\n        segmentation_mask = segmentation_mask.unsqueeze(dim=hsi.spectral_axis)\n\n        return segmentation_mask\n\n    @staticmethod\n    def _get_patch_segmentation_mask(hsi: HSI, patch_size: int | float = 10, *args: Any, **kwargs: Any) -&gt; torch.Tensor:\n        \"\"\"\n        Creates a segmentation mask using the patch method - creates small squares of the same size\n            and assigns a unique value to each square.\n\n        Args:\n            hsi (HSI): An HSI object for which the segmentation mask is created.\n            patch_size (int, optional): Size of the patch, the hsi size should be divisible by this value.\n                Defaults to 10.\n\n        Returns:\n            torch.Tensor: An output segmentation mask.\n        \"\"\"\n        if patch_size &lt; 1 or not isinstance(patch_size, (int, float)):\n            raise ValueError(\"Invalid patch_size. patch_size must be a positive integer\")\n\n        if hsi.image.shape[1] % patch_size != 0 or hsi.image.shape[2] % patch_size != 0:\n            raise ValueError(\"Invalid patch_size. patch_size must be a factor of both width and height of the hsi\")\n\n        height, width = hsi.image.shape[1], hsi.image.shape[2]\n\n        idx_mask = torch.arange(height // patch_size * width // patch_size, device=hsi.device).reshape(\n            height // patch_size, width // patch_size\n        )\n        idx_mask += 1\n        segmentation_mask = torch.repeat_interleave(idx_mask, patch_size, dim=0)\n        segmentation_mask = torch.repeat_interleave(segmentation_mask, patch_size, dim=1)\n        segmentation_mask = segmentation_mask * hsi.spatial_binary_mask\n        # segmentation_mask = torch.repeat_interleave(\n        # torch.unsqueeze(segmentation_mask, dim=hsi.spectral_axis),\n        # repeats=hsi.image.shape[hsi.spectral_axis], dim=hsi.spectral_axis)\n        segmentation_mask = segmentation_mask.unsqueeze(dim=hsi.spectral_axis)\n\n        mask_idx = np.unique(segmentation_mask).tolist()\n        for idx, mask_val in enumerate(mask_idx):\n            segmentation_mask[segmentation_mask == mask_val] = idx\n\n        return segmentation_mask\n</code></pre>"},{"location":"reference/#src.meteors.attr.lime.Lime.attribute","title":"<code>attribute(hsi, target=None, attribution_type=None, **kwargs)</code>","text":"<p>A wrapper function to attribute the image using the LIME method. It executes either the <code>get_spatial_attributes</code> or <code>get_spectral_attributes</code> method based on the provided <code>attribution_type</code>. For more detailed description of the methods, please refer to the respective method documentation.</p> <p>Additional, nondefault parameters, should be passed as keyword arguments to avoid misalignment of the arguments.</p> <p>Parameters:</p> Name Type Description Default <code>attribution_type</code> <code>Literal['spatial', 'spectral'] | None</code> <p>An attribution type to be executed. By default None.</p> <code>None</code> <code>hsi</code> <code>HSI</code> <p>an image on which the explanation is performed.</p> required <code>target</code> <code>int | None</code> <p>Target output index for the explanation. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>HSISpectralAttributes | HSISpatialAttributes</code> <p>HSISpectralAttributes | HSISpatialAttributes: An object containing the image, the attributions and additional information. In case the <code>attribution_type</code> is <code>spatial</code>, the object is of type <code>HSISpatialAttributes</code>, otherwise it is of type <code>HSISpectralAttributes</code>.</p> Source code in <code>src/meteors/attr/lime.py</code> <pre><code>def attribute(\n    self,\n    hsi: HSI,\n    target: int | None = None,\n    attribution_type: Literal[\"spatial\", \"spectral\"] | None = None,\n    **kwargs,\n) -&gt; HSISpectralAttributes | HSISpatialAttributes:\n    \"\"\"A wrapper function to attribute the image using the LIME method. It executes either the\n    `get_spatial_attributes` or `get_spectral_attributes` method based on the provided `attribution_type`. For more\n    detailed description of the methods, please refer to the respective method documentation.\n\n    Additional, nondefault parameters, should be passed as keyword arguments to avoid misalignment of the arguments.\n\n    Args:\n        attribution_type (Literal[\"spatial\", \"spectral\"] | None): An attribution type to be executed. By default None.\n        hsi (HSI): an image on which the explanation is performed.\n        target (int | None, optional): Target output index for the explanation. Defaults to None.\n\n    Returns:\n        HSISpectralAttributes | HSISpatialAttributes: An object containing the image, the attributions and additional information. In case the `attribution_type` is `spatial`, the object is of type `HSISpatialAttributes`, otherwise it is of type `HSISpectralAttributes`.\n    \"\"\"\n    if attribution_type == \"spatial\":\n        return self.get_spatial_attributes(hsi, target=target, **kwargs)\n    elif attribution_type == \"spectral\":\n        return self.get_spectral_attributes(hsi, target=target, **kwargs)\n    raise ValueError(f\"Unsupported attribution type: {attribution_type}. Use 'spatial' or 'spectral'\")\n</code></pre>"},{"location":"reference/#src.meteors.attr.lime.Lime.get_band_mask","title":"<code>get_band_mask(hsi, band_names=None, band_indices=None, band_wavelengths=None, device=None, repeat_dimensions=False)</code>  <code>staticmethod</code>","text":"<p>Generates a band mask based on the provided hsi and band information.</p> <p>Remember you need to provide either band_names, band_indices, or band_wavelengths to create the band mask. If you provide more than one, the band mask will be created using only one using the following priority: band_names &gt; band_wavelengths &gt; band_indices.</p> <p>Parameters:</p> Name Type Description Default <code>hsi</code> <code>HSI</code> <p>The input hyperspectral image.</p> required <code>band_names</code> <code>None | list[str | list[str]] | dict[tuple[str, ...] | str, int]</code> <p>The names of the spectral bands to include in the mask. Defaults to None.</p> <code>None</code> <code>band_indices</code> <code>None | dict[str | tuple[str, ...], list[tuple[int, int]] | tuple[int, int] | list[int]]</code> <p>The indices or ranges of indices of the spectral bands to include in the mask. Defaults to None.</p> <code>None</code> <code>band_wavelengths</code> <code>None | dict[str | tuple[str, ...], list[tuple[float, float]] | tuple[float, float], list[float], float]</code> <p>The wavelengths or ranges of wavelengths of the spectral bands to include in the mask. Defaults to None.</p> <code>None</code> <code>device</code> <code>str | device | None</code> <p>The device to use for computation. Defaults to None.</p> <code>None</code> <code>repeat_dimensions</code> <code>bool</code> <p>Whether to repeat the dimensions of the mask to match the input hsi shape. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>tuple[torch.Tensor, dict[tuple[str, ...] | str, int]]: A tuple containing the band mask tensor and a dictionary</p> <code>dict[tuple[str, ...] | str, int]</code> <p>mapping band names to segment IDs.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input hsi is not an instance of the HSI class.</p> <code>ValueError</code> <p>If no band names, indices, or wavelengths are provided.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((len(wavelengths), 10, 10)), wavelengths=wavelengths)\n&gt;&gt;&gt; band_names = [\"R\", \"G\"]\n&gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_names=band_names)\n&gt;&gt;&gt; dict_labels_to_segment_ids\n{\"R\": 1, \"G\": 2}\n&gt;&gt;&gt; band_indices = {\"RGB\": [0, 1, 2]}\n&gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_indices=band_indices)\n&gt;&gt;&gt; dict_labels_to_segment_ids\n{\"RGB\": 1}\n&gt;&gt;&gt; band_wavelengths = {\"RGB\": [(462.08, 465.27), (465.27, 468.47), (468.47, 471.68)]}\n&gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_wavelengths=band_wavelengths)\n&gt;&gt;&gt; dict_labels_to_segment_ids\n{\"RGB\": 1}\n</code></pre> Source code in <code>src/meteors/attr/lime.py</code> <pre><code>@staticmethod\ndef get_band_mask(\n    hsi: HSI,\n    band_names: None | list[str | list[str]] | dict[tuple[str, ...] | str, int] = None,\n    band_indices: None | dict[str | tuple[str, ...], ListOfWavelengthsIndices] = None,\n    band_wavelengths: None | dict[str | tuple[str, ...], ListOfWavelengths] = None,\n    device: str | torch.device | None = None,\n    repeat_dimensions: bool = False,\n) -&gt; tuple[torch.Tensor, dict[tuple[str, ...] | str, int]]:\n    \"\"\"Generates a band mask based on the provided hsi and band information.\n\n    Remember you need to provide either band_names, band_indices, or band_wavelengths to create the band mask.\n    If you provide more than one, the band mask will be created using only one using the following priority:\n    band_names &gt; band_wavelengths &gt; band_indices.\n\n    Args:\n        hsi (HSI): The input hyperspectral image.\n        band_names (None | list[str | list[str]] | dict[tuple[str, ...] | str, int], optional):\n            The names of the spectral bands to include in the mask. Defaults to None.\n        band_indices (None | dict[str | tuple[str, ...], list[tuple[int, int]] | tuple[int, int] | list[int]], optional):\n            The indices or ranges of indices of the spectral bands to include in the mask. Defaults to None.\n        band_wavelengths (None | dict[str | tuple[str, ...], list[tuple[float, float]] | tuple[float, float], list[float], float], optional):\n            The wavelengths or ranges of wavelengths of the spectral bands to include in the mask. Defaults to None.\n        device (str | torch.device | None, optional):\n            The device to use for computation. Defaults to None.\n        repeat_dimensions (bool, optional):\n            Whether to repeat the dimensions of the mask to match the input hsi shape. Defaults to False.\n\n    Returns:\n        tuple[torch.Tensor, dict[tuple[str, ...] | str, int]]: A tuple containing the band mask tensor and a dictionary\n        mapping band names to segment IDs.\n\n    Raises:\n        ValueError: If the input hsi is not an instance of the HSI class.\n        ValueError: If no band names, indices, or wavelengths are provided.\n\n    Examples:\n        &gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((len(wavelengths), 10, 10)), wavelengths=wavelengths)\n        &gt;&gt;&gt; band_names = [\"R\", \"G\"]\n        &gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_names=band_names)\n        &gt;&gt;&gt; dict_labels_to_segment_ids\n        {\"R\": 1, \"G\": 2}\n        &gt;&gt;&gt; band_indices = {\"RGB\": [0, 1, 2]}\n        &gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_indices=band_indices)\n        &gt;&gt;&gt; dict_labels_to_segment_ids\n        {\"RGB\": 1}\n        &gt;&gt;&gt; band_wavelengths = {\"RGB\": [(462.08, 465.27), (465.27, 468.47), (468.47, 471.68)]}\n        &gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_wavelengths=band_wavelengths)\n        &gt;&gt;&gt; dict_labels_to_segment_ids\n        {\"RGB\": 1}\n    \"\"\"\n    if not isinstance(hsi, HSI):\n        raise ValueError(\"hsi should be an instance of HSI class\")\n\n    assert (\n        band_names is not None or band_indices is not None or band_wavelengths is not None\n    ), \"No band names, indices, or wavelengths are provided.\"\n\n    # validate types\n    dict_labels_to_segment_ids = None\n    if band_names is not None:\n        logger.debug(\"Getting band mask from band names of spectral bands\")\n        if band_wavelengths is not None or band_indices is not None:\n            ignored_params = [\n                param\n                for param in [\"band_wavelengths\", \"band_indices\"]\n                if param in locals() and locals()[param] is not None\n            ]\n            ignored_params_str = \" and \".join(ignored_params)\n            logger.info(\n                f\"Only the band names will be used to create the band mask. The additional parameters {ignored_params_str} will be ignored.\"\n            )\n        try:\n            validate_band_names(band_names)\n            band_groups, dict_labels_to_segment_ids = Lime._get_band_wavelengths_indices_from_band_names(\n                hsi.wavelengths, band_names\n            )\n        except Exception as e:\n            raise ValueError(f\"Incorrect band names provided: {e}\") from e\n    elif band_wavelengths is not None:\n        logger.debug(\"Getting band mask from band groups given by ranges of wavelengths\")\n        if band_indices is not None:\n            logger.info(\n                \"Only the band wavelengths will be used to create the band mask. The band_indices will be ignored.\"\n            )\n        validate_band_format(band_wavelengths, variable_name=\"band_wavelengths\")\n        try:\n            band_groups = Lime._get_band_indices_from_band_wavelengths(\n                hsi.wavelengths,\n                band_wavelengths,\n            )\n        except Exception as e:\n            raise ValueError(\n                f\"Incorrect band ranges wavelengths provided, please check if provided wavelengths are correct: {e}\"\n            ) from e\n    elif band_indices is not None:\n        logger.debug(\"Getting band mask from band groups given by ranges of indices\")\n        validate_band_format(band_indices, variable_name=\"band_indices\")\n        try:\n            band_groups = Lime._get_band_indices_from_input_band_indices(hsi.wavelengths, band_indices)\n        except Exception as e:\n            raise ValueError(\n                f\"Incorrect band ranges indices provided, please check if provided indices are correct: {e}\"\n            ) from e\n\n    return Lime._create_tensor_band_mask(\n        hsi,\n        band_groups,\n        dict_labels_to_segment_ids=dict_labels_to_segment_ids,\n        device=device,\n        repeat_dimensions=repeat_dimensions,\n        return_dict_labels_to_segment_ids=True,\n    )\n</code></pre>"},{"location":"reference/#src.meteors.attr.lime.Lime.get_segmentation_mask","title":"<code>get_segmentation_mask(hsi, segmentation_method='slic', **segmentation_method_params)</code>  <code>staticmethod</code>","text":"<p>Generates a segmentation mask for the given hsi using the specified segmentation method.</p> <p>Parameters:</p> Name Type Description Default <code>hsi</code> <code>HSI</code> <p>The input hyperspectral image for which the segmentation mask needs to be generated.</p> required <code>segmentation_method</code> <code>Literal['patch', 'slic']</code> <p>The segmentation method to be used. Defaults to \"slic\".</p> <code>'slic'</code> <code>**segmentation_method_params</code> <code>Any</code> <p>Additional parameters specific to the chosen segmentation method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The segmentation mask as a tensor.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input hsi is not an instance of the HSI class.</p> <code>ValueError</code> <p>If an unsupported segmentation method is specified.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hsi = meteors.HSI(image=torch.ones((3, 240, 240)), wavelengths=[462.08, 465.27, 468.47])\n&gt;&gt;&gt; segmentation_mask = mt_lime.Lime.get_segmentation_mask(hsi, segmentation_method=\"slic\")\n&gt;&gt;&gt; segmentation_mask.shape\ntorch.Size([1, 240, 240])\n&gt;&gt;&gt; segmentation_mask = meteors.attr.Lime.get_segmentation_mask(hsi, segmentation_method=\"patch\", patch_size=2)\n&gt;&gt;&gt; segmentation_mask.shape\ntorch.Size([1, 240, 240])\n&gt;&gt;&gt; segmentation_mask[0, :2, :2]\ntorch.tensor([[1, 1],\n              [1, 1]])\n&gt;&gt;&gt; segmentation_mask[0, 2:4, :2]\ntorch.tensor([[2, 2],\n              [2, 2]])\n</code></pre> Source code in <code>src/meteors/attr/lime.py</code> <pre><code>@staticmethod\ndef get_segmentation_mask(\n    hsi: HSI,\n    segmentation_method: Literal[\"patch\", \"slic\"] = \"slic\",\n    **segmentation_method_params: Any,\n) -&gt; torch.Tensor:\n    \"\"\"Generates a segmentation mask for the given hsi using the specified segmentation method.\n\n    Args:\n        hsi (HSI): The input hyperspectral image for which the segmentation mask needs to be generated.\n        segmentation_method (Literal[\"patch\", \"slic\"], optional): The segmentation method to be used.\n            Defaults to \"slic\".\n        **segmentation_method_params (Any): Additional parameters specific to the chosen segmentation method.\n\n    Returns:\n        torch.Tensor: The segmentation mask as a tensor.\n\n    Raises:\n        ValueError: If the input hsi is not an instance of the HSI class.\n        ValueError: If an unsupported segmentation method is specified.\n\n    Examples:\n        &gt;&gt;&gt; hsi = meteors.HSI(image=torch.ones((3, 240, 240)), wavelengths=[462.08, 465.27, 468.47])\n        &gt;&gt;&gt; segmentation_mask = mt_lime.Lime.get_segmentation_mask(hsi, segmentation_method=\"slic\")\n        &gt;&gt;&gt; segmentation_mask.shape\n        torch.Size([1, 240, 240])\n        &gt;&gt;&gt; segmentation_mask = meteors.attr.Lime.get_segmentation_mask(hsi, segmentation_method=\"patch\", patch_size=2)\n        &gt;&gt;&gt; segmentation_mask.shape\n        torch.Size([1, 240, 240])\n        &gt;&gt;&gt; segmentation_mask[0, :2, :2]\n        torch.tensor([[1, 1],\n                      [1, 1]])\n        &gt;&gt;&gt; segmentation_mask[0, 2:4, :2]\n        torch.tensor([[2, 2],\n                      [2, 2]])\n    \"\"\"\n    if not isinstance(hsi, HSI):\n        raise ValueError(\"hsi should be an instance of HSI class\")\n\n    if segmentation_method == \"slic\":\n        return Lime._get_slick_segmentation_mask(hsi, **segmentation_method_params)\n    elif segmentation_method == \"patch\":\n        return Lime._get_patch_segmentation_mask(hsi, **segmentation_method_params)\n    else:\n        raise ValueError(f\"Unsupported segmentation method: {segmentation_method}\")\n</code></pre>"},{"location":"reference/#src.meteors.attr.lime.Lime.get_spatial_attributes","title":"<code>get_spatial_attributes(hsi, segmentation_mask=None, target=None, n_samples=10, perturbations_per_eval=4, verbose=False, postprocessing_segmentation_output=None, segmentation_method='slic', **segmentation_method_params)</code>","text":"<p>Get spatial attributes of an hsi image using the LIME method. Based on the provided hsi and segmentation mask LIME method attributes the <code>superpixels</code> provided by the segmentation mask. Please refer to the original paper <code>https://arxiv.org/abs/1602.04938</code> for more details or to Christoph Molnar's book <code>https://christophm.github.io/interpretable-ml-book/lime.html</code>.</p> <p>This function attributes the hyperspectral image using the LIME (Local Interpretable Model-Agnostic Explanations) method for spatial data. It returns an <code>HSISpatialAttributes</code> object that contains the hyperspectral image,, the attributions, the segmentation mask, and the score of the interpretable model used for the explanation.</p> <p>Parameters:</p> Name Type Description Default <code>hsi</code> <code>HSI</code> <p>An HSI object for which the attribution is performed.</p> required <code>segmentation_mask</code> <code>ndarray | Tensor | None</code> <p>A segmentation mask according to which the attribution should be performed. The segmentation mask should have a 3D shape, which can be broadcastable to the shape of the input image. The only dimension on which the image and the mask shapes can differ is the spectral dimension, marked with letter <code>C</code> in the <code>image.orientation</code> parameter. If None, a new segmentation mask is created using the <code>segmentation_method</code>.     Additional parameters for the segmentation method may be passed as kwargs. Defaults to None.</p> <code>None</code> <code>target</code> <code>int</code> <p>If the model creates more than one output, it analyzes the given target. Defaults to None.</p> <code>None</code> <code>n_samples</code> <code>int</code> <p>The number of samples to generate/analyze in LIME. The more the better but slower. Defaults to 10.</p> <code>10</code> <code>perturbations_per_eval</code> <code>int</code> <p>The number of perturbations to evaluate at once (Simply the inner batch size). Defaults to 4.</p> <code>4</code> <code>verbose</code> <code>bool</code> <p>Whether to show the progress bar. Defaults to False. postprocessing_segmentation_output (Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None): A segmentation postprocessing function for segmentation problem type. This is required for segmentation problem type as lime surrogate model needs to be optimized on the 1d output, and the model should be able to modify the model output with inner lime active region mask as input and return the 1d output (for example number of pixel for each class) and not class mask.    Defaults to None.</p> <code>False</code> <code>segmentation_method</code> <code>Literal['slic', 'patch']</code> <p>Segmentation method used only if <code>segmentation_mask</code> is None. Defaults to \"slic\".</p> <code>'slic'</code> <code>**segmentation_method_params</code> <code>Any</code> <p>Additional parameters for the segmentation method.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>HSISpatialAttributes</code> <code>HSISpatialAttributes</code> <p>A <code>HSISpatialAttributes</code> object that contains the image, the attributions, the segmentation mask, and the score of the interpretable model used for the explanation.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the Lime object is not initialized or is not an instance of LimeBase.</p> <code>AssertionError</code> <p>If explainable model type is <code>segmentation</code> and <code>postprocessing_segmentation_output</code> is not provided.</p> <code>AssertionError</code> <p>If the hsi is not an instance of the HSI class.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; simple_model = lambda x: torch.rand((x.shape[0], 2))\n&gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n&gt;&gt;&gt; segmentation_mask = torch.randint(1, 4, (1, 240, 240))\n&gt;&gt;&gt; lime = meteors.attr.Lime(\n        explainable_model=ExplainableModel(simple_model, \"regression\"), interpretable_model=SkLearnLasso(alpha=0.1)\n    )\n&gt;&gt;&gt; spatial_attribution = lime.get_spatial_attributes(hsi, segmentation_mask=segmentation_mask, target=0)\n&gt;&gt;&gt; spatial_attribution.hsi\nHSI(shape=(4, 240, 240), dtype=torch.float32)\n&gt;&gt;&gt; spatial_attribution.attributes.shape\ntorch.Size([4, 240, 240])\n&gt;&gt;&gt; spatial_attribution.segmentation_mask.shape\ntorch.Size([1, 240, 240])\n&gt;&gt;&gt; spatial_attribution.score\n1.0\n</code></pre> Source code in <code>src/meteors/attr/lime.py</code> <pre><code>def get_spatial_attributes(\n    self,\n    hsi: HSI,\n    segmentation_mask: np.ndarray | torch.Tensor | None = None,\n    target: int | None = None,\n    n_samples: int = 10,\n    perturbations_per_eval: int = 4,\n    verbose: bool = False,\n    postprocessing_segmentation_output: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None = None,\n    segmentation_method: Literal[\"slic\", \"patch\"] = \"slic\",\n    **segmentation_method_params: Any,\n) -&gt; HSISpatialAttributes:\n    \"\"\"\n    Get spatial attributes of an hsi image using the LIME method. Based on the provided hsi and segmentation mask\n    LIME method attributes the `superpixels` provided by the segmentation mask. Please refer to the original paper\n    `https://arxiv.org/abs/1602.04938` for more details or to Christoph Molnar's book\n    `https://christophm.github.io/interpretable-ml-book/lime.html`.\n\n    This function attributes the hyperspectral image using the LIME (Local Interpretable Model-Agnostic Explanations)\n    method for spatial data. It returns an `HSISpatialAttributes` object that contains the hyperspectral image,,\n    the attributions, the segmentation mask, and the score of the interpretable model used for the explanation.\n\n    Args:\n        hsi (HSI): An HSI object for which the attribution is performed.\n        segmentation_mask (np.ndarray | torch.Tensor | None, optional):\n            A segmentation mask according to which the attribution should be performed.\n            The segmentation mask should have a 3D shape, which can be broadcastable to the shape of the input image.\n            The only dimension on which the image and the mask shapes can differ is the spectral dimension, marked with letter `C` in the `image.orientation` parameter.\n            If None, a new segmentation mask is created using the `segmentation_method`.\n                Additional parameters for the segmentation method may be passed as kwargs. Defaults to None.\n        target (int, optional): If the model creates more than one output, it analyzes the given target.\n            Defaults to None.\n        n_samples (int, optional): The number of samples to generate/analyze in LIME. The more the better but slower. Defaults to 10.\n        perturbations_per_eval (int, optional): The number of perturbations to evaluate at once (Simply the inner batch size).\n            Defaults to 4.\n        verbose (bool, optional): Whether to show the progress bar. Defaults to False.\n            postprocessing_segmentation_output (Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None): A\n            segmentation postprocessing function for segmentation problem type. This is required for segmentation problem type as\n            lime surrogate model needs to be optimized on the 1d output, and the model should be able to modify the model output with\n            inner lime active region mask as input and return the 1d output (for example number of pixel for each class) and not class mask.\n               Defaults to None.\n        segmentation_method (Literal[\"slic\", \"patch\"], optional):\n            Segmentation method used only if `segmentation_mask` is None. Defaults to \"slic\".\n        **segmentation_method_params (Any): Additional parameters for the segmentation method.\n\n    Returns:\n        HSISpatialAttributes: A `HSISpatialAttributes` object that contains the image, the attributions,\n            the segmentation mask, and the score of the interpretable model used for the explanation.\n\n    Raises:\n        ValueError: If the Lime object is not initialized or is not an instance of LimeBase.\n        AssertionError: If explainable model type is `segmentation` and `postprocessing_segmentation_output` is not provided.\n        AssertionError: If the hsi is not an instance of the HSI class.\n\n    Examples:\n        &gt;&gt;&gt; simple_model = lambda x: torch.rand((x.shape[0], 2))\n        &gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n        &gt;&gt;&gt; segmentation_mask = torch.randint(1, 4, (1, 240, 240))\n        &gt;&gt;&gt; lime = meteors.attr.Lime(\n                explainable_model=ExplainableModel(simple_model, \"regression\"), interpretable_model=SkLearnLasso(alpha=0.1)\n            )\n        &gt;&gt;&gt; spatial_attribution = lime.get_spatial_attributes(hsi, segmentation_mask=segmentation_mask, target=0)\n        &gt;&gt;&gt; spatial_attribution.hsi\n        HSI(shape=(4, 240, 240), dtype=torch.float32)\n        &gt;&gt;&gt; spatial_attribution.attributes.shape\n        torch.Size([4, 240, 240])\n        &gt;&gt;&gt; spatial_attribution.segmentation_mask.shape\n        torch.Size([1, 240, 240])\n        &gt;&gt;&gt; spatial_attribution.score\n        1.0\n    \"\"\"\n    if self._attribution_method is None or not isinstance(self._attribution_method, LimeBase):\n        raise ValueError(\"Lime object not initialized\")  # pragma: no cover\n\n    assert isinstance(hsi, HSI), \"hsi should be an instance of HSI class\"\n\n    if self.explainable_model.problem_type == \"segmentation\":\n        assert postprocessing_segmentation_output, (\n            \"postprocessing_segmentation_output is required for segmentation problem type, please provide \"\n            \"the `postprocessing_segmentation_output`. For a reference \"\n            \"we provided an example function to use `agg_segmentation_postprocessing` from `meteors.utils.utils` module\"\n        )\n    elif postprocessing_segmentation_output is not None:\n        logger.warning(\n            \"postprocessing_segmentation_output is provided but the problem is not segmentation, will be ignored\"\n        )\n        postprocessing_segmentation_output = None\n\n    if segmentation_mask is None:\n        segmentation_mask = self.get_segmentation_mask(hsi, segmentation_method, **segmentation_method_params)\n    segmentation_mask = ensure_torch_tensor(\n        segmentation_mask, \"Segmentation mask should be None, numpy array, or torch tensor\"\n    )\n\n    segmentation_mask = validate_mask_shape(\"segmentation\", hsi, segmentation_mask)\n\n    hsi = hsi.to(self.device)\n    segmentation_mask = segmentation_mask.to(self.device)\n\n    lime_attributes, score = self._attribution_method.attribute(\n        inputs=hsi.get_image().unsqueeze(0),\n        target=target,\n        feature_mask=segmentation_mask.unsqueeze(0),\n        n_samples=n_samples,\n        perturbations_per_eval=perturbations_per_eval,\n        model_postprocessing=postprocessing_segmentation_output,\n        show_progress=verbose,\n        return_input_shape=True,\n    )\n\n    spatial_attribution = HSISpatialAttributes(\n        hsi=hsi,\n        attributes=lime_attributes.squeeze(0),\n        mask=segmentation_mask.expand_as(hsi.image),\n        score=score,\n        attribution_method=\"Lime\",\n    )\n\n    return spatial_attribution\n</code></pre>"},{"location":"reference/#src.meteors.attr.lime.Lime.get_spectral_attributes","title":"<code>get_spectral_attributes(hsi, band_mask=None, target=None, n_samples=10, perturbations_per_eval=4, verbose=False, postprocessing_segmentation_output=None, band_names=None)</code>","text":"<p>Attributes the hsi image using LIME method for spectral data. Based on the provided hsi and band mask, the LIME method attributes the hsi based on <code>superbands</code> (clustered bands) provided by the band mask. Please refer to the original paper <code>https://arxiv.org/abs/1602.04938</code> for more details or to Christoph Molnar's book <code>https://christophm.github.io/interpretable-ml-book/lime.html</code>.</p> <p>The function returns a HSISpectralAttributes object that contains the image, the attributions, the band mask, the band names, and the score of the interpretable model used for the explanation.</p> <p>Parameters:</p> Name Type Description Default <code>hsi</code> <code>HSI</code> <p>An HSI object for which the attribution is performed.</p> required <code>band_mask</code> <code>ndarray | Tensor | None</code> <p>Band mask that is used for the spectral attribution. The band mask should have a 3D shape, which can be broadcastable to the shape of the input image. The only dimensions on which the image and the mask shapes can differ is the height and width dimensions, marked with letters <code>H</code> and <code>W</code> in the <code>image.orientation</code> parameter. If equals to None, the band mask is created within the function. Defaults to None.</p> <code>None</code> <code>target</code> <code>int</code> <p>If the model creates more than one output, it analyzes the given target. Defaults to None.</p> <code>None</code> <code>n_samples</code> <code>int</code> <p>The number of samples to generate/analyze in LIME. The more the better but slower. Defaults to 10.</p> <code>10</code> <code>perturbations_per_eval</code> <code>int</code> <p>The number of perturbations to evaluate at once (Simply the inner batch size). Defaults to 4.</p> <code>4</code> <code>verbose</code> <code>bool</code> <p>Specifies whether to show progress during the attribution process. Defaults to False.</p> <code>False</code> <code>postprocessing_segmentation_output</code> <code>Callable[[Tensor, Tensor], Tensor] | None</code> <p>(Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None): A segmentation postprocessing function for segmentation problem type. This is required for segmentation problem type as lime surrogate model needs to be optimized on the 1d output, and the model should be able to modify the model output with inner lime active region mask as input and return the 1d output (for example number of pixel for each class) and not class mask. Defaults to None.</p> <code>None</code> <code>band_names</code> <code>list[str] | dict[str | tuple[str, ...], int] | None</code> <p>Band names. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>HSISpectralAttributes</code> <code>HSISpectralAttributes</code> <p>A HSISpectralAttributes object containing the image, the attributions, the band mask, the band names, and the score of the interpretable model used for the explanation.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the Lime object is not initialized or is not an instance of LimeBase.</p> <code>AssertionError</code> <p>If explainable model type is <code>segmentation</code> and <code>postprocessing_segmentation_output</code> is not provided.</p> <code>AssertionError</code> <p>If the hsi is not an instance of the HSI class.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; simple_model = lambda x: torch.rand((x.shape[0], 2))\n&gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n&gt;&gt;&gt; band_mask = torch.randint(1, 4, (4, 1, 1)).repeat(1, 240, 240)\n&gt;&gt;&gt; band_names = [\"R\", \"G\", \"B\"]\n&gt;&gt;&gt; lime = meteors.attr.Lime(\n        explainable_model=ExplainableModel(simple_model, \"regression\"), interpretable_model=SkLearnLasso(alpha=0.1)\n    )\n&gt;&gt;&gt; spectral_attribution = lime.get_spectral_attributes(hsi, band_mask=band_mask, band_names=band_names, target=0)\n&gt;&gt;&gt; spectral_attribution.hsi\nHSI(shape=(4, 240, 240), dtype=torch.float32)\n&gt;&gt;&gt; spectral_attribution.attributes.shape\ntorch.Size([4, 240, 240])\n&gt;&gt;&gt; spectral_attribution.band_mask.shape\ntorch.Size([4, 240, 240])\n&gt;&gt;&gt; spectral_attribution.band_names\n[\"R\", \"G\", \"B\"]\n&gt;&gt;&gt; spectral_attribution.score\n1.0\n</code></pre> Source code in <code>src/meteors/attr/lime.py</code> <pre><code>def get_spectral_attributes(\n    self,\n    hsi: HSI,\n    band_mask: np.ndarray | torch.Tensor | None = None,\n    target=None,\n    n_samples: int = 10,\n    perturbations_per_eval: int = 4,\n    verbose: bool = False,\n    postprocessing_segmentation_output: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None = None,\n    band_names: list[str | list[str]] | dict[tuple[str, ...] | str, int] | None = None,\n) -&gt; HSISpectralAttributes:\n    \"\"\"\n    Attributes the hsi image using LIME method for spectral data. Based on the provided hsi and band mask, the LIME\n    method attributes the hsi based on `superbands` (clustered bands) provided by the band mask.\n    Please refer to the original paper `https://arxiv.org/abs/1602.04938` for more details or to\n    Christoph Molnar's book `https://christophm.github.io/interpretable-ml-book/lime.html`.\n\n    The function returns a HSISpectralAttributes object that contains the image, the attributions, the band mask,\n    the band names, and the score of the interpretable model used for the explanation.\n\n    Args:\n        hsi (HSI): An HSI object for which the attribution is performed.\n        band_mask (np.ndarray | torch.Tensor | None, optional): Band mask that is used for the spectral attribution.\n            The band mask should have a 3D shape, which can be broadcastable to the shape of the input image.\n            The only dimensions on which the image and the mask shapes can differ is the height and width dimensions, marked with letters `H` and `W` in the `image.orientation` parameter.\n            If equals to None, the band mask is created within the function. Defaults to None.\n        target (int, optional): If the model creates more than one output, it analyzes the given target.\n            Defaults to None.\n        n_samples (int, optional): The number of samples to generate/analyze in LIME. The more the better but slower. Defaults to 10.\n        perturbations_per_eval (int, optional): The number of perturbations to evaluate at once (Simply the inner batch size).\n            Defaults to 4.\n        verbose (bool, optional): Specifies whether to show progress during the attribution process. Defaults to False.\n        postprocessing_segmentation_output: (Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None):\n            A segmentation postprocessing function for segmentation problem type. This is required for segmentation problem type as\n            lime surrogate model needs to be optimized on the 1d output, and the model should be able to modify the model output with\n            inner lime active region mask as input and return the 1d output (for example number of pixel for each class) and not class mask.\n            Defaults to None.\n        band_names (list[str] | dict[str | tuple[str, ...], int] | None, optional): Band names. Defaults to None.\n\n    Returns:\n        HSISpectralAttributes: A HSISpectralAttributes object containing the image, the attributions,\n            the band mask, the band names, and the score of the interpretable model used for the explanation.\n\n    Raises:\n        ValueError: If the Lime object is not initialized or is not an instance of LimeBase.\n        AssertionError: If explainable model type is `segmentation` and `postprocessing_segmentation_output` is not provided.\n        AssertionError: If the hsi is not an instance of the HSI class.\n\n    Examples:\n        &gt;&gt;&gt; simple_model = lambda x: torch.rand((x.shape[0], 2))\n        &gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n        &gt;&gt;&gt; band_mask = torch.randint(1, 4, (4, 1, 1)).repeat(1, 240, 240)\n        &gt;&gt;&gt; band_names = [\"R\", \"G\", \"B\"]\n        &gt;&gt;&gt; lime = meteors.attr.Lime(\n                explainable_model=ExplainableModel(simple_model, \"regression\"), interpretable_model=SkLearnLasso(alpha=0.1)\n            )\n        &gt;&gt;&gt; spectral_attribution = lime.get_spectral_attributes(hsi, band_mask=band_mask, band_names=band_names, target=0)\n        &gt;&gt;&gt; spectral_attribution.hsi\n        HSI(shape=(4, 240, 240), dtype=torch.float32)\n        &gt;&gt;&gt; spectral_attribution.attributes.shape\n        torch.Size([4, 240, 240])\n        &gt;&gt;&gt; spectral_attribution.band_mask.shape\n        torch.Size([4, 240, 240])\n        &gt;&gt;&gt; spectral_attribution.band_names\n        [\"R\", \"G\", \"B\"]\n        &gt;&gt;&gt; spectral_attribution.score\n        1.0\n    \"\"\"\n\n    if self._attribution_method is None or not isinstance(self._attribution_method, LimeBase):\n        raise ValueError(\"Lime object not initialized\")  # pragma: no cover\n\n    if self.explainable_model.problem_type == \"segmentation\":\n        assert postprocessing_segmentation_output, (\n            \"postprocessing_segmentation_output is required for segmentation problem type, please provide \"\n            \"the `postprocessing_segmentation_output`. For a reference \"\n            \"we provided an example function to use `agg_segmentation_postprocessing` from `meteors.utils.utils` module\"\n        )\n    elif postprocessing_segmentation_output is not None:\n        logger.warning(\n            \"postprocessing_segmentation_output is provided but the problem is not segmentation, will be ignored\"\n        )\n        postprocessing_segmentation_output = None\n\n    assert isinstance(hsi, HSI), \"hsi should be an instance of HSI class\"\n\n    if band_mask is None:\n        band_mask, band_names = self.get_band_mask(hsi, band_names)\n    band_mask = ensure_torch_tensor(band_mask, \"Band mask should be None, numpy array, or torch tensor\")\n    if band_mask.shape != hsi.image.shape:\n        band_mask = expand_spectral_mask(hsi, band_mask, repeat_dimensions=True)\n    band_mask = band_mask.int()\n\n    if band_names is None:\n        unique_segments = torch.unique(band_mask)\n        band_names = {str(segment): idx for idx, segment in enumerate(unique_segments)}\n    else:\n        # checking consistency of names\n        # unique_segments = torch.unique(band_mask)\n        # if isinstance(band_names, dict):\n        #     assert set(unique_segments).issubset(set(band_names.values())), \"Incorrect band names\"\n        logger.debug(\n            \"Band names are provided and will be used. In the future, there should be an option to validate them.\"\n        )\n\n    band_mask = validate_mask_shape(\"band\", hsi, band_mask)\n\n    hsi = hsi.to(self.device)\n    band_mask = band_mask.to(self.device)\n\n    lime_attributes, score = self._attribution_method.attribute(\n        inputs=hsi.get_image().unsqueeze(0),\n        target=target,\n        feature_mask=band_mask.unsqueeze(0),\n        n_samples=n_samples,\n        perturbations_per_eval=perturbations_per_eval,\n        model_postprocessing=postprocessing_segmentation_output,\n        show_progress=verbose,\n        return_input_shape=True,\n    )\n\n    spectral_attribution = HSISpectralAttributes(\n        hsi=hsi,\n        attributes=lime_attributes.squeeze(0),\n        mask=band_mask.expand_as(hsi.image),\n        band_names=band_names,\n        score=score,\n        attribution_method=\"Lime\",\n    )\n\n    return spectral_attribution\n</code></pre>"},{"location":"reference/#lime-base","title":"Lime Base","text":"<p>The Lime Base class was adapted from the Captum Lime implementation. This adaptation builds upon the original work, extending and customizing it for specific use cases within this project. To see the original implementation, please refer to the Captum repository.</p>"},{"location":"reference/#src.meteors.attr.integrated_gradients.IntegratedGradients","title":"<code>IntegratedGradients</code>","text":"<p>               Bases: <code>Explainer</code></p> <p>IntegratedGradients explainer class for generating attributions using the Integrated Gradients method. The Integrated Gradients method is based on the <code>captum</code> implementation and is an implementation of an idea coming from the original paper on Integrated Gradients, where more details about this method can be found.</p> <p>Attributes:</p> Name Type Description <code>_attribution_method</code> <code>IntegratedGradients</code> <p>The Integrated Gradients method from the <code>captum</code> library.</p> Source code in <code>src/meteors/attr/integrated_gradients.py</code> <pre><code>class IntegratedGradients(Explainer):\n    \"\"\"\n    IntegratedGradients explainer class for generating attributions using the Integrated Gradients method.\n    The Integrated Gradients method is based on the [`captum` implementation](https://captum.ai/docs/extension/integrated_gradients)\n    and is an implementation of an idea coming from the [original paper on Integrated Gradients](https://arxiv.org/pdf/1703.01365),\n    where more details about this method can be found.\n\n    Attributes:\n        _attribution_method (CaptumIntegratedGradients): The Integrated Gradients method from the `captum` library.\n    \"\"\"\n\n    def __init__(self, explainable_model: ExplainableModel, multiply_by_inputs: bool = True):\n        super().__init__(explainable_model)\n        self._attribution_method = CaptumIntegratedGradients(\n            explainable_model.forward_func, multiply_by_inputs=multiply_by_inputs\n        )\n\n    def attribute(\n        self,\n        hsi: HSI,\n        baseline: int | float | torch.Tensor = None,\n        target: int | None = None,\n        method: Literal[\n            \"riemann_right\", \"riemann_left\", \"riemann_middle\", \"riemann_trapezoid\", \"gausslegendre\"\n        ] = \"gausslegendre\",\n        return_convergence_delta: bool = False,\n    ) -&gt; HSIAttributes:\n        if self._attribution_method is None:\n            raise ValueError(\"IntegratedGradients explainer is not initialized\")\n\n        baseline = validate_and_transform_baseline(baseline, hsi)\n\n        ig_attributions = self._attribution_method.attribute(\n            hsi.get_image().unsqueeze(0),\n            baselines=baseline.unsqueeze(0),\n            target=target,\n            method=method,\n            return_convergence_delta=return_convergence_delta,\n        )\n        if return_convergence_delta:\n            attributions, approximation_error_tensor = ig_attributions\n\n            # ig_attributions is a tuple of attributions and approximation_error_tensor, where tensor has the same length as the number of example inputs\n            approximation_error = (\n                approximation_error_tensor.item() if target is None else approximation_error_tensor[target].item()\n            )\n\n        else:\n            attributions, approximation_error = ig_attributions, None\n\n        attributes = HSIAttributes(\n            hsi=hsi,\n            attributes=attributions.squeeze(0),\n            score=approximation_error,\n            attribution_method=self.get_name(),\n        )\n\n        return attributes\n</code></pre>"},{"location":"reference/#src.meteors.attr.input_x_gradients.InputXGradient","title":"<code>InputXGradient</code>","text":"<p>               Bases: <code>Explainer</code></p> <p>Initializes the InputXGradient explainer. The InputXGradients method is a straightforward approach to computing attribution. It simply multiplies the input image with the gradient with respect to the input. This method is based on the <code>captum</code> implementation</p> <p>Attributes:</p> Name Type Description <code>_attribution_method</code> <code>CaptumIntegratedGradients</code> <p>The InputXGradient method from the <code>captum</code> library.</p> Source code in <code>src/meteors/attr/input_x_gradients.py</code> <pre><code>class InputXGradient(Explainer):\n    \"\"\"\n    Initializes the InputXGradient explainer. The InputXGradients method is a straightforward approach to\n    computing attribution. It simply multiplies the input image with the gradient with respect to the input.\n    This method is based on the [`captum` implementation](https://captum.ai/api/input_x_gradient.html)\n\n    Attributes:\n        _attribution_method (CaptumIntegratedGradients): The InputXGradient method from the `captum` library.\n    \"\"\"\n\n    def __init__(self, explainable_model: ExplainableModel):\n        super().__init__(explainable_model)\n        self._attribution_method = CaptumInputXGradient(explainable_model.forward_func)\n\n    def attribute(\n        self,\n        hsi: HSI,\n        target: int | None = None,\n        additional_forward_args: Any = None,\n    ) -&gt; HSIAttributes:\n        if self._attribution_method is None:\n            raise ValueError(\"InputXGradient explainer is not initialized\")\n\n        gradient_attribution = self._attribution_method.attribute(\n            hsi.get_image().unsqueeze(0), target=target, additional_forward_args=additional_forward_args\n        )\n        attributes = HSIAttributes(\n            hsi=hsi, attributes=gradient_attribution.squeeze(0), attribution_method=self.get_name()\n        )\n\n        return attributes\n</code></pre>"},{"location":"reference/#src.meteors.attr.occlusion.Occlusion","title":"<code>Occlusion</code>","text":"<p>               Bases: <code>Explainer</code></p> <p>Occlusion explainer class for generating attributions using the Occlusion method. This attribution method perturbs the input by replacing the contiguous rectangular region with a given baseline and computing the difference in output. In our case, features are located in multiple regions, and attribution from different hyper-rectangles is averaged. The implementation of this method is also based on the <code>captum</code> repository. More details about this approach can be found in the original paper</p> <p>Attributes:</p> Name Type Description <code>_attribution_method</code> <code>Occlusion</code> <p>The Occlusion method from the <code>captum</code> library.</p> Source code in <code>src/meteors/attr/occlusion.py</code> <pre><code>class Occlusion(Explainer):\n    \"\"\"\n    Occlusion explainer class for generating attributions using the Occlusion method.\n    This attribution method perturbs the input by replacing the contiguous rectangular region\n    with a given baseline and computing the difference in output.\n    In our case, features are located in multiple regions, and attribution from different hyper-rectangles is averaged.\n    The implementation of this method is also based on the [`captum` repository](https://captum.ai/api/occlusion.html).\n    More details about this approach can be found in the [original paper](https://arxiv.org/abs/1311.2901)\n\n    Attributes:\n        _attribution_method (CaptumOcclusion): The Occlusion method from the `captum` library.\n    \"\"\"\n\n    def __init__(self, explainable_model: ExplainableModel):\n        super().__init__(explainable_model)\n        self._attribution_method = CaptumOcclusion(explainable_model.forward_func)\n\n    def attribute(\n        self,\n        hsi: HSI,\n        sliding_window_shapes,\n        strides=None,  # TODO add default value\n        target: int | None = None,\n        baseline: int | float | torch.Tensor = None,\n        additional_forward_args: Any = None,\n        perturbations_per_eval: int = 1,\n        show_progress: bool = False,\n    ) -&gt; HSIAttributes:\n        if self._attribution_method is None:\n            raise ValueError(\"Occlusion explainer is not initialized\")\n\n        baseline = validate_and_transform_baseline(baseline, hsi)\n\n        occlusion_attributions = self._attribution_method.attribute(\n            hsi.get_image().unsqueeze(0),\n            sliding_window_shapes=(1,)\n            + sliding_window_shapes,  # I'am not sure about this scaling method - need to check how exactly occlusion modifies the image shape\n            strides=(1,) + strides,\n            target=target,\n            baselines=baseline.unsqueeze(0),\n            additional_forward_args=additional_forward_args,\n            perturbations_per_eval=perturbations_per_eval,\n            show_progress=show_progress,\n        )\n        occlusion_attributions = occlusion_attributions.squeeze(0)\n        attributes = HSIAttributes(hsi=hsi, attributes=occlusion_attributions, attribution_method=self.get_name())\n\n        return attributes\n</code></pre>"},{"location":"reference/#src.meteors.attr.saliency.Saliency","title":"<code>Saliency</code>","text":"<p>               Bases: <code>Explainer</code></p> <p>Saliency explainer class for generating attributions using the Saliency method. This baseline method for computing input attribution calculates gradients with respect to inputs. It also has an option to return the absolute value of the gradients, which is the default behaviour. Implementation of this method is based on the <code>captum</code> repository</p> <p>Attributes:</p> Name Type Description <code>_attribution_method</code> <code>Saliency</code> <p>The Saliency method from the <code>captum</code> library.</p> Source code in <code>src/meteors/attr/saliency.py</code> <pre><code>class Saliency(Explainer):\n    \"\"\"\n    Saliency explainer class for generating attributions using the Saliency method.\n    This baseline method for computing input attribution calculates gradients with respect to inputs.\n    It also has an option to return the absolute value of the gradients, which is the default behaviour.\n    Implementation of this method is based on the [`captum` repository](https://captum.ai/api/saliency.html)\n\n    Attributes:\n        _attribution_method (CaptumSaliency): The Saliency method from the `captum` library.\n    \"\"\"\n\n    def __init__(\n        self,\n        explainable_model: ExplainableModel,\n    ):\n        super().__init__(explainable_model)\n\n        self._attribution_method = CaptumSaliency(explainable_model.forward_func)\n\n    def attribute(\n        self,\n        hsi: HSI,\n        target: int | None = None,\n        abs: bool = True,\n        additional_forward_args: Any = None,\n    ) -&gt; HSIAttributes:\n        if self._attribution_method is None:\n            raise ValueError(\"Saliency explainer is not initialized\")\n\n        saliency_attributions = self._attribution_method.attribute(\n            hsi.get_image().unsqueeze(0), target=target, abs=abs, additional_forward_args=additional_forward_args\n        )\n        attributes = HSIAttributes(\n            hsi=hsi, attributes=saliency_attributions.squeeze(0), attribution_method=self.get_name()\n        )\n\n        return attributes\n</code></pre>"},{"location":"reference/#src.meteors.attr.noise_tunnel.NoiseTunnel","title":"<code>NoiseTunnel</code>","text":"<p>               Bases: <code>Explainer</code></p> <p>NoiseTunnel explainer class for generating attributions using the Noise Tunnel method. This attribution method works on top of a different one to better approximate its explanations. The Noise Tunnel (Smooth Grad) adds Gaussian noise to each input in the batch and applies the given attribution algorithm to each modified sample. This method is based on the <code>captum</code> implementation</p> <p>Attributes:</p> Name Type Description <code>_attribution_method</code> <code>NoiseTunnel</code> <p>The Noise Tunnel method from the <code>captum</code> library.</p> Source code in <code>src/meteors/attr/noise_tunnel.py</code> <pre><code>class NoiseTunnel(Explainer):\n    \"\"\"\n    NoiseTunnel explainer class for generating attributions using the Noise Tunnel method.\n    This attribution method works on top of a different one to better approximate its explanations.\n    The Noise Tunnel (Smooth Grad) adds Gaussian noise to each input in the batch and applies the given attribution algorithm to each modified sample.\n    This method is based on the [`captum` implementation](https://captum.ai/api/noise_tunnel.html)\n\n    Attributes:\n        _attribution_method (CaptumNoiseTunnel): The Noise Tunnel method from the `captum` library.\n    \"\"\"\n\n    def __init__(self, attribution_method: Explainer):\n        super().__init__(attribution_method)\n        validate_attribution_method_initialization(attribution_method)\n        self._attribution_method = CaptumNoiseTunnel(attribution_method._attribution_method)\n\n    def attribute(\n        self,\n        hsi: HSI,\n        target: int | None = None,\n        nt_type=\"smoothgrad\",\n        nt_samples=5,\n        nt_samples_batch_size=None,\n        stdevs=1.0,\n        draw_baseline_from_distrib=False,\n    ) -&gt; HSIAttributes:\n        if self._attribution_method is None:\n            raise ValueError(\"NoiseTunnel explainer is not initialized\")\n        if self.chained_explainer is None:\n            raise ValueError(\n                f\"The attribution method {self.chained_explainer.__class__.__name__} is not properly initialized\"\n            )\n\n        noise_tunnel_attributes = self._attribution_method.attribute(\n            hsi.get_image().unsqueeze(0),\n            target=target,\n            nt_type=nt_type,\n            nt_samples=nt_samples,\n            nt_samples_batch_size=nt_samples_batch_size,\n            stdevs=stdevs,\n            draw_baseline_from_distrib=draw_baseline_from_distrib,\n        )\n\n        attributes = HSIAttributes(\n            hsi=hsi, attributes=noise_tunnel_attributes.squeeze(0), attribution_method=self.get_name()\n        )\n\n        return attributes\n</code></pre>"},{"location":"reference/#hyper-noise-tunnel","title":"Hyper Noise Tunnel","text":""},{"location":"reference/#src.meteors.attr.hyper_noise_tunnel.HyperNoiseTunnel","title":"<code>HyperNoiseTunnel</code>","text":"<p>               Bases: <code>Explainer</code></p> <p>Hyper Noise Tunnel is our novel method, designed specifically to explain hyperspectral satellite images. It is inspired by the behaviour of the classical Noise Tunnel (Smooth Grad) method, but instead of sampling noise into the original image, it randomly removes some of the bands. In the process, the created noised samples are close to the distribution of the original image yet differ enough to smoothen the produced attribution map.</p> <p>Attributes:</p> Name Type Description <code>_attribution_method</code> <code>BaseHyperNoiseTunnel</code> <p>The Hyper Noise Base Tunnel method</p> Source code in <code>src/meteors/attr/hyper_noise_tunnel.py</code> <pre><code>class HyperNoiseTunnel(Explainer):\n    \"\"\"Hyper Noise Tunnel is our novel method, designed specifically to explain hyperspectral satellite images. It is\n    inspired by the behaviour of the classical Noise Tunnel (Smooth Grad) method, but instead of sampling noise into the\n    original image, it randomly removes some of the bands. In the process, the created _noised_ samples are close to the\n    distribution of the original image yet differ enough to smoothen the produced attribution map.\n\n    Attributes:\n        _attribution_method (BaseHyperNoiseTunnel): The Hyper Noise Base Tunnel method\n    \"\"\"\n\n    def __init__(self, attribution_method):\n        super().__init__(attribution_method)\n        validate_attribution_method_initialization(attribution_method)\n        self._attribution_method: Attribution = BaseHyperNoiseTunnel(attribution_method._attribution_method)\n\n    def attribute(\n        self,\n        hsi: HSI,\n        baselines: int | float | Tensor | None = None,\n        target: int | None = None,\n        n_samples: int = 5,\n        steps_per_batch: int = 1,\n        perturbation_prob: float = 0.5,\n        num_perturbed_bands: int | None = None,\n        method: Literal[\"smoothgrad\", \"smoothgrad_sq\", \"vargrad\"] = \"smoothgrad\",\n    ):\n        change_orientation = False\n        original_orientation = (\"C\", \"H\", \"W\")\n\n        if hsi.orientation != (\"C\", \"H\", \"W\"):\n            change_orientation = True\n            original_orientation = hsi.orientation\n            logger.warning(f\"The hsi orientation is {hsi.orientation}. Switching the orientation to ('C', 'H', 'W')\")\n            hsi = hsi.change_orientation(\"CHW\")\n\n        baselines = validate_and_transform_baseline(baselines, hsi)\n\n        attributes = self._attribution_method.attribute(\n            hsi.image,\n            baselines=baselines,\n            target=target,\n            n_samples=n_samples,\n            steps_per_batch=steps_per_batch,\n            method=method,\n            perturbation_prob=perturbation_prob,\n            num_perturbed_bands=num_perturbed_bands,\n        )\n        attributes = attributes.squeeze(0)\n\n        hsi_attributes = HSIAttributes(hsi=hsi, attributes=attributes, attribution_method=self.get_name())\n\n        # change back the attributes orientation\n        if change_orientation:\n            hsi_attributes = hsi_attributes.change_orientation(original_orientation)\n\n        return hsi_attributes\n</code></pre>"},{"location":"reference/#src.meteors.attr.hyper_noise_tunnel.BaseHyperNoiseTunnel","title":"<code>BaseHyperNoiseTunnel</code>","text":"<p>               Bases: <code>Attribution</code></p> <p>Compute the attribution of the given inputs using the hyper noise tunnel method.</p> Source code in <code>src/meteors/attr/hyper_noise_tunnel.py</code> <pre><code>class BaseHyperNoiseTunnel(Attribution):\n    \"\"\"Compute the attribution of the given inputs using the hyper noise tunnel method.\"\"\"\n\n    def __init__(self, model: GradientAttribution):\n        self.attribute_main = model.attribute\n        sig = inspect.signature(self.attribute_main)\n        if \"abs\" in sig.parameters:\n            self.attribute_main = partial(self.attribute_main, abs=False)\n\n    @staticmethod\n    def perturb_input(\n        input, baseline, n_samples: int = 1, perturbation_prob: float = 0.5, num_perturbed_bands: int | None = None\n    ) -&gt; torch.Tensor:\n        \"\"\"The perturbation function used in the hyper noise tunnel. It randomly selects a subset of the input bands\n        that will be masked out and replaced with the baseline. The parameters `num_perturbed_bands` and\n        `perturbation_prob` control the number of bands that will be perturbed (masked). If `num_perturbed_bands` is\n        set, it will be used as the number of bands to perturb, which will be randomly selected. Otherwise, the number\n        of bands will be drawn from a binomial distribution with `perturbation_prob` as the probability of success.\n\n        Args:\n            input (torch.Tensor): An input tensor to be perturbed. It should have the shape (C, H, W).\n            baseline (torch.Tensor): A baseline tensor to replace the perturbed bands.\n            n_samples (int): A number of samples to be drawn - number of perturbed inputs to be generated.\n            perturbation_prob (float, optional): A probability that each band will be perturbed intependently. Defaults to 0.5.\n            num_perturbed_bands (int | None, optional): A number of perturbed bands in the whole image. If set to None, the bands are perturbed with probability `perturbation_prob` each. Defaults to None.\n\n        Returns:\n            torch.Tensor: A perturbed tensor, which contains `n_samples` perturbed inputs.\n        \"\"\"\n        # validate the baseline against the input\n        if baseline.shape != input.shape:\n            raise ValueError(f\"Baseline shape {baseline.shape} does not match input shape {input.shape}\")\n\n        if n_samples &lt; 1:\n            raise ValueError(\"Number of perturbated samples to be generated must be greater than 0\")\n\n        if perturbation_prob &lt; 0 or perturbation_prob &gt; 1:\n            raise ValueError(\"Perturbation probability must be in the range [0, 1]\")\n\n        # the perturbation\n        perturbed_input = input.clone().unsqueeze(0)\n        # repeat the perturbed_input on the first dimension n_samples times\n        perturbed_input = perturbed_input.repeat_interleave(n_samples, dim=0)\n\n        n_samples_x_channels_shape = (\n            n_samples,\n            input.shape[0],\n        )  # shape of the tensor containing the perturbed channels for each sample\n\n        channels_to_be_perturbed: torch.Tensor = torch.zeros(n_samples_x_channels_shape, device=input.device).bool()\n\n        if num_perturbed_bands is None:\n            channel_perturbation_probabilities = (\n                torch.ones(n_samples_x_channels_shape, device=input.device) * perturbation_prob\n            )\n            channels_to_be_perturbed = torch.bernoulli(channel_perturbation_probabilities).bool()\n\n        else:\n            if num_perturbed_bands &lt; 0 or num_perturbed_bands &gt; input.shape[0]:\n                raise ValueError(\n                    f\"Cannot perturb {num_perturbed_bands} bands in the input with {input.shape[0]} channels. The number of perturbed bands must be in the range [0, {input.shape[0]}]\"\n                )\n\n            channels_to_be_perturbed = torch_random_choice(\n                input.shape[0], num_perturbed_bands, n_samples, device=input.device\n            )\n\n        # now having chosen the perturbed channels, we can replace them with the baseline\n\n        reshaped_baseline = baseline.unsqueeze(0).repeat_interleave(n_samples, dim=0)\n        perturbed_input[channels_to_be_perturbed] = reshaped_baseline[channels_to_be_perturbed]\n\n        perturbed_input.requires_grad_(True)\n\n        return perturbed_input\n\n    def attribute(\n        self,\n        inputs: Tensor,\n        baselines: Union[Tensor, int, float],\n        target: int | None = None,\n        additional_forward_args: Any = None,\n        n_samples: int = 5,\n        steps_per_batch: int = 1,\n        method: str = \"smoothgrad\",\n        perturbation_prob: float = 0.5,\n        num_perturbed_bands: int | None = None,\n    ) -&gt; Tensor:\n        if method not in [\"smoothgrad\", \"smoothgrad_sq\", \"vargrad\"]:\n            raise ValueError(\"Method must be one of 'smoothgrad', 'smoothgrad_sq', 'vargrad'\")\n\n        if inputs.dim() == 3:\n            inputs = inputs.unsqueeze(0)\n        elif inputs.dim() != 4:\n            raise ValueError(\"Input must be in the format (N, C, H, W)\")\n\n        if not isinstance(baselines, Tensor) and not isinstance(baselines, int) and not isinstance(baselines, float):\n            raise ValueError(\"Baselines must be a tensor or a scalar\")\n\n        if isinstance(baselines, int) or isinstance(baselines, float):\n            baselines = torch.zeros_like(inputs, device=inputs.device).squeeze(0) + baselines\n        elif baselines.dim() == 4:\n            baselines = baselines.squeeze(0)\n        elif baselines.dim() != 3:\n            raise ValueError(\"Baselines must be in the format (C, H, W)\")\n\n        attributions = torch.empty((n_samples,) + inputs.shape, device=inputs.device)\n\n        for batch in range(0, inputs.shape[0]):\n            input = inputs[batch]\n            perturbed_input = BaseHyperNoiseTunnel.perturb_input(\n                input, baselines, n_samples, perturbation_prob, num_perturbed_bands\n            )\n            for i in range(0, n_samples, steps_per_batch):\n                perturbed_batch = perturbed_input[i : i + steps_per_batch]\n                attributions[i : i + steps_per_batch, batch] = self.attribute_main(\n                    perturbed_batch, target=target, additional_forward_args=additional_forward_args\n                )\n            else:\n                steps_left = n_samples % steps_per_batch\n                if steps_left:\n                    perturbed_batch = perturbed_input[-steps_left:]\n                    attributions[-steps_left:, batch] = self.attribute_main(\n                        perturbed_batch, target=target, additional_forward_args=additional_forward_args\n                    )\n\n        if method == \"smoothgrad\":\n            return attributions.mean(dim=0)\n        elif method == \"smoothgrad_sq\":\n            return (attributions**2).mean(dim=0)\n        else:\n            return (attributions**2 - attributions.mean(dim=0) ** 2).mean(dim=0)\n</code></pre>"},{"location":"reference/#src.meteors.attr.hyper_noise_tunnel.BaseHyperNoiseTunnel.perturb_input","title":"<code>perturb_input(input, baseline, n_samples=1, perturbation_prob=0.5, num_perturbed_bands=None)</code>  <code>staticmethod</code>","text":"<p>The perturbation function used in the hyper noise tunnel. It randomly selects a subset of the input bands that will be masked out and replaced with the baseline. The parameters <code>num_perturbed_bands</code> and <code>perturbation_prob</code> control the number of bands that will be perturbed (masked). If <code>num_perturbed_bands</code> is set, it will be used as the number of bands to perturb, which will be randomly selected. Otherwise, the number of bands will be drawn from a binomial distribution with <code>perturbation_prob</code> as the probability of success.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>An input tensor to be perturbed. It should have the shape (C, H, W).</p> required <code>baseline</code> <code>Tensor</code> <p>A baseline tensor to replace the perturbed bands.</p> required <code>n_samples</code> <code>int</code> <p>A number of samples to be drawn - number of perturbed inputs to be generated.</p> <code>1</code> <code>perturbation_prob</code> <code>float</code> <p>A probability that each band will be perturbed intependently. Defaults to 0.5.</p> <code>0.5</code> <code>num_perturbed_bands</code> <code>int | None</code> <p>A number of perturbed bands in the whole image. If set to None, the bands are perturbed with probability <code>perturbation_prob</code> each. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A perturbed tensor, which contains <code>n_samples</code> perturbed inputs.</p> Source code in <code>src/meteors/attr/hyper_noise_tunnel.py</code> <pre><code>@staticmethod\ndef perturb_input(\n    input, baseline, n_samples: int = 1, perturbation_prob: float = 0.5, num_perturbed_bands: int | None = None\n) -&gt; torch.Tensor:\n    \"\"\"The perturbation function used in the hyper noise tunnel. It randomly selects a subset of the input bands\n    that will be masked out and replaced with the baseline. The parameters `num_perturbed_bands` and\n    `perturbation_prob` control the number of bands that will be perturbed (masked). If `num_perturbed_bands` is\n    set, it will be used as the number of bands to perturb, which will be randomly selected. Otherwise, the number\n    of bands will be drawn from a binomial distribution with `perturbation_prob` as the probability of success.\n\n    Args:\n        input (torch.Tensor): An input tensor to be perturbed. It should have the shape (C, H, W).\n        baseline (torch.Tensor): A baseline tensor to replace the perturbed bands.\n        n_samples (int): A number of samples to be drawn - number of perturbed inputs to be generated.\n        perturbation_prob (float, optional): A probability that each band will be perturbed intependently. Defaults to 0.5.\n        num_perturbed_bands (int | None, optional): A number of perturbed bands in the whole image. If set to None, the bands are perturbed with probability `perturbation_prob` each. Defaults to None.\n\n    Returns:\n        torch.Tensor: A perturbed tensor, which contains `n_samples` perturbed inputs.\n    \"\"\"\n    # validate the baseline against the input\n    if baseline.shape != input.shape:\n        raise ValueError(f\"Baseline shape {baseline.shape} does not match input shape {input.shape}\")\n\n    if n_samples &lt; 1:\n        raise ValueError(\"Number of perturbated samples to be generated must be greater than 0\")\n\n    if perturbation_prob &lt; 0 or perturbation_prob &gt; 1:\n        raise ValueError(\"Perturbation probability must be in the range [0, 1]\")\n\n    # the perturbation\n    perturbed_input = input.clone().unsqueeze(0)\n    # repeat the perturbed_input on the first dimension n_samples times\n    perturbed_input = perturbed_input.repeat_interleave(n_samples, dim=0)\n\n    n_samples_x_channels_shape = (\n        n_samples,\n        input.shape[0],\n    )  # shape of the tensor containing the perturbed channels for each sample\n\n    channels_to_be_perturbed: torch.Tensor = torch.zeros(n_samples_x_channels_shape, device=input.device).bool()\n\n    if num_perturbed_bands is None:\n        channel_perturbation_probabilities = (\n            torch.ones(n_samples_x_channels_shape, device=input.device) * perturbation_prob\n        )\n        channels_to_be_perturbed = torch.bernoulli(channel_perturbation_probabilities).bool()\n\n    else:\n        if num_perturbed_bands &lt; 0 or num_perturbed_bands &gt; input.shape[0]:\n            raise ValueError(\n                f\"Cannot perturb {num_perturbed_bands} bands in the input with {input.shape[0]} channels. The number of perturbed bands must be in the range [0, {input.shape[0]}]\"\n            )\n\n        channels_to_be_perturbed = torch_random_choice(\n            input.shape[0], num_perturbed_bands, n_samples, device=input.device\n        )\n\n    # now having chosen the perturbed channels, we can replace them with the baseline\n\n    reshaped_baseline = baseline.unsqueeze(0).repeat_interleave(n_samples, dim=0)\n    perturbed_input[channels_to_be_perturbed] = reshaped_baseline[channels_to_be_perturbed]\n\n    perturbed_input.requires_grad_(True)\n\n    return perturbed_input\n</code></pre>"},{"location":"tutorials/introduction/","title":"\ud83c\udf93 Introduction to Tutorials","text":"<p>Welcome to the Meteors tutorials! These tutorials are designed to help you get started with using Meteors for explaining and visualizing hyperspectral and multispectral images. Whether you're new to Meteors or have some experience, these tutorials will guide you through various features and techniques step by step.</p>"},{"location":"tutorials/introduction/#tutorial-list","title":"\ud83d\udcda Tutorial List","text":"<ol> <li> <p>LIME</p> </li> <li> <p>HYPERVIEW Challenge</p> </li> <li>Local Interpretable Model-agnostic Explanations (LIME) for hyperspectral images</li> <li>Vision Transformer (ViT) model for hyperspectral image regression</li> </ol>"},{"location":"tutorials/introduction/#getting-started","title":"\ud83d\ude80 Getting Started","text":"<p>To get started with the tutorials, make sure you have Meteors installed and set up on your machine. If you haven't done so already, follow the installation instructions in the Quickstart tutorial.</p> <p>Each tutorial is self-contained and focuses on a specific topic or technique. You can follow the tutorials in order or jump to the ones that interest you the most. The tutorials provide step-by-step instructions, code examples, and explanations to help you understand and apply the concepts effectively.</p>"},{"location":"tutorials/introduction/#tips-and-tricks","title":"\ud83d\udca1 Tips and Tricks","text":"<ul> <li>Take your time and experiment with the code examples provided in the tutorials.</li> <li>Don't hesitate to modify the code and try different parameters to see how they affect the results.</li> <li>Refer to the API Reference for detailed information on the Meteors functions and classes used in the tutorials.</li> <li>If you encounter any issues or have questions, feel free to reach out to the Meteors community or open an issue on the GitHub repository.</li> </ul>"},{"location":"tutorials/introduction/#lets-get-started","title":"\ud83c\udf89 Let's Get Started!","text":"<p>Ready to dive into the world of hyperspectral image explanation with Meteors? Choose a tutorial from the list above and start your journey!</p> <p>Happy learning and exploring with Meteors! \u2604\ufe0f\ud83d\udd0d\u2728</p>"},{"location":"tutorials/lime/","title":"Interpreting Hyperspectral Images with LIME: HYPERVIEW Challenge","text":"<p>This notebook demonstrates the usability of the Local Interpretable Model-agnostic Explanations (LIME) algorithm to interpret the predictions of a hyperspectral image regression model. The dataset used in this notebook is from the HYPERVIEW Challenge organized by The European Space Agency (ESA). The goal of the challenge is to predict soil parameters based on the hyperspectral images of the ground.</p> <p>The model used in this notebook is one of the top-performing models in the challenge. The trained model architecture is based on the Vision Transformer (ViT) and CLIP (Contrastive Language-Image Pretraining), and its fine-tuned weights are open-sourced under the Apache License in the Hugging Face Model Hub. In the same place the original implementation of the CLIP model can be found. </p> <p>Note: Before running this notebook, make sure to install the required libraries used in the notebook. It should be sufficient to install the package <code>meteors</code> from PyPI, as it carry all the required dependencies, but in some cases, you might need to install additional ones. The <code>clip_model</code> module contains the code needed for additional preprocessing and model loading and can be downloaded from the Vignettes in the <code>meteors</code> repository</p>"},{"location":"tutorials/lime/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Loading the Model</li> <li>2. Loading the Hyperspectral data from HYPERVIEW Challenge</li> <li>3. Convert data into HSI image and preview the images</li> <li>4. Analyze HSI data with LIME</li> <li>4.1. Spatial Analysis</li> <li>4.2. Spectral Analysis</li> </ul> <pre><code>import torch\nimport numpy as np\nfrom tqdm import tqdm\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport meteors as mt\n\nfrom clip_model import build_model\n\ntorch.manual_seed(0)  # set seed for reproducibility\n</code></pre> <pre><code>&lt;torch._C.Generator at 0x24d39bf3290&gt;\n</code></pre> <pre><code>device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n</code></pre>"},{"location":"tutorials/lime/#1-loading-the-model","title":"1. Loading the Model","text":"<p>The dataset used for training this model can be found on the official page for the HYPERVIEW Challenge.</p> <p>The following code snippet includes additional functions to load pre-trained CLIP weights and fine-tuned weights for the model.</p> <pre><code>import os\nimport hashlib\nimport urllib\nimport warnings\n\n\nBASE_MODEL_URL = \"https://openaipublic.azureedge.net/clip/models/b8cca3fd41ae0c99ba7e8951adf17d267cdb84cd88be6f7c2e0eca1737a03836/ViT-L-14.pt\"\n\n\ndef download(url: str, root: str, error_checksum: bool = True) -&gt; str:\n    os.makedirs(root, exist_ok=True)\n    filename = os.path.basename(url)\n\n    expected_sha256 = url.split(\"/\")[-2]\n    if expected_sha256 == \"main\":\n        expected_sha256 = \"0cc03ba20aff35a41312de47da843a0d8541acba3c2101d9691f3ab999128d34\"  # CLIP sha256\n    download_target = os.path.join(root, filename)\n\n    if os.path.exists(download_target) and not os.path.isfile(download_target):\n        raise RuntimeError(f\"{download_target} exists and is not a regular file\")\n\n    if os.path.isfile(download_target):\n        # real_sha256 = hashlib.sha256(open(download_target, \"rb\").read()).hexdigest()\n        # print(\"INFO: Real SHA256: {}\".format(real_sha256))\n        # print(\"INFO: Expected SHA256: {}\".format(expected_sha256))\n        if hashlib.sha256(open(download_target, \"rb\").read()).hexdigest() == expected_sha256:\n            return download_target\n        else:\n            warnings.warn(f\"{download_target} exists, but the SHA256 checksum does not match; re-downloading the file\")\n\n    with urllib.request.urlopen(url) as source, open(download_target, \"wb\") as output:\n        with tqdm(\n            total=int(source.info().get(\"Content-Length\")), ncols=80, unit=\"iB\", unit_scale=True, unit_divisor=1024\n        ) as loop:\n            while True:\n                buffer = source.read(8192)\n                if not buffer:\n                    break\n\n                output.write(buffer)\n                loop.update(len(buffer))\n\n    if hashlib.sha256(open(download_target, \"rb\").read()).hexdigest() != expected_sha256:\n        if error_checksum:\n            raise RuntimeError(\"Model has been downloaded but the SHA256 checksum does not not match\")\n        else:\n            warnings.warn(\"Model has been downloaded but the SHA256 checksum does not not match\")\n\n    return download_target\n\n\ndef load_base_clip(download_root: str, class_num: int = 1000):\n    model_path = download(BASE_MODEL_URL, download_root)\n    model = torch.jit.load(model_path, map_location=\"cpu\").eval()\n    model = build_model(model.state_dict(), downstream_task=4, class_num=class_num)\n    model.float()\n    return model\n</code></pre> <pre><code>download_root = os.path.expanduser(\"~/.cache/clip\")  # Change this to the directory where you want to download the model\nnum_classes = 4  # Number of classes in the original HYPERVIEW dataset\n\n# Load the CLIP model with the HYPERVIEW head\nmodel = load_base_clip(download_root=download_root, class_num=num_classes)\n\n# Load the pre-trained weights\nvit_checkpoint_path = download(\n    \"https://huggingface.co/KPLabs/HYPERVIEW-VisionTransformer/resolve/main/VisionTransformer.pt\",\n    download_root,\n    error_checksum=False,\n)\nmodel.load_state_dict(torch.load(vit_checkpoint_path, map_location=device))\nmodel.eval()\nmodel = model.to(device)\n</code></pre> <pre><code>C:\\Users\\tymot\\AppData\\Local\\Temp\\ipykernel_13308\\1125051604.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(vit_checkpoint_path, map_location=device))\n</code></pre>"},{"location":"tutorials/lime/#2-load-the-hyperspectral-data-from-hyperview-challenge","title":"2. Load the Hyperspectral data from HYPERVIEW Challenge","text":"<p>In this notebook, we will use the sample images from the HYPERVIEW Challenge dataset. The images are stored in the vignettes folder for the lime example in the <code>data</code> folder. The images are in the <code>.npy</code> format. The images are 3D hyperspectral images with 150 bands and various spatial dimensions. The images are stored in the raw format and contain both the hyperspectral image and the mask applied while training the model.</p> <p>First, we need to define the loading and preprocessing functions for the data.</p> <pre><code>def _shape_pad(data):\n    max_edge = np.max(data.shape[1:])\n    shape = (max_edge, max_edge)\n    # padded = np.pad(data, ((0, 0), (0, (shape[0] - data.shape[1])), (0, (shape[1] - data.shape[2]))), \"wrap\")\n    padded = np.pad(\n        data,\n        ((0, 0), (0, (shape[0] - data.shape[1])), (0, (shape[1] - data.shape[2]))),\n        \"constant\",\n        constant_values=0.0,\n    )\n    return padded\n\n\ndef load_single_npz_image(image_path):\n    with np.load(image_path) as npz:\n        data = npz[\"data\"]\n        mask = npz[\"mask\"]\n\n        mask = 1 - mask.astype(int)\n\n        mask = _shape_pad(mask)\n        data = _shape_pad(data)\n\n        mask = mask.transpose((1, 2, 0))\n        data = data.transpose((1, 2, 0))\n        data = data / 5419\n\n        return data, mask\n\n\ndef get_eval_transform(image_shape):\n    return transforms.Compose(\n        [\n            transforms.Resize((image_shape, image_shape)),\n        ]\n    )\n</code></pre> <p>Now, let's load the hyperspectral image using functions we defined earlier.</p> <pre><code>data, mask = load_single_npz_image(\"data/0.npz\")\nmasked_data = data * mask\nmasked_data = torch.from_numpy(masked_data.astype(np.float32)).permute(2, 0, 1)\neval_tr = get_eval_transform(224)\n\nimage_torch = eval_tr(masked_data)\nnot_masked_image_torch = eval_tr(torch.from_numpy(data.astype(np.float32)).permute(2, 0, 1))\n\nprint(f\"Original data shape: {data.shape}\")\nprint(f\"Original mask shape: {mask.shape}\")\nprint(f\"Transformed data shape: {image_torch.shape}\")\n</code></pre> <pre><code>Original data shape: (89, 89, 150)\nOriginal mask shape: (89, 89, 150)\nTransformed data shape: torch.Size([150, 224, 224])\n</code></pre> <p>As we can see, the image is a 3D NumPy array with a shape transformed to (150, 224, 224), where 150 is the number of bands and 224x224 is the spatial dimension of the image. We will keep the image in both masked and unmasked formats.</p> <p>Now, we need to specify the wavelengths of the bands in the image. The wavelengths were provided in the challenge dataset, but to avoid loading additional files, we save it as a simple pythonic list</p> <pre><code>wavelengths = [\n    462.08,\n    465.27,\n    468.47,\n    471.67,\n    474.86,\n    478.06,\n    481.26,\n    484.45,\n    487.65,\n    490.85,\n    494.04,\n    497.24,\n    500.43,\n    503.63,\n    506.83,\n    510.03,\n    513.22,\n    516.42,\n    519.61,\n    522.81,\n    526.01,\n    529.2,\n    532.4,\n    535.6,\n    538.79,\n    541.99,\n    545.19,\n    548.38,\n    551.58,\n    554.78,\n    557.97,\n    561.17,\n    564.37,\n    567.56,\n    570.76,\n    573.96,\n    577.15,\n    580.35,\n    583.55,\n    586.74,\n    589.94,\n    593.14,\n    596.33,\n    599.53,\n    602.73,\n    605.92,\n    609.12,\n    612.32,\n    615.51,\n    618.71,\n    621.91,\n    625.1,\n    628.3,\n    631.5,\n    634.69,\n    637.89,\n    641.09,\n    644.28,\n    647.48,\n    650.67,\n    653.87,\n    657.07,\n    660.27,\n    663.46,\n    666.66,\n    669.85,\n    673.05,\n    676.25,\n    679.45,\n    682.64,\n    685.84,\n    689.03,\n    692.23,\n    695.43,\n    698.62,\n    701.82,\n    705.02,\n    708.21,\n    711.41,\n    714.61,\n    717.8,\n    721.0,\n    724.2,\n    727.39,\n    730.59,\n    733.79,\n    736.98,\n    740.18,\n    743.38,\n    746.57,\n    749.77,\n    752.97,\n    756.16,\n    759.36,\n    762.56,\n    765.75,\n    768.95,\n    772.15,\n    775.34,\n    778.54,\n    781.74,\n    784.93,\n    788.13,\n    791.33,\n    794.52,\n    797.72,\n    800.92,\n    804.11,\n    807.31,\n    810.51,\n    813.7,\n    816.9,\n    820.1,\n    823.29,\n    826.49,\n    829.68,\n    832.88,\n    836.08,\n    839.28,\n    842.47,\n    845.67,\n    848.86,\n    852.06,\n    855.26,\n    858.46,\n    861.65,\n    864.85,\n    868.04,\n    871.24,\n    874.44,\n    877.63,\n    880.83,\n    884.03,\n    887.22,\n    890.42,\n    893.62,\n    896.81,\n    900.01,\n    903.21,\n    906.4,\n    909.6,\n    912.8,\n    915.99,\n    919.19,\n    922.39,\n    925.58,\n    928.78,\n    931.98,\n    935.17,\n    938.37,\n]\n</code></pre>"},{"location":"tutorials/lime/#3-convert-data-into-hsi-image-and-preview-the-images","title":"3. Convert Data into HSI Image and Preview the Images","text":"<p>Now, having the raw data - the tensor representing the image, its wavelengths and the image orientation, we can to combine this information into a complete hyperspectral image. To create the hyperspectral image, we will use the <code>HSI</code> data class from the <code>meteors</code> package.</p> <p>The <code>HSI</code> (HyperSpectral Image) class takes the hyperspectral image data, the wavelength data, and the orientation of the image as input and creates the meaningful hyperspectral image, that can be easily analysed in any downstream task.</p> <p>Additionally, we may provide the binary mask, which may cover data irrelevant for the task, as suggested by the challenge dataset providers. In our case we cover the regions where there is land whose parameters we do not want to estimate, for instance some forests, roads or rivers. If we learned the model on such unmasked data, it could faslely underestimate the predictions, since the measured soil parameters in such uncultivated land is usually lower.</p> <pre><code># Create a binary mask from the image, where 1 is the masked region and 0 is the unmasked region\nbinary_mask = (image_torch &gt; 0.0).int()\n\nhsi_0 = mt.HSI(\n    image=not_masked_image_torch,  # The preprocessed image saved as tensor/numpy ndarray\n    wavelengths=wavelengths,  # The wavelengths list of the hyperspectral image\n    orientation=\"CWH\",  # The orientation of the image tensor, here it is CWH (Channels, Width, Height)\n    binary_mask=binary_mask,  # The binary mask tensor/numpy\n    device=device,  # The device where the image data will be stored. We can provide it later with the .to(device) method\n)\n</code></pre> <p>Now, let's view the hyperspectral image along with the masked and unmasked versions.</p> <pre><code>fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n\nmt.visualize.visualize_hsi(hsi_0, ax1, use_mask=True)\nax1.set_title(\"Masked Image\")\n\nax2.imshow(binary_mask[0, ...].T.cpu().numpy(), cmap=\"gray\")\nax2.set_title(\"Binary Mask\")\n\nmt.visualize.visualize_hsi(hsi_0, ax3, use_mask=False)\nax3.set_title(\"Unmasked Image\")\n\nfig.suptitle(\"Sample image from the HYPERVIEW dataset\")\nplt.show()\n</code></pre> <p></p> <p>The <code>HSI</code> dataclass automatically provides us the clean RGB chart of the hyperspectral image and releases us from the obligation of selecting the specific wavelengths to be plotted and considering the image orientation.</p> <p>Now, we can provide the hyperspectral image to the model and get the prediction. The model will return the predictions for the 4 classes of the hyperspectral image.</p> <pre><code>original_prediction = model(not_masked_image_torch.unsqueeze(0))  # original image\nhsi_prediction = model(hsi_0.image.unsqueeze(0))  # Hsi image\nassert torch.allclose(original_prediction, hsi_prediction, atol=1e-3)\n\nprediction_dict = {0: \"Phosphorus\", 1: \"Potassium\", 2: \"Magnesium\", 3: \"pH\"}  # The classes of the HYPERVIEW dataset\n\npredictions = {prediction_dict[i]: float(hsi_prediction[0, i].cpu().detach().numpy()) for i in range(4)}\npredictions = pd.Series(predictions)\npredictions\n</code></pre> <pre><code>Phosphorus    0.210551\nPotassium     0.350670\nMagnesium     0.391935\npH            0.883228\ndtype: float64\n</code></pre>"},{"location":"tutorials/lime/#4-analyze-hsi-data-with-lime","title":"4. Analyze HSI Data with LIME","text":"<p>LIME (Local Interpretable Model-agnostic Explanations) is a model-agnostic algorithm that explains the predictions of a model by approximating the model's decision boundary around the prediction. It transforms the local area of the sample into interpretable space by creating superbands or superpixels which are the blocks that make up the sample. The algorithm generates a set of perturbed samples around the input sample and fits a linear model to the predictions of the perturbed samples. The linear model is then used to explain the prediction of the input sample.</p> Illustrarion of LIME method idea. The colored areas corresponds to the descison regions of a complex classification model, which we aim to explain. The black cross indicates the sample (observation) of interest. Dots correspond to artificial data around the instance of interest.The dashed line represents a simple linear model fitted to the artificial data and explained model predictions. The simple surrogate model \"explains\" local behaviour of the black-box model around the instance of interest  <p>An image above with explanation of LIME idea is taken from the Explanatory Model Analysis book, by Przemyslaw Biecek and Tomasz Burzykowski (2021)</p> <p>To initialize the LIME object, we need to provide the following parameters: - <code>explainable_model</code>: The model to be explained, used to predict the hyperspectral image and the task type. In order to integrate any model with our methods, the model has to be wrapped in a class from the meteors package, providing additional info for the LIME explainer, such as the problem type. - <code>interpretable_model</code>: The model used to approximate the decision boundary of the model. It could be any kind of easily interpretable model. The package provides implementations of Ridge and Lasso linear models ported from <code>sklearn</code> library and in this notebook we use a Lasso regression model. </p> <p>Meteors package supports explaining the model solving 3 different machine learning problems: - regression - classification - segmentation</p> <p>Since the linear, interpretable models used in the package for creating the LIME explanations do not solve directly the segmentation problem, we apply a handy trick that converts this problem into a regression problem that can be solved by linear models. This idea is inspired from the <code>captum</code> library and involves appropriately counting pixels in the segmentation mask so that the number of pixels in each segment can be estimated by the regression model.</p> <p>Let's initialize the LIME explainer with the CLIP model!</p> <pre><code>explainable_model = mt.utils.models.ExplainableModel(model, \"regression\")  # The model to be explained\ninterpretable_model = mt.utils.models.SkLearnLasso(\n    alpha=0.001\n)  # The interpretable model with regularization strength of 0.001\n\nlime = mt.attr.Lime(explainable_model, interpretable_model)  # The LIME explainer\n</code></pre>"},{"location":"tutorials/lime/#4-analyze-hsi-data-with-lime_1","title":"4. Analyze HSI data with LIME","text":"<p>Our implementation of LIME explainer enables to analyze HSI data based on the spatial or spectral dimension. It allows to investigate which regions or which spectral bands are the most relevant for the model. The rest of the notebook will be divided into spectral and spatial analysis of HSI data.</p>"},{"location":"tutorials/lime/#41-spatial-analysis","title":"4.1. Spatial Analysis","text":"<p>Similar to the original implementation of LIME for images, we will create spatial superpixels which are perturbed and used to train a surrogate model for explaining. These explanations will produce a correlation map with the output for each superpixel. Firstly, to generate attributions, we need to prepare the segmentation mask. Essentially, the segmentation mask is a torch tensor or numpy ndarray that contains information to which superpixel (region) does the specified pixel belongs. Integer values in such tensor represent the labels of superpixels. The mask should have the same shape as the image, but should be repeated along the channels dimension. The package now supports three methods to create the mask: - Using SLIC for superpixel detection - Using patch segmentation. Knowing that ViT uses squared non-overlapping sliding windows, we can also make superpixels based on the same technique - Providing a custom mask</p> <p>In this notebook we will present, how can we utilize different segmentation masks to investigate the model's performance and explore interesting areas in the images</p> <pre><code>segmentation_mask_slic = lime.get_segmentation_mask(hsi_0, segmentation_method=\"slic\")\nsegmentation_mask_patch = lime.get_segmentation_mask(hsi_0, segmentation_method=\"patch\", patch_size=14)\n</code></pre> <pre><code>fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n\nax1.imshow(segmentation_mask_slic[0, ...].T)\nax1.set_title(\"SLIC Mask Segmentation\")\n\nmt.visualize.visualize_hsi(hsi_0, ax2, use_mask=True)\nax2.set_title(\"Original HSI Image\")\n\nax3.imshow(segmentation_mask_patch[0, ...].T)\nax3.set_title(\"Patch Mask Segmentation\")\n\nfig.suptitle(\"Segmentation masks for the LIME explainer\")\n\nplt.show()\n</code></pre> <p></p> <p>Now, since we have our HSI sample data, segmentation mask and LIME model prepared, we will produce attribution maps for the segments. To do this, we simply need to execute one method <code>get_spatial_attributes</code>, and provide the HSI data, segmentation mask, and target class to be analyzed. If our model predicts more than one class (the model's output is multidimensional), we need to specify which target class we want to analyze. For each class, the analysis of the correlation with segments can be different.</p> <pre><code>spatial_attributes = lime.get_spatial_attributes(\n    hsi_0,  # HSI data\n    segmentation_mask_slic,  # Segmentation mask\n    target=1,  # class analyzed: K - potassium\n    n_samples=10,  # Number of perturbation, the more the better explainer is trained but the analsys is slower\n    perturbations_per_eval=4,  # Number of perturbations evaluated at once\n)\n</code></pre> <p>method <code>get_spatial_attributes</code> apart from the 3 required fields, takes as well few optional hyperparameters of explanations. Those are: - <code>n_samples</code> - it is a number of the generated artificial samples on which the linear model is trained. The larger number of samples, the explanations produced by the linear model are usually better, since its predictions should better mimic the predictions of the explained model. However, the larger <code>n_samples</code> the longer attribution takes to be performed - <code>perturbations_per_eval</code> - an inner batch size, this parameter may fasten the attribution, depending your machine capacity - <code>verbose</code> - a parameter specifying whether to output a progress bar, that makes the waiting time for attribution more pleasant</p> <p>The method has also an option to generate the segmentaion mask by itself, utilizing the static method <code>get_segmentation_mask</code> method under the hood.</p> <p>More information about the <code>get_spatial_attributes</code> function can be found in its reference page on the documentation website.</p> <p>The obtained <code>spatial_attributes</code> object is an object containing all the necessary data about the explanation. It consists of few fields: - <code>hsi</code> - a HyperSpectral Image object,  - <code>mask</code> - here used for creation superpixels  - <code>attributes</code> - explanations produced by the explainer of the same shape as the HSI - <code>score</code> - R2 score of the linear model used for the attribution</p> <p>Now, let us see how the attributions look like! </p> <p>Using the visualization capabilities provided by the <code>meteors</code> package, it is incredibely easy.</p> <pre><code>mt.visualize.visualize_spatial_attributes(spatial_attributes)\nplt.show()\n</code></pre> <p></p> <p>The plot presents three components (from the left) 1. On the left, the original image provided to LIME. 2. In the middle, the attribution map for each segment. 3. On the right, the segmented mask with IDs of the segments. The number 0 represents the masked region, and because it surrounds each segment, the placement of the number is in the middle of the segment.</p> <p>The colors in the attribution map have the following meanings: - Red: This superpixel is negatively correlated with the input. In our case, it means that the presence of this superpixel contributed to lowering the value for the target class 1. - White: This segment did not have a significant impact on the output. - Green: This superpixel is positively correlated with the output. Its presence increases the value for the class <code>1</code>.</p> <p>To validate how well the surrogate model was trained, we also provide the <code>score</code> attribute, which indicates the <code>R2</code> metric (coefficient of determination) that measures how well the surrogate model was trained.</p> <pre><code>spatial_attributes.score\n</code></pre> <pre><code>0.9498704075813293\n</code></pre> <p>High R2 score for the surrogate model hints that the attributions are sensible. In case the R2 score were very low, it could mean that the surrogate linear model can't tell which regions are more important for the explainable model preditions.</p> <p>Let's analyze the attribution maps for class <code>0</code> representing phosphorus estimation. We can simply modify the <code>target</code> parameter, utilizing the same segmentation mask, and rerun the explanation process.</p> <pre><code>spatial_attributes = lime.get_spatial_attributes(hsi_0, segmentation_mask_slic, target=0, n_samples=10)\n</code></pre> <pre><code>mt.visualize.visualize_spatial_attributes(spatial_attributes)\nprint(f\"R2 metric: {spatial_attributes.score:.4f}\")\nplt.show()\n</code></pre> <pre><code>R2 metric: 0.6901\n</code></pre> <p></p> <p>In our case, the green regions may correspond to areas with a higher concentration of the parameter being tested - here phosporus. </p>"},{"location":"tutorials/lime/#different-segmentation-masks","title":"Different segmentation masks","text":"<p>The most semantically meaningful segmentation mask is the one created by slic method. It contains superpixels that are created based on the image structure, which choice is very similiar to the regions that a human would choose</p> <p>However, <code>meteors</code> also supports creating the patch segmentation mask, but we are planning to increase support for different methods very soon.</p>"},{"location":"tutorials/lime/#patch-segmentation","title":"Patch segmentation","text":"<p>Patch segmentation mask tests the importance of regions shaped as rectangles. It is designed to work well with the ViT architecure, but it gives less semantic meaning.</p> <p>Let's check the <code>patch</code> segmentation mask and see how it affects the attribution maps.</p> <pre><code>spatial_attributes = lime.get_spatial_attributes(hsi_0, segmentation_mask_patch, target=1, n_samples=10)\n</code></pre> <pre><code>fig, ax = mt.visualize.visualize_spatial_attributes(spatial_attributes)\nprint(f\"R2 metric: {spatial_attributes.score:.4f}\")\nfig.suptitle(\"Patch Mask Segmentation for Potassium\")\nplt.show()\n</code></pre> <pre><code>R2 metric: 0.8699\n</code></pre> <p></p> <p>As we can see the Lime explainer focused on different regions of the image, which are not consistent with the previous results. Let's try using a larger number of perturbed samples. In this way, the linear model will be trained on much more perturbed versions of the original image and will be able to mimic the predictions of the explained model better.</p> <pre><code>spatial_attributes = lime.get_spatial_attributes(\n    hsi_0, segmentation_mask_patch, target=1, n_samples=100, perturbations_per_eval=10\n)\n</code></pre> <pre><code>fig, ax = mt.visualize.visualize_spatial_attributes(spatial_attributes)\nprint(f\"R2 metric: {spatial_attributes.score:.4f}\")\nfig.suptitle(\"LIME Explanation for Potassium with increased number of perturbed samples\")\nplt.show()\n</code></pre> <pre><code>R2 metric: 0.5538\n</code></pre> <p></p> <p>The explainer focuses again on the part of the image in the lower right corner and marks it as positively correlated with the output. It may suggests that the model indeed finds something interesting in this important region.</p>"},{"location":"tutorials/lime/#custom-segmentation-mask","title":"Custom segmentation mask","text":"<p>Additionally, an user might want to create their own segmentation mask, or modify the one created by the package.</p> <p>Therefore, we can inspect this lower right region more thoroughly by creating a more specific segmentation mask based on the slic one.</p> <pre><code>thorough_segmentation_mask_slic = segmentation_mask_slic.clone()\nthorough_segmentation_mask_slic[(thorough_segmentation_mask_slic != 10) &amp; (thorough_segmentation_mask_slic != 0)] = 1\n\nspatial_attributes = lime.get_spatial_attributes(\n    hsi_0, thorough_segmentation_mask_slic, target=1, n_samples=100, perturbations_per_eval=10\n)\n</code></pre> <pre><code>mt.visualize.visualize_spatial_attributes(spatial_attributes)\nprint(f\"R2 metric: {spatial_attributes.score:.4f}\")\n</code></pre> <pre><code>R2 metric: 0.9755\n</code></pre> <p></p> <p>it is visible, that this superpixel covers area that seems to be the most important region for the model. Why? This is another problem to be explained.</p>"},{"location":"tutorials/lime/#mask-importance","title":"Mask importance","text":"<p>Using an another custom segmentation mask, we can inspect, if binary mask covers regions that should not be relevant for the model. Now we will use the binary mask used for covering irrelevant regions as a segmentation mask to verify our hypothesis.</p> <p>Firstly, let's create a another HSI object, this time a plain image without any covering binary mask</p> <pre><code>image_without_mask = not_masked_image_torch.clone()\nhsi_without_mask = mt.HSI(\n    image=image_without_mask,  # The preprocessed image tensor, but without the mask\n    wavelengths=wavelengths,\n    orientation=\"CWH\",\n    device=device,\n)\n</code></pre> <p>Now, this image with the segmentation mask that consists of only two classes, we may explore, how each region is important for the model. To do so, we repeat this process for the potassium class.</p> <pre><code>segmentation_mask_from_binary = binary_mask + 1\nspatial_attributes = lime.get_spatial_attributes(\n    hsi_without_mask, segmentation_mask_from_binary, target=1, n_samples=10\n)\n</code></pre> <pre><code>C:\\Users\\tymot\\Documents\\praca\\pineapple\\meteors\\src\\meteors\\attr\\lime_base.py:738: UserWarning: Minimum element in feature mask is not 0, shifting indices to start at 0.\n  warnings.warn(\"Minimum element in feature mask is not 0, shifting indices to\" \" start at 0.\")\n</code></pre> <pre><code>fig, ax = mt.visualize.visualize_spatial_attributes(spatial_attributes)\nfig.suptitle(\"Importance of regions covered by the mask for Potassium class\")\nprint(f\"R2 metric: {spatial_attributes.score:.4f}\")\n</code></pre> <pre><code>R2 metric: 0.4493\n</code></pre> <p></p> <p>as we can see, the model correctly has learned that the relevant information is not covered with the mask. The region that was covered by default using the mask is strongly negatively correlated with the output, which may suggest that indeed, mask covers some regions, causing the model to output lower values for the estimated soil parameter</p>"},{"location":"tutorials/lime/#42-spectral-analysis","title":"4.2. Spectral Analysis","text":"<p>The spectral analysis is similar to the spatial one, but instead of analyzing the spatial dimension of the hyperspectral images, we analyze the spectral dimension - the image bands. In the process we group the specific channels of the image into superbands (groups of bands) and investigate importances of such groups. The spectral or band mask is a similar torch tensor or numpy ndarray as the segmentation mask, but instead of grouping regions it groups image channels. In the similar manner it is repeated along the width and height dimensions of the image.</p> <p>Since this kind of spectral analysis has sense only in hyperspectral imaginery, we paid special attention to this novel feature. As in the case of segmentation mask, user has several options how to create the band mask, to ensure that they had no difficulty using the analysis package and could rather focus on explaining the model. In the current package version user can: - provide the spectral indices or indexes of the commmonly recognized bands - specify exactly which wavelengths should compose the band mask - specify which wavelength indices corresponding to the wavelength list from the explained HSI object should be used</p> <p>All these band masks can be obtained using one simple method <code>get_band_mask</code>, which detailed documentation may also be found in the reference. Now we will go through these different methods of creating the band mask and create some attributions.</p>"},{"location":"tutorials/lime/#band-and-indices-names","title":"Band and indices names","text":"<p>This, definetely the fastest for the user, method provides a quick way to explore importance of some well known superbands. To create the band mask using this approach, all we need to do is to pass a list or a dictionary of the band names:</p> <pre><code>band_mask, band_names = lime.get_band_mask(hsi_0, [\"R\", \"G\", \"B\"])\n</code></pre> <pre><code>\u001b[32m2024-09-24 02:43:21.378\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments G and B are overlapping on wavelength 510.0299987792969\u001b[0m\n\u001b[32m2024-09-24 02:43:21.381\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments G and B are overlapping on wavelength 513.219970703125\u001b[0m\n\u001b[32m2024-09-24 02:43:21.382\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments G and B are overlapping on wavelength 516.4199829101562\u001b[0m\n\u001b[32m2024-09-24 02:43:21.383\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments G and B are overlapping on wavelength 519.6099853515625\u001b[0m\n\u001b[32m2024-09-24 02:43:21.384\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments G and B are overlapping on wavelength 522.8099975585938\u001b[0m\n\u001b[32m2024-09-24 02:43:21.384\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments G and B are overlapping on wavelength 526.010009765625\u001b[0m\n\u001b[32m2024-09-24 02:43:21.385\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments G and B are overlapping on wavelength 529.2000122070312\u001b[0m\n</code></pre> <p>this method outpus a tuple of two variables: - <code>band_mask</code> - a created band mask - <code>band_names</code> - a dictionary containing mapping from provided labels and segment indices</p> <p>In this case we created a band mask that contains 4 superpixels - one for each of the base colours and another one including all the background.</p> <pre><code>band_names\n</code></pre> <pre><code>{'R': 1, 'G': 2, 'B': 3}\n</code></pre> <pre><code>plt.scatter(wavelengths, band_mask.squeeze(1).squeeze(1))\n\nplt.title(\"Band Mask for the RGB bands\")\n\nplt.show()\n</code></pre> <p></p> <p>The plot presents how bands are grouped. The bands with the value 0 creates the additional band group <code>not_included</code> which also will be used in the analysis.</p> <p>Now, we can analyze the hyperspectral image based on the spectral dimension. We will use the same LIME model as in the spatial analysis (initialize with the same parameters), but we will provide the band mask instead of the segmentation mask and also band names.</p> <pre><code>lime = mt.attr.Lime(\n    explainable_model=mt.utils.models.ExplainableModel(model, \"regression\"),\n    interpretable_model=mt.utils.models.SkLearnLasso(alpha=0.001),\n)\n</code></pre> <pre><code>spectral_attributes = lime.get_spectral_attributes(\n    hsi_0,  # HSI data\n    band_mask=band_mask,  # Band mask\n    target=1,  # class analyzed - K - potassium\n    band_names=band_names,  # Band names\n    n_samples=10,  # Number of perturbation, the more the better explainer is trained but the analsys is slower\n)\n</code></pre> <pre><code>C:\\Users\\tymot\\Documents\\praca\\pineapple\\meteors\\src\\meteors\\attr\\attributes.py:148: UserWarning: Adding 'not_included' to band names because 0 ids is present in the mask and not in band names\n  warnings.warn(\n</code></pre> <p>The spectral attributions are similar to spatial attributes consisting of <code>hsi</code>, <code>mask</code>, <code>attributes</code> and <code>score</code> of the linear model.</p> <pre><code>assert len(wavelengths) == spectral_attributes.flattened_attributes.shape[0]\nplt.scatter(wavelengths, spectral_attributes.flattened_attributes)\nplt.title(\"Spectral Attributes Map\")\nplt.show()\n</code></pre> <p></p> <p>But again as with spatial analysis it is much easier to visualize the results using the provided meteors visualization functions.</p> <pre><code>fig, ax = mt.visualize.visualize_spectral_attributes(spectral_attributes, show_not_included=True)\nfig.suptitle(\"Spectral Attributes for Potassium class and RGB bands\")\nplt.show()\n</code></pre> <p></p> <p>The plot this time consists of two parts. On the left, we have the attribution value per band for the hyperspectral image it helps to identify, which particular bands are important, whilist the right plot helps to identify the most important superbands and compare magnitudes of its importance. </p> <p>On the plot above, we may see that the red superband is much more important than the green and blue ones. How would the situation change if we compared red superband and blue and green superband together?</p> <p>Fortunately, thanks to the method <code>get_band_mask</code> it is incredibely easy - we just need to specify bands that will produce the superband.</p> <pre><code>band_mask, band_names = lime.get_band_mask(hsi_0, [\"R\", [\"G\", \"B\"]])\n</code></pre> <p>and as before we can create the attributions for the selected superbands using LIME explainer.</p> <pre><code>spectral_attributes = lime.get_spectral_attributes(\n    hsi_0,  # HSI data\n    band_mask=band_mask,  # Band mask\n    target=1,  # class analyzed - K - potassium\n    band_names=band_names,  # Band names\n    n_samples=10,  # Number of perturbation, the more the better explainer is trained but the analsys is slower\n)\n</code></pre> <pre><code>C:\\Users\\tymot\\Documents\\praca\\pineapple\\meteors\\src\\meteors\\attr\\attributes.py:148: UserWarning: Adding 'not_included' to band names because 0 ids is present in the mask and not in band names\n  warnings.warn(\n</code></pre> <pre><code>fig, ax = mt.visualize.visualize_spectral_attributes(spectral_attributes, show_not_included=True)\nfig.suptitle(\"Spectral Attributes for Potassium class and R and GB superbands\")\nplt.show()\n</code></pre> <p></p> <p>it looks that, indeed, green and blue superbands combined are more important than the red one.</p> <p>To validate the model, we again can use the <code>score</code> attribute, which indicates the <code>R2</code> metric of how well-trained the surrogate model was.</p> <pre><code>spectral_attributes.score\n</code></pre> <pre><code>0.6528714895248413\n</code></pre> <p>All the band names are sourced from the Awesome Spectral Indices repository and handled using the <code>spyndex</code> library. Therefore, we can explore all the bands or try out some more exotic combinations using the predefined band indices here </p> <p>We will use now one of the indices taken from the library, a Bare Soil Index, which is a combination of couple of base bands. It can be defined as $$ BI = \\frac{(S1 + R) - (N + B)}{(S1 + R) + (N + B)} $$</p> <p>where S1 corresponds to SWIR 1 band, R and B to red and blue respectively and N to NIR band. It is used, as the name suggests, to detect bare soil in the hyperspectral imaginery and possibly can be used as well to detect soil parameters.</p> <pre><code>band_mask, band_names = lime.get_band_mask(hsi_0, [[\"G\", \"B\"], \"R\", \"BI\"])\n</code></pre> <pre><code>\u001b[32m2024-09-24 02:43:35.036\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_get_band_wavelengths_indices_from_band_names\u001b[0m:\u001b[36m634\u001b[0m - \u001b[33m\u001b[1mBand S1 is not present in the given wavelengths. Band ranges from 1550 nm to 1750 nm and the HSI wavelengths range from 462.08 nm to 938.37 nm. The given band will be skipped\u001b[0m\n\u001b[32m2024-09-24 02:43:35.038\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments ('G', 'B') and BI are overlapping on wavelength 462.0799865722656\u001b[0m\n\u001b[32m2024-09-24 02:43:35.039\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments ('G', 'B') and BI are overlapping on wavelength 465.2699890136719\u001b[0m\n\u001b[32m2024-09-24 02:43:35.040\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments ('G', 'B') and BI are overlapping on wavelength 468.4700012207031\u001b[0m\n\u001b[32m2024-09-24 02:43:35.040\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments ('G', 'B') and BI are overlapping on wavelength 471.6700134277344\u001b[0m\n\u001b[32m2024-09-24 02:43:35.041\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments ('G', 'B') and BI are overlapping on wavelength 474.8599853515625\u001b[0m\n\u001b[32m2024-09-24 02:43:35.042\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments ('G', 'B') and BI are overlapping on wavelength 478.05999755859375\u001b[0m\n\u001b[32m2024-09-24 02:43:35.043\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments ('G', 'B') and BI are overlapping on wavelength 481.260009765625\u001b[0m\n\u001b[32m2024-09-24 02:43:35.043\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments ('G', 'B') and BI are overlapping on wavelength 484.45001220703125\u001b[0m\n\u001b[32m2024-09-24 02:43:35.044\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments ('G', 'B') and BI are overlapping on wavelength 487.6499938964844\u001b[0m\n\u001b[32m2024-09-24 02:43:35.045\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments ('G', 'B') and BI are overlapping on wavelength 490.8500061035156\u001b[0m\n\u001b[32m2024-09-24 02:43:35.046\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments ('G', 'B') and BI are overlapping on wavelength 494.0400085449219\u001b[0m\n\u001b[32m2024-09-24 02:43:35.047\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments ('G', 'B') and BI are overlapping on wavelength 497.239990234375\u001b[0m\n\u001b[32m2024-09-24 02:43:35.047\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments ('G', 'B') and BI are overlapping on wavelength 500.42999267578125\u001b[0m\n\u001b[32m2024-09-24 02:43:35.048\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments ('G', 'B') and BI are overlapping on wavelength 503.6300048828125\u001b[0m\n\u001b[32m2024-09-24 02:43:35.048\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments ('G', 'B') and BI are overlapping on wavelength 506.8299865722656\u001b[0m\n\u001b[32m2024-09-24 02:43:35.049\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments ('G', 'B') and BI are overlapping on wavelength 510.0299987792969\u001b[0m\n\u001b[32m2024-09-24 02:43:35.050\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments ('G', 'B') and BI are overlapping on wavelength 513.219970703125\u001b[0m\n\u001b[32m2024-09-24 02:43:35.050\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments ('G', 'B') and BI are overlapping on wavelength 516.4199829101562\u001b[0m\n\u001b[32m2024-09-24 02:43:35.051\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments ('G', 'B') and BI are overlapping on wavelength 519.6099853515625\u001b[0m\n\u001b[32m2024-09-24 02:43:35.051\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments ('G', 'B') and BI are overlapping on wavelength 522.8099975585938\u001b[0m\n\u001b[32m2024-09-24 02:43:35.052\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments ('G', 'B') and BI are overlapping on wavelength 526.010009765625\u001b[0m\n\u001b[32m2024-09-24 02:43:35.053\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments ('G', 'B') and BI are overlapping on wavelength 529.2000122070312\u001b[0m\n\u001b[32m2024-09-24 02:43:35.054\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments R and BI are overlapping on wavelength 621.9099731445312\u001b[0m\n\u001b[32m2024-09-24 02:43:35.054\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments R and BI are overlapping on wavelength 625.0999755859375\u001b[0m\n\u001b[32m2024-09-24 02:43:35.055\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments R and BI are overlapping on wavelength 628.2999877929688\u001b[0m\n\u001b[32m2024-09-24 02:43:35.056\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments R and BI are overlapping on wavelength 631.5\u001b[0m\n\u001b[32m2024-09-24 02:43:35.057\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments R and BI are overlapping on wavelength 634.6900024414062\u001b[0m\n\u001b[32m2024-09-24 02:43:35.057\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments R and BI are overlapping on wavelength 637.8900146484375\u001b[0m\n\u001b[32m2024-09-24 02:43:35.058\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments R and BI are overlapping on wavelength 641.0900268554688\u001b[0m\n\u001b[32m2024-09-24 02:43:35.058\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments R and BI are overlapping on wavelength 644.280029296875\u001b[0m\n\u001b[32m2024-09-24 02:43:35.059\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments R and BI are overlapping on wavelength 647.47998046875\u001b[0m\n\u001b[32m2024-09-24 02:43:35.060\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments R and BI are overlapping on wavelength 650.6699829101562\u001b[0m\n\u001b[32m2024-09-24 02:43:35.061\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments R and BI are overlapping on wavelength 653.8699951171875\u001b[0m\n\u001b[32m2024-09-24 02:43:35.062\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments R and BI are overlapping on wavelength 657.0700073242188\u001b[0m\n\u001b[32m2024-09-24 02:43:35.062\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments R and BI are overlapping on wavelength 660.27001953125\u001b[0m\n\u001b[32m2024-09-24 02:43:35.063\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments R and BI are overlapping on wavelength 663.4600219726562\u001b[0m\n\u001b[32m2024-09-24 02:43:35.064\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments R and BI are overlapping on wavelength 666.6599731445312\u001b[0m\n\u001b[32m2024-09-24 02:43:35.065\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments R and BI are overlapping on wavelength 669.8499755859375\u001b[0m\n\u001b[32m2024-09-24 02:43:35.065\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments R and BI are overlapping on wavelength 673.0499877929688\u001b[0m\n\u001b[32m2024-09-24 02:43:35.066\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments R and BI are overlapping on wavelength 676.25\u001b[0m\n\u001b[32m2024-09-24 02:43:35.067\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments R and BI are overlapping on wavelength 679.4500122070312\u001b[0m\n\u001b[32m2024-09-24 02:43:35.068\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments R and BI are overlapping on wavelength 682.6400146484375\u001b[0m\n\u001b[32m2024-09-24 02:43:35.068\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments R and BI are overlapping on wavelength 685.8400268554688\u001b[0m\n\u001b[32m2024-09-24 02:43:35.069\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.lime\u001b[0m:\u001b[36m_check_overlapping_segments\u001b[0m:\u001b[36m800\u001b[0m - \u001b[33m\u001b[1mSegments R and BI are overlapping on wavelength 689.030029296875\u001b[0m\n</code></pre> <p>now, using the same methods as before, we can attribute the new superbands using the LIME explainer and visualize the output</p> <pre><code>band_names\n</code></pre> <pre><code>{('G', 'B'): 1, 'R': 2, 'BI': 3}\n</code></pre> <pre><code>spectral_attributes = lime.get_spectral_attributes(\n    hsi_0,  # HSI data\n    band_mask=band_mask,  # Band mask\n    target=1,  # class analyzed - K - potassium\n    band_names=band_names,  # Band names\n    n_samples=10,  # Number of perturbation, the more the better explainer is trained but the analsys is slower\n)\n</code></pre> <pre><code>C:\\Users\\tymot\\Documents\\praca\\pineapple\\meteors\\src\\meteors\\attr\\attributes.py:148: UserWarning: Adding 'not_included' to band names because 0 ids is present in the mask and not in band names\n  warnings.warn(\n</code></pre> <pre><code>mt.visualize.visualize_spectral_attributes(spectral_attributes, show_not_included=True)\nprint(f\"R2 metric: {spectral_attributes.score:.4f}\")\nplt.show()\n</code></pre> <pre><code>R2 metric: 0.7993\n</code></pre> <p></p> <p>it seems that the superband BI is actually irrelevant for the model in this specific case. It looks that the model does not base its predictions for the current image on this specific bands</p> <p>In this way, we may investigate if the model uses the bands that were commonly used for the similar tasks in the literature, which could help us debbuging the model!  Now we will use some bands, that should really be important to the model.</p>"},{"location":"tutorials/lime/#wavelengths-ranges","title":"Wavelengths ranges","text":"<p>In some cases, we do not want to use any well known band combinations. In our team, we had access to knowledge of domain experts who gave us the exact wavelength values that are used to detect potassium, phosphorus, magnessium and pH in the soil. Now we can utilize this knowledge and create our own superbands. </p> <p>Now we will try out the values for the potassium. Unfortunately, not all the wavelengths provided are exactly mentioned in our wavelengths list, thus we need to find the closest corresponding indices to the values given by the experts.</p> <pre><code>potassium_superband_indices = [0, 1, 4, 10, 43, 46, 47]  # supposedly important bands for potassium\n# or differently\npotassium_superband_wavelengths = [wavelengths[i] for i in potassium_superband_indices]\n</code></pre> <pre><code>potassium_superband_wavelengths\n</code></pre> <pre><code>[462.08, 465.27, 474.86, 494.04, 599.53, 609.12, 612.32]\n</code></pre> <pre><code>band_dict = {\"potassium\": potassium_superband_indices, \"another_superpixel\": [i for i in range(20, 30)]}\nband_dict\n</code></pre> <pre><code>{'potassium': [0, 1, 4, 10, 43, 46, 47],\n 'another_superpixel': [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]}\n</code></pre> <pre><code>band_mask, band_names = lime.get_band_mask(hsi_0, band_indices=band_dict)\n</code></pre> <pre><code>band_names  # the mapping from names to segment labels\n</code></pre> <pre><code>{'potassium': 1, 'another_superpixel': 2}\n</code></pre> <pre><code>spectral_attributes = lime.get_spectral_attributes(\n    hsi_0,\n    band_mask=band_mask,\n    target=1,\n    band_names=band_names,\n    n_samples=100,\n)\n</code></pre> <pre><code>C:\\Users\\tymot\\Documents\\praca\\pineapple\\meteors\\src\\meteors\\attr\\attributes.py:148: UserWarning: Adding 'not_included' to band names because 0 ids is present in the mask and not in band names\n  warnings.warn(\n</code></pre> <pre><code>mt.visualize.visualize_spectral_attributes(spectral_attributes, show_not_included=True)\nprint(f\"R2 metric: {spectral_attributes.score:.4f}\")\nplt.show()\n</code></pre> <pre><code>R2 metric: 0.4769\n</code></pre> <p></p> <p>As it turns out, in this case, the bands given us by the experts are not necessarily important for the model. We need to remember, that this analysis is performed for solely one image. Perhaps, this is just one outlier and in different cases model might actually use the specified bands. For such cases, we can utilize the global explanations which are attributions aggregeated for multiple input images. </p>"},{"location":"tutorials/lime/#global-attributions","title":"Global attributions","text":"<p>An interesting capability unique to spectral analysis is the ability to aggregate results across multiple samples, allowing us to transition from local interpretation to global interpretation. This is usually not possible for spatial analysis, as the images differ significantly when it comes to the covered land. Aggregating spatial information is challenging due to the lack of straightforward method for determining which parts of different images are similar, as the covered land can vary significantly. On the contrary, spectral analysis benefits from consistent bands accross images, allowing for specification of common superbands.</p> <p>To give an idea how to perform such analysis, we need a second sample of the hyperspectral image.</p> <pre><code>data, mask = load_single_npz_image(\"data/1.npz\")\nmasked_data = data * mask\nmasked_data = torch.from_numpy(masked_data.astype(np.float32)).permute(2, 0, 1)\neval_tr = get_eval_transform(224)\n\nimage_torch_1 = eval_tr(masked_data)\nnot_masked_image_torch_1 = eval_tr(torch.from_numpy(data.astype(np.float32)).permute(2, 0, 1))\n</code></pre> <pre><code>binary_mask_1 = (image_torch_1 &gt; 0.0).int()\n\nhsi_1 = mt.HSI(\n    image=not_masked_image_torch_1, wavelengths=wavelengths, orientation=\"CWH\", binary_mask=binary_mask_1, device=device\n)\n\nax = mt.visualize.visualize_hsi(hsi_1, use_mask=True)\nax.set_title(\"Another sample image from the HYPERVIEW dataset\")\n</code></pre> <pre><code>Text(0.5, 1.0, 'Another sample image from the HYPERVIEW dataset')\n</code></pre> <p></p> <p>Now, once the image is properly loaded and preprocessed, let's get the attributions for the second sample, using the same band mask as before</p> <pre><code>spectral_attributes_1 = lime.get_spectral_attributes(\n    hsi_1, band_mask=band_mask, target=1, band_names=band_names, n_samples=100\n)\n</code></pre> <pre><code>C:\\Users\\tymot\\Documents\\praca\\pineapple\\meteors\\src\\meteors\\attr\\attributes.py:148: UserWarning: Adding 'not_included' to band names because 0 ids is present in the mask and not in band names\n  warnings.warn(\n</code></pre> <pre><code>mt.visualize.visualize_spectral_attributes(spectral_attributes, show_not_included=True)\nprint(f\"R2 metric: {spectral_attributes.score:.4f}\")\nplt.show()\n</code></pre> <pre><code>R2 metric: 0.4769\n</code></pre> <p></p> <p>To get the global interpretation we will provide the list of attributions to the meteors visualizer to create the global interpretation visualization.</p> <pre><code>mt.visualize.visualize_spectral_attributes([spectral_attributes, spectral_attributes_1], show_not_included=True)\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>As it turns out, the model does not necessarily use the specified bands in the prediction of the potassium class. This is probably insufficient to conclude anything using only attributions from 2 images, especially because the <code>score</code> of the explanations was low, but our model suprisingly does not use the expected wavelengths.</p>"}]}