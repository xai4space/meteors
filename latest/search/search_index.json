{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\u2604\ufe0f\ud83d\udef0\ufe0f Meteors","text":""},{"location":"#introduction","title":"\ud83d\udef0\ufe0f Introduction","text":"<p>Meteors is an open-source package for creating explanations of hyperspectral and multispectral images. Developed primarily for Pytorch models, Meteors was inspired by the Captum library. Our goal is to provide not only the ability to create explanations for hyperspectral images but also to visualize them in a user-friendly way.</p> <p>Please note that this package is still in the development phase, and we welcome any feedback and suggestions to help improve the library.</p> <p>Meteors emerged from a research grant project between the Warsaw University of Technology research group MI2.ai and Kp Labs, financially supported by the European Space Agency (ESA).</p>"},{"location":"#target-audience","title":"\ud83c\udfaf Target Audience","text":"<p>Meteors is designed for:</p> <ul> <li>Researchers, data scientists, and developers who work with hyperspectral and multispectral images and want to understand the decisions made by their models.</li> <li>Engineers who build models for production and want to troubleshoot through improved model interpretability.</li> <li>Developers seeking to deliver better explanations to end users on why they're seeing specific content.</li> </ul>"},{"location":"#installation","title":"\ud83d\udce6 Installation","text":"<p>Requirements</p> <ul> <li>Python &gt;= 3.9</li> <li>PyTorch &gt;= 1.10</li> <li>Captum &gt;= 0.7.0</li> </ul> <p>Install with <code>pip</code>:</p> <pre><code>pip install meteors\n</code></pre> <p>With conda: Coming soon</p>"},{"location":"#documentation","title":"\ud83d\udcda Documentation","text":"<p>Please refer to the documentation for more information on how to use Meteors.</p>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>As an open-source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation.</p> <p>We use rye as our project and package management tool. To start developing, follow these steps:</p> <pre><code>curl -sSf https://rye.astral.sh/get | bash # Install Rye\nrye pin &lt;python version &gt;=3.9&gt; # Pin the python version\nrye sync # Sync the environment\n</code></pre> <p>Before pushing your changes, please run the tests and the linter:</p> <pre><code>rye test\nrye run pre-commit run --all-files\n</code></pre> <p>For more information on how to contribute, please refer to our Contributing Guide.</p> <p>Thank you for considering contributing to Meteors!</p>"},{"location":"#contributors","title":"\ud83d\udcab Contributors","text":""},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v010-2024-10-29","title":"v0.1.0 (2024-10-29)","text":""},{"location":"changelog/#chores","title":"\ud83d\udece\ufe0f Chores","text":"<ul> <li>update the project version (#112)</li> <li>reduced coverage threshold (#94)</li> </ul>"},{"location":"changelog/#features","title":"\ud83d\udd28 Features","text":"<ul> <li>96 feat segmentation support for attribution methods (#103)</li> <li>refactored the package structure (#111)</li> <li>Custom errors (#101)</li> </ul>"},{"location":"changelog/#bug-fixes","title":"\ud83e\ude7a Bug Fixes","text":"<ul> <li>corrected visualizations (#106)</li> <li>Updated occlusion (#93)</li> <li>91 docs the lime illustration image is missing in the docs (#92)</li> </ul>"},{"location":"changelog/#documentation","title":"\ud83d\udcda Documentation","text":"<ul> <li>update changelog.md for 0.0.4 [skip ci] (#90)</li> </ul>"},{"location":"changelog/#v004-2024-09-25","title":"v0.0.4 (2024-09-25)","text":""},{"location":"changelog/#bug-fixes_1","title":"\ud83e\ude7a Bug Fixes","text":"<ul> <li>infinite loop in segmentation (#87)</li> </ul>"},{"location":"changelog/#features_1","title":"\ud83d\udd28 Features","text":"<ul> <li>HyperNoiseTunnel and captum attribution methods (#51)</li> </ul>"},{"location":"changelog/#v003-2024-09-23","title":"v0.0.3 (2024-09-23)","text":""},{"location":"changelog/#bug-fixes_2","title":"\ud83e\ude7a Bug Fixes","text":"<ul> <li>github action release workflow to pypi (#83)</li> </ul>"},{"location":"changelog/#meteors-002-2024-08-11","title":"meteors 0.0.2 (2024-08-11)","text":"<ul> <li>No release</li> <li>Refined package structure - simple modules for models and visualisation, installation using toml file</li> <li>Spectral attributions using LIME</li> <li>CUDA compatibility of LIME</li> </ul>"},{"location":"changelog/#meteors-001-2024-06-02","title":"meteors 0.0.1 (2024-06-02)","text":"<ul> <li>No release</li> <li>Prepared a simple draft of package along with some ideas and sample files for implementation of LIME for hyperspectral images.</li> <li>Segmentation mask for LIME using slic</li> <li>Spatial attributions using LIME</li> </ul>"},{"location":"how-to-guides/","title":"\ud83e\udd1d How to Contribute","text":"<p>Thank you for your interest in contributing to Meteors! We welcome contributions from the community to help improve and expand the library. This guide will walk you through the process of contributing to the project.</p>"},{"location":"how-to-guides/#types-of-contributions","title":"\ud83d\udccb Types of Contributions","text":"<p>There are several ways you can contribute to Meteors:</p> <ul> <li>\ud83d\udc1b Reporting bugs</li> <li>\ud83d\udca1 Suggesting new features or enhancements</li> <li>\ud83d\udcd6 Improving documentation</li> <li>\ud83d\udcbb Writing code (fixing bugs, implementing new features)</li> <li>\ud83e\uddea Adding or improving test cases</li> <li>\ud83d\udce3 Spreading the word about Meteors</li> </ul>"},{"location":"how-to-guides/#getting-started","title":"\ud83c\udf3f Getting Started","text":"<p>To start contributing to Meteors, follow these steps:</p> <ol> <li>\ud83c\udf74 Fork the Meteors repository on GitHub.</li> <li>\ud83d\udce5 Clone your forked repository to your local machine.</li> <li>\ud83d\udd00 Create a new branch for your contribution.</li> <li>\ud83d\udce6 Set rye <code>rye pin &lt;python_version&gt; &amp;&amp; rye sync</code>.</li> <li>\ud83d\udee0\ufe0f Make your changes or additions.</li> <li>\u2705 Test your changes to ensure they work as expected <code>rye test</code>.</li> <li>\ud83e\uddf9 Lint your code <code>rye run pre-commit run --all-files</code>.</li> <li>\ud83d\udcdd Commit your changes with a clear and descriptive commit message.</li> <li>\ud83d\udce4 Push your changes to your forked repository.</li> <li>\ud83d\udd0c Submit a pull request to the main Meteors repository.</li> <li>\ud83d\udcdd Fill out the pull request template with the necessary information.</li> </ol>"},{"location":"how-to-guides/#code-guidelines","title":"\ud83d\udcd0 Code Guidelines","text":"<p>When contributing code to Meteors, please follow these guidelines:</p> <ul> <li>\ud83d\udcda Document your code using docstrings and comments.</li> <li>\u2705 Write unit tests for new features or bug fixes.</li> <li>\ud83e\uddf9 Ensure your code is clean, readable, and well-structured.</li> <li>\ud83d\udea8 Run the existing tests and make sure they pass before submitting a pull request.</li> <li>\ud83d\udc0d Run linter to clean the code.</li> </ul>"},{"location":"how-to-guides/#code-of-conduct","title":"\ud83d\udcdc Code of Conduct","text":"<p>Please note that by contributing to Meteors, you are expected to adhere to our Code of Conduct. Be respectful, inclusive, and considerate in your interactions with others.</p>"},{"location":"how-to-guides/#recognition","title":"\ud83d\ude4c Recognition","text":"<p>We appreciate all contributions to Meteors, and we make sure to recognize our contributors. Your name will be added to the list of contributors in the project's README file.</p> <p>If you have any questions or need further assistance, feel free to reach out to the maintainers or open an issue on the GitHub repository.</p> <p>Thank you for your contribution to Meteors! \ud83c\udf89\ud83d\ude80</p>"},{"location":"quickstart/","title":"\ud83d\ude80 Quickstart","text":"<p>Welcome to the Quickstart Guide for Meteors! This guide will walk you through the basic steps to get started with using Meteors for explaining your hyperspectral and multispectral image models.</p>"},{"location":"quickstart/#prerequisites","title":"\ud83d\udccb Prerequisites","text":"<p>Before you begin, make sure you have the following installed:</p> <ul> <li>Python &gt;= 3.9</li> <li>PyTorch &gt;= 1.10</li> <li>Captum &gt;= 0.7.0</li> </ul>"},{"location":"quickstart/#installation","title":"\ud83d\udce5 Installation","text":"<p>To install Meteors, simply run the following command:</p> <pre><code>pip install meteors\n</code></pre> <p>For conda users, we will provide a conda installation method in the near future. We promise!\ud83e\udd1e</p>"},{"location":"quickstart/#basic-hyperspectral-or-multispectral-data-object","title":"\ud83c\udf1f Basic Hyperspectral or Multispectral Data Object","text":"<p>Meteors provide an easy-to-use object for handling hyperspectral and multispectral images. The <code>HSI</code> object is a simple way to process and manipulate your data.</p> <pre><code>from meteors import HSI\n</code></pre> <p>Remember, when providing data to the model, make sure it is in the final format that the model expects, without the batch dimension. The <code>HSI</code> object will handle the rest. We also recommend providing the image data channel orientation, height, width, and the number of channels in the format <code>(CHW)</code>. For example:</p> <ul> <li>Number of channels: C</li> <li>Height: H</li> <li>Width: W</li> </ul>"},{"location":"quickstart/#explanation-methods","title":"\ud83d\udd0d Explanation Methods","text":"<p>Meteors provides several explanation methods for hyperspectral and multispectral images, including:</p> <ul> <li>LIME: Local Interpretable Model-agnostic Explanations</li> <li>More methods coming soon!</li> </ul> <p>To use a specific explanation method in tutorials we provide for each method, example code.</p>"},{"location":"quickstart/#visualization-options","title":"\ud83c\udfa8 Visualization Options","text":"<p>Meteors offers various visualization options to help you understand and interpret the explanations in package <code>meteors.visualize</code>.</p> <pre><code>from meteors.visualize import visualize_spectral_attributes, visualize_spatial_attributes\n</code></pre>"},{"location":"quickstart/#tutorials","title":"\ud83d\udcda Tutorials","text":"<p>We have several tutorials to help get you off the ground with Meteors. The tutorials are Jupyter notebooks and cover the basics along with demonstrating usage of Meteors .</p> <p>View the tutorials page here.</p>"},{"location":"quickstart/#api-reference","title":"\ud83d\udcd6 API Reference","text":"<p>For an in-depth reference of the various Meteors internals, see our API Reference.</p>"},{"location":"quickstart/#contributing","title":"\ud83d\ude4c Contributing","text":"<p>We welcome contributions to Meteors! Please refer to our Contributing Guide for more information on how to get involved.</p>"},{"location":"reference/","title":"API Reference","text":"<p>The architecture of the package can be seen on the UML diagram: </p>"},{"location":"reference/#hyperspectral-image","title":"HyperSpectral Image","text":""},{"location":"reference/#src.meteors.hsi.HSI","title":"<code>HSI</code>","text":"<p>A dataclass for hyperspectral image data, including the image, wavelengths, and binary mask.</p> <p>Attributes:</p> Name Type Description <code>image</code> <code>Tensor</code> <p>The hyperspectral image data as a PyTorch tensor.</p> <code>wavelengths</code> <code>Tensor</code> <p>The wavelengths present in the image.</p> <code>orientation</code> <code>tuple[str, str, str]</code> <p>The orientation of the image data.</p> <code>device</code> <code>device</code> <p>The device to be used for inference.</p> <code>binary_mask</code> <code>Tensor</code> <p>A binary mask used to cover unimportant parts of the image.</p> Source code in <code>src/meteors/hsi.py</code> <pre><code>class HSI(BaseModel):\n    \"\"\"A dataclass for hyperspectral image data, including the image, wavelengths, and binary mask.\n\n    Attributes:\n        image (torch.Tensor): The hyperspectral image data as a PyTorch tensor.\n        wavelengths (torch.Tensor): The wavelengths present in the image.\n        orientation (tuple[str, str, str]): The orientation of the image data.\n        device (torch.device): The device to be used for inference.\n        binary_mask (torch.Tensor): A binary mask used to cover unimportant parts of the image.\n    \"\"\"\n\n    image: Annotated[  # Should always be a first field\n        torch.Tensor,\n        PlainValidator(ensure_image_tensor),\n        Field(description=\"Hyperspectral image. Converted to torch tensor.\"),\n    ]\n    wavelengths: Annotated[\n        torch.Tensor,\n        PlainValidator(ensure_wavelengths_tensor),\n        Field(description=\"Wavelengths present in the image. Defaults to None.\"),\n    ]\n    orientation: Annotated[\n        tuple[str, str, str],\n        PlainValidator(validate_orientation),\n        Field(\n            description=(\n                'Orientation of the image - sequence of three one-letter strings in any order: \"C\", \"H\", \"W\" '\n                'meaning respectively channels, height and width of the image. Defaults to (\"C\", \"H\", \"W\").'\n            ),\n        ),\n    ] = (\"C\", \"H\", \"W\")\n    device: Annotated[\n        torch.device,\n        PlainValidator(resolve_inference_device_hsi),\n        Field(\n            validate_default=True,\n            exclude=True,\n            description=\"Device to be used for inference. If None, the device of the input image will be used. Defaults to None.\",\n        ),\n    ] = None\n    binary_mask: Annotated[\n        torch.Tensor,\n        PlainValidator(process_and_validate_binary_mask),\n        Field(\n            validate_default=True,\n            description=(\n                \"Binary mask used to cover not important parts of the base image, masked parts have values equals to 0. \"\n                \"Converted to torch tensor. Defaults to None.\"\n            ),\n        ),\n    ] = None\n\n    @property\n    def spectral_axis(self) -&gt; int:\n        \"\"\"Returns the index of the spectral (wavelength) axis based on the current data orientation.\n\n        In hyperspectral imaging, the spectral axis represents the dimension along which\n        different spectral bands or wavelengths are arranged. This property dynamically\n        determines the index of this axis based on the current orientation of the data.\n\n        Returns:\n            int: The index of the spectral axis in the current data structure.\n                - 0 for 'CHW' or 'CWH' orientations (Channel/Wavelength first)\n                - 2 for 'HWC' or 'WHC' orientations (Channel/Wavelength last)\n                - 1 for 'HCW' or 'WCH' orientations (Channel/Wavelength in the middle)\n\n        Note:\n            The orientation is typically represented as a string where:\n            - 'C' represents the spectral/wavelength dimension\n            - 'H' represents the height (rows) of the image\n            - 'W' represents the width (columns) of the image\n\n        Examples:\n            &gt;&gt;&gt; hsi_image = HSI()\n            &gt;&gt;&gt; hsi_image.orientation = \"CHW\"\n            &gt;&gt;&gt; hsi_image.spectral_axis\n            0\n            &gt;&gt;&gt; hsi_image.orientation = \"HWC\"\n            &gt;&gt;&gt; hsi_image.spectral_axis\n            2\n        \"\"\"\n        return get_channel_axis(self.orientation)\n\n    @property\n    def spatial_binary_mask(self) -&gt; torch.Tensor:\n        \"\"\"Returns a 2D spatial representation of the binary mask.\n\n        This property extracts a single 2D slice from the 3D binary mask, assuming that\n        the mask is identical across all spectral bands. It handles different data\n        orientations by first ensuring the spectral dimension is the last dimension\n        before extracting the 2D spatial mask.\n\n        Returns:\n            torch.Tensor: A 2D tensor representing the spatial binary mask.\n                The shape will be (H, W) where H is height and W is width of the image.\n\n        Note:\n            - This assumes that the binary mask is consistent across all spectral bands.\n            - The returned mask is always 2D, regardless of the original data orientation.\n\n        Examples:\n            &gt;&gt;&gt; # If self.binary_mask has shape (100, 100, 5) with spectral_axis=2:\n            &gt;&gt;&gt; hsi_image = HSI(binary_mask=torch.rand(100, 100, 5), orientation=(\"H\", \"W\", \"C\"))\n            &gt;&gt;&gt; hsi_image.spatial_binary_mask.shape\n            torch.Size([100, 100])\n            &gt;&gt;&gt; If self.binary_mask has shape (5, 100, 100) with spectral_axis=0:\n            &gt;&gt;&gt; hsi_image = HSI(binary_mask=torch.rand(5, 100, 100), orientation=(\"C\", \"H\", \"W\"))\n            &gt;&gt;&gt; hsi_image.spatial_binary_mask.shape\n            torch.Size([100, 100])\n        \"\"\"\n        mask = self.binary_mask if self.binary_mask is not None else torch.ones_like(self.image)\n        return mask.select(dim=self.spectral_axis, index=0)\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @model_validator(mode=\"after\")\n    def validate_image_data(self) -&gt; Self:\n        \"\"\"Validates the image data by checking the shape of the wavelengths, image, and spectral_axis.\n\n        Returns:\n            Self: The instance of the class.\n        \"\"\"\n        validate_shapes(self.wavelengths, self.image, self.spectral_axis)\n        return self\n\n    def to(self, device: str | torch.device) -&gt; Self:\n        \"\"\"Moves the image and binary mask (if available) to the specified device.\n\n        Args:\n            device (str or torch.device): The device to move the image and binary mask to.\n\n        Returns:\n            Self: The updated HSI object.\n\n        Examples:\n            &gt;&gt;&gt; # Create an HSI object\n            &gt;&gt;&gt; hsi_image = HSI(image=torch.rand(10, 10, 10), wavelengths=np.arange(10))\n            &gt;&gt;&gt; # Move the image to cpu\n            &gt;&gt;&gt; hsi_image = hsi_image.to(\"cpu\")\n            &gt;&gt;&gt; hsi_image.device\n            device(type='cpu')\n            &gt;&gt;&gt; # Move the image to cuda\n            &gt;&gt;&gt; hsi_image = hsi_image.to(\"cuda\")\n            &gt;&gt;&gt; hsi_image.device\n            device(type='cuda', index=0)\n        \"\"\"\n        self.image = self.image.to(device)\n        self.binary_mask = self.binary_mask.to(device)\n        self.device = self.image.device\n        return self\n\n    def get_image(self, apply_mask: bool = True) -&gt; torch.Tensor:\n        \"\"\"Returns the hyperspectral image data with optional masking applied.\n\n        Args:\n            apply_mask (bool, optional): Whether to apply the binary mask to the image.\n                Defaults to True.\n        Returns:\n            torch.Tensor: The hyperspectral image data.\n\n        Notes:\n            - If apply_mask is True, the binary mask will be applied to the image based on the `binary_mask` attribute.\n\n        Examples:\n            &gt;&gt;&gt; hsi_image = HSI(image=torch.rand(10, 100, 100), wavelengths=np.linspace(400, 1000, 10))\n            &gt;&gt;&gt; image = hsi_image.get_image()\n            &gt;&gt;&gt; image.shape\n            torch.Size([10, 100, 100])\n            &gt;&gt;&gt; image = hsi_image.get_image(apply_mask=False)\n            &gt;&gt;&gt; image.shape\n            torch.Size([10, 100, 100])\n        \"\"\"\n        if apply_mask and self.binary_mask is not None:\n            return self.image * self.binary_mask\n        return self.image\n\n    def get_rgb_image(\n        self, apply_mask: bool = True, apply_min_cutoff: bool = False, output_channel_axis: int | None = None\n    ) -&gt; torch.Tensor:\n        \"\"\"Extracts an RGB representation from the hyperspectral image data.\n\n        This method creates a 3-channel RGB image by selecting appropriate bands\n        corresponding to red, green, and blue wavelengths from the hyperspectral data.\n\n        Args:\n            apply_mask (bool, optional): Whether to apply the binary mask to the image.\n                Defaults to True.\n            apply_min_cutoff (bool, optional): Whether to apply a minimum intensity\n                cutoff to the image. Defaults to False.\n            output_channel_axis (int | None, optional): The axis where the RGB channels\n                should be placed in the output tensor. If None, uses the current spectral\n                axis of the hyperspectral data. Defaults to None.\n\n        Returns:\n            torch.Tensor: The RGB representation of the hyperspectral image.\n                Shape will be either (H, W, 3), (3, H, W), or (H, 3, W) depending on\n                the specified output_channel_axis, where H is height and W is width.\n\n        Notes:\n            - The RGB bands are extracted using predefined wavelength ranges for R, G, and B.\n            - Each band is normalized independently before combining into the RGB image.\n            - If apply_mask is True, masked areas will be set to zero in the output.\n            - If apply_min_cutoff is True, a minimum intensity threshold is applied to each band.\n\n        Examples:\n            &gt;&gt;&gt; hsi_image = HSI(image=torch.rand(10, 100, 100), wavelengths=np.linspace(400, 1000, 10))\n            &gt;&gt;&gt; rgb_image = hsi_image.get_rgb_image()\n            &gt;&gt;&gt; rgb_image.shape\n            torch.Size([100, 100, 3])\n\n            &gt;&gt;&gt; rgb_image = hsi_image.get_rgb_image(output_channel_axis=0)\n            &gt;&gt;&gt; rgb_image.shape\n            torch.Size([3, 100, 100])\n\n            &gt;&gt;&gt; rgb_image = hsi_image.get_rgb_image(apply_mask=False, apply_min_cutoff=True)\n            &gt;&gt;&gt; rgb_image.shape\n            torch.Size([100, 100, 3])\n        \"\"\"\n        if output_channel_axis is None:\n            output_channel_axis = self.spectral_axis\n\n        rgb_img = torch.stack(\n            [\n                self.extract_band_by_name(\n                    band, apply_mask=apply_mask, apply_min_cutoff=apply_min_cutoff, normalize=True\n                )\n                for band in [\"R\", \"G\", \"B\"]\n            ],\n            dim=self.spectral_axis,\n        )\n\n        return (\n            rgb_img\n            if output_channel_axis == self.spectral_axis\n            else torch.moveaxis(rgb_img, self.spectral_axis, output_channel_axis)\n        )\n\n    def _extract_central_slice_from_band(\n        self,\n        band_wavelengths: torch.Tensor,\n        apply_mask: bool = True,\n        apply_min_cutoff: bool = False,\n        normalize: bool = True,\n    ) -&gt; torch.Tensor:\n        \"\"\"Extracts and processes the central wavelength band from a given range in the hyperspectral image.\n\n        This method selects the central band from a specified range of wavelengths,\n        applies optional processing steps (masking, normalization, and minimum cutoff),\n        and returns the resulting 2D image slice.\n\n        Args:\n            band_wavelengths (torch.Tensor): The selected wavelengths that define the whole band\n                from which the central slice will be extracted.\n                All of the passed wavelengths must be present in the image.\n            apply_mask (bool, optional): Whether to apply the binary mask to the extracted band.\n                Defaults to True.\n            apply_min_cutoff (bool, optional): Whether to apply a minimum intensity cutoff.\n                If True, sets the minimum non-zero value to zero after normalization.\n                Defaults to False.\n            normalize (bool, optional): Whether to normalize the band values to [0, 1] range.\n                Defaults to True.\n\n        Returns:\n            torch.Tensor: A 2D tensor representing the processed central wavelength band.\n                Shape will be (H, W), where H is height and W is width of the image.\n\n        Notes:\n            - The central wavelength is determined as the middle index of the provided wavelengths list.\n            - If normalization is applied, it's done before masking and cutoff operations.\n            - The binary mask, if applied, is expected to have the same spatial dimensions as the image.\n\n        Examples:\n            &gt;&gt;&gt; hsi_image = HSI(image=torch.rand(13, 100, 100), wavelengths=np.linspace(400, 1000, 13))\n            &gt;&gt;&gt; band_wavelengths = torch.tensor([500, 600, 650, 700])\n            &gt;&gt;&gt; central_slice = hsi_image._extract_central_slice_from_band(band_wavelengths)\n            &gt;&gt;&gt; central_slice.shape\n            torch.Size([100, 100])\n\n            &gt;&gt;&gt; # Extract a slice without normalization or masking\n            &gt;&gt;&gt; raw_band = hsi_image._extract_central_slice_from_band(band_wavelengths, apply_mask=False, normalize=False)\n        \"\"\"\n        # check if all wavelengths from the `band_wavelengths` are present in the image\n        if not all(wave in self.wavelengths for wave in band_wavelengths):\n            raise ValueError(\"All of the passed wavelengths must be present in the image\")\n\n        # sort the `band_wavelengths` to ensure the central band is selected\n        band_wavelengths = torch.sort(band_wavelengths).values\n\n        start_index = np.where(self.wavelengths == band_wavelengths[0])[0][0]\n        relative_center_band_index = len(band_wavelengths) // 2\n        central_band_index = start_index + relative_center_band_index\n\n        # Ensure the spectral dimension is the last\n        image = self.image if self.spectral_axis == 2 else torch.moveaxis(self.image, self.spectral_axis, 2)\n\n        slice = image[..., central_band_index]\n\n        if normalize:\n            if apply_min_cutoff:\n                slice_min = slice[slice != 0].min()\n            else:\n                slice_min = slice.min()\n\n            slice_max = slice.max()\n            if slice_max &gt; slice_min:  # Avoid division by zero\n                slice = (slice - slice_min) / (slice_max - slice_min)\n\n            if apply_min_cutoff:\n                slice[slice == slice.min()] = 0  # Set minimum values to zero\n\n        if apply_mask:\n            mask = (\n                self.binary_mask if self.spectral_axis == 2 else torch.moveaxis(self.binary_mask, self.spectral_axis, 2)\n            )\n            slice = slice * mask[..., central_band_index]\n\n        return slice\n\n    def extract_band_by_name(\n        self,\n        band_name: str,\n        selection_method: str = \"center\",\n        apply_mask: bool = True,\n        apply_min_cutoff: bool = False,\n        normalize: bool = True,\n    ) -&gt; torch.Tensor:\n        \"\"\"Extracts a single spectral band from the hyperspectral image based on a standardized band name.\n\n        This method uses the spyndex library to map standardized band names to wavelength ranges,\n        then extracts the corresponding band from the hyperspectral data.\n\n        Args:\n            band_name (str): The standardized name of the band to extract (e.g., \"Red\", \"NIR\", \"SWIR1\").\n            selection_method (str, optional): The method to use for selecting the band within the wavelength range.\n                Currently, only \"center\" is supported, which selects the central wavelength.\n                Defaults to \"center\".\n            apply_mask (bool, optional): Whether to apply the binary mask to the extracted band.\n                Defaults to True.\n            apply_min_cutoff (bool, optional): Whether to apply a minimum intensity cutoff after normalization.\n                If True, sets the minimum non-zero value to zero. Defaults to False.\n            normalize (bool, optional): Whether to normalize the band values to the [0, 1] range.\n                Defaults to True.\n\n        Returns:\n            torch.Tensor: A 2D tensor representing the extracted and processed spectral band.\n                Shape will be (H, W), where H is height and W is width of the image.\n\n        Raises:\n            BandSelectionError: If the specified band name is not found in the spyndex library.\n            NotImplementedError: If a selection method other than \"center\" is specified.\n\n        Notes:\n            - The spyndex library is used to map band names to wavelength ranges.\n            - Currently, only the \"center\" selection method is implemented, which chooses\n            the central wavelength within the specified range.\n            - Processing steps are applied in the order: normalization, cutoff, masking.\n\n        Examples:\n            &gt;&gt;&gt; hsi_image = HSI(image=torch.rand(200, 100, 100), wavelengths=np.linspace(400, 2500, 200))\n            &gt;&gt;&gt; red_band = hsi_image.extract_band_by_name(\"Red\")\n            &gt;&gt;&gt; red_band.shape\n            torch.Size([100, 100])\n\n            &gt;&gt;&gt; # Extract NIR band without normalization or masking\n            &gt;&gt;&gt; nir_band = hsi_image.extract_band_by_name(\"NIR\", apply_mask=False, normalize=False)\n        \"\"\"\n        band_info = spyndex.bands.get(band_name)\n        if band_info is None:\n            raise BandSelectionError(f\"Band name '{band_name}' not found in the spyndex library\")\n\n        min_wave, max_wave = band_info.min_wavelength, band_info.max_wavelength\n        selected_wavelengths = self.wavelengths[(self.wavelengths &gt;= min_wave) &amp; (self.wavelengths &lt;= max_wave)]\n\n        if selection_method == \"center\":\n            return self._extract_central_slice_from_band(\n                selected_wavelengths, apply_mask=apply_mask, apply_min_cutoff=apply_min_cutoff, normalize=normalize\n            )\n        else:\n            raise NotImplementedError(\n                f\"Selection method '{selection_method}' is not supported. Only 'center' is currently available.\"\n            )\n\n    def change_orientation(self, target_orientation: tuple[str, str, str] | list[str] | str, inplace=False) -&gt; Self:\n        \"\"\"Changes the orientation of the hsi data to the target orientation.\n\n        Args:\n            target_orientation (tuple[str, str, str], list[str], str): The target orientation for the hsi data.\n                This should be a tuple of three one-letter strings in any order: \"C\", \"H\", \"W\".\n            inplace (bool, optional): Whether to modify the hsi data in place or return a new object.\n\n        Returns:\n            Self: The updated HSI object with the new orientation.\n\n        Raises:\n            ValueError: If the target orientation is not a valid tuple of three one-letter strings.\n        \"\"\"\n        target_orientation = validate_orientation(target_orientation)\n\n        if inplace:\n            hsi = self\n        else:\n            hsi = self.model_copy()\n\n        if target_orientation == self.orientation:\n            return hsi\n\n        permute_dims = [hsi.orientation.index(dim) for dim in target_orientation]\n\n        # permute the image\n        hsi.image = hsi.image.permute(permute_dims)\n\n        # permute the binary mask\n        if hsi.binary_mask is not None:\n            hsi.binary_mask = hsi.binary_mask.permute(permute_dims)\n\n        hsi.orientation = target_orientation\n\n        return hsi\n</code></pre>"},{"location":"reference/#src.meteors.hsi.HSI.spatial_binary_mask","title":"<code>spatial_binary_mask: torch.Tensor</code>  <code>property</code>","text":"<p>Returns a 2D spatial representation of the binary mask.</p> <p>This property extracts a single 2D slice from the 3D binary mask, assuming that the mask is identical across all spectral bands. It handles different data orientations by first ensuring the spectral dimension is the last dimension before extracting the 2D spatial mask.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A 2D tensor representing the spatial binary mask. The shape will be (H, W) where H is height and W is width of the image.</p> Note <ul> <li>This assumes that the binary mask is consistent across all spectral bands.</li> <li>The returned mask is always 2D, regardless of the original data orientation.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # If self.binary_mask has shape (100, 100, 5) with spectral_axis=2:\n&gt;&gt;&gt; hsi_image = HSI(binary_mask=torch.rand(100, 100, 5), orientation=(\"H\", \"W\", \"C\"))\n&gt;&gt;&gt; hsi_image.spatial_binary_mask.shape\ntorch.Size([100, 100])\n&gt;&gt;&gt; If self.binary_mask has shape (5, 100, 100) with spectral_axis=0:\n&gt;&gt;&gt; hsi_image = HSI(binary_mask=torch.rand(5, 100, 100), orientation=(\"C\", \"H\", \"W\"))\n&gt;&gt;&gt; hsi_image.spatial_binary_mask.shape\ntorch.Size([100, 100])\n</code></pre>"},{"location":"reference/#src.meteors.hsi.HSI.spectral_axis","title":"<code>spectral_axis: int</code>  <code>property</code>","text":"<p>Returns the index of the spectral (wavelength) axis based on the current data orientation.</p> <p>In hyperspectral imaging, the spectral axis represents the dimension along which different spectral bands or wavelengths are arranged. This property dynamically determines the index of this axis based on the current orientation of the data.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The index of the spectral axis in the current data structure. - 0 for 'CHW' or 'CWH' orientations (Channel/Wavelength first) - 2 for 'HWC' or 'WHC' orientations (Channel/Wavelength last) - 1 for 'HCW' or 'WCH' orientations (Channel/Wavelength in the middle)</p> Note <p>The orientation is typically represented as a string where: - 'C' represents the spectral/wavelength dimension - 'H' represents the height (rows) of the image - 'W' represents the width (columns) of the image</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hsi_image = HSI()\n&gt;&gt;&gt; hsi_image.orientation = \"CHW\"\n&gt;&gt;&gt; hsi_image.spectral_axis\n0\n&gt;&gt;&gt; hsi_image.orientation = \"HWC\"\n&gt;&gt;&gt; hsi_image.spectral_axis\n2\n</code></pre>"},{"location":"reference/#src.meteors.hsi.HSI.change_orientation","title":"<code>change_orientation(target_orientation, inplace=False)</code>","text":"<p>Changes the orientation of the hsi data to the target orientation.</p> <p>Parameters:</p> Name Type Description Default <code>target_orientation</code> <code>(tuple[str, str, str], list[str], str)</code> <p>The target orientation for the hsi data. This should be a tuple of three one-letter strings in any order: \"C\", \"H\", \"W\".</p> required <code>inplace</code> <code>bool</code> <p>Whether to modify the hsi data in place or return a new object.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>The updated HSI object with the new orientation.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the target orientation is not a valid tuple of three one-letter strings.</p> Source code in <code>src/meteors/hsi.py</code> <pre><code>def change_orientation(self, target_orientation: tuple[str, str, str] | list[str] | str, inplace=False) -&gt; Self:\n    \"\"\"Changes the orientation of the hsi data to the target orientation.\n\n    Args:\n        target_orientation (tuple[str, str, str], list[str], str): The target orientation for the hsi data.\n            This should be a tuple of three one-letter strings in any order: \"C\", \"H\", \"W\".\n        inplace (bool, optional): Whether to modify the hsi data in place or return a new object.\n\n    Returns:\n        Self: The updated HSI object with the new orientation.\n\n    Raises:\n        ValueError: If the target orientation is not a valid tuple of three one-letter strings.\n    \"\"\"\n    target_orientation = validate_orientation(target_orientation)\n\n    if inplace:\n        hsi = self\n    else:\n        hsi = self.model_copy()\n\n    if target_orientation == self.orientation:\n        return hsi\n\n    permute_dims = [hsi.orientation.index(dim) for dim in target_orientation]\n\n    # permute the image\n    hsi.image = hsi.image.permute(permute_dims)\n\n    # permute the binary mask\n    if hsi.binary_mask is not None:\n        hsi.binary_mask = hsi.binary_mask.permute(permute_dims)\n\n    hsi.orientation = target_orientation\n\n    return hsi\n</code></pre>"},{"location":"reference/#src.meteors.hsi.HSI.extract_band_by_name","title":"<code>extract_band_by_name(band_name, selection_method='center', apply_mask=True, apply_min_cutoff=False, normalize=True)</code>","text":"<p>Extracts a single spectral band from the hyperspectral image based on a standardized band name.</p> <p>This method uses the spyndex library to map standardized band names to wavelength ranges, then extracts the corresponding band from the hyperspectral data.</p> <p>Parameters:</p> Name Type Description Default <code>band_name</code> <code>str</code> <p>The standardized name of the band to extract (e.g., \"Red\", \"NIR\", \"SWIR1\").</p> required <code>selection_method</code> <code>str</code> <p>The method to use for selecting the band within the wavelength range. Currently, only \"center\" is supported, which selects the central wavelength. Defaults to \"center\".</p> <code>'center'</code> <code>apply_mask</code> <code>bool</code> <p>Whether to apply the binary mask to the extracted band. Defaults to True.</p> <code>True</code> <code>apply_min_cutoff</code> <code>bool</code> <p>Whether to apply a minimum intensity cutoff after normalization. If True, sets the minimum non-zero value to zero. Defaults to False.</p> <code>False</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize the band values to the [0, 1] range. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A 2D tensor representing the extracted and processed spectral band. Shape will be (H, W), where H is height and W is width of the image.</p> <p>Raises:</p> Type Description <code>BandSelectionError</code> <p>If the specified band name is not found in the spyndex library.</p> <code>NotImplementedError</code> <p>If a selection method other than \"center\" is specified.</p> Notes <ul> <li>The spyndex library is used to map band names to wavelength ranges.</li> <li>Currently, only the \"center\" selection method is implemented, which chooses the central wavelength within the specified range.</li> <li>Processing steps are applied in the order: normalization, cutoff, masking.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hsi_image = HSI(image=torch.rand(200, 100, 100), wavelengths=np.linspace(400, 2500, 200))\n&gt;&gt;&gt; red_band = hsi_image.extract_band_by_name(\"Red\")\n&gt;&gt;&gt; red_band.shape\ntorch.Size([100, 100])\n</code></pre> <pre><code>&gt;&gt;&gt; # Extract NIR band without normalization or masking\n&gt;&gt;&gt; nir_band = hsi_image.extract_band_by_name(\"NIR\", apply_mask=False, normalize=False)\n</code></pre> Source code in <code>src/meteors/hsi.py</code> <pre><code>def extract_band_by_name(\n    self,\n    band_name: str,\n    selection_method: str = \"center\",\n    apply_mask: bool = True,\n    apply_min_cutoff: bool = False,\n    normalize: bool = True,\n) -&gt; torch.Tensor:\n    \"\"\"Extracts a single spectral band from the hyperspectral image based on a standardized band name.\n\n    This method uses the spyndex library to map standardized band names to wavelength ranges,\n    then extracts the corresponding band from the hyperspectral data.\n\n    Args:\n        band_name (str): The standardized name of the band to extract (e.g., \"Red\", \"NIR\", \"SWIR1\").\n        selection_method (str, optional): The method to use for selecting the band within the wavelength range.\n            Currently, only \"center\" is supported, which selects the central wavelength.\n            Defaults to \"center\".\n        apply_mask (bool, optional): Whether to apply the binary mask to the extracted band.\n            Defaults to True.\n        apply_min_cutoff (bool, optional): Whether to apply a minimum intensity cutoff after normalization.\n            If True, sets the minimum non-zero value to zero. Defaults to False.\n        normalize (bool, optional): Whether to normalize the band values to the [0, 1] range.\n            Defaults to True.\n\n    Returns:\n        torch.Tensor: A 2D tensor representing the extracted and processed spectral band.\n            Shape will be (H, W), where H is height and W is width of the image.\n\n    Raises:\n        BandSelectionError: If the specified band name is not found in the spyndex library.\n        NotImplementedError: If a selection method other than \"center\" is specified.\n\n    Notes:\n        - The spyndex library is used to map band names to wavelength ranges.\n        - Currently, only the \"center\" selection method is implemented, which chooses\n        the central wavelength within the specified range.\n        - Processing steps are applied in the order: normalization, cutoff, masking.\n\n    Examples:\n        &gt;&gt;&gt; hsi_image = HSI(image=torch.rand(200, 100, 100), wavelengths=np.linspace(400, 2500, 200))\n        &gt;&gt;&gt; red_band = hsi_image.extract_band_by_name(\"Red\")\n        &gt;&gt;&gt; red_band.shape\n        torch.Size([100, 100])\n\n        &gt;&gt;&gt; # Extract NIR band without normalization or masking\n        &gt;&gt;&gt; nir_band = hsi_image.extract_band_by_name(\"NIR\", apply_mask=False, normalize=False)\n    \"\"\"\n    band_info = spyndex.bands.get(band_name)\n    if band_info is None:\n        raise BandSelectionError(f\"Band name '{band_name}' not found in the spyndex library\")\n\n    min_wave, max_wave = band_info.min_wavelength, band_info.max_wavelength\n    selected_wavelengths = self.wavelengths[(self.wavelengths &gt;= min_wave) &amp; (self.wavelengths &lt;= max_wave)]\n\n    if selection_method == \"center\":\n        return self._extract_central_slice_from_band(\n            selected_wavelengths, apply_mask=apply_mask, apply_min_cutoff=apply_min_cutoff, normalize=normalize\n        )\n    else:\n        raise NotImplementedError(\n            f\"Selection method '{selection_method}' is not supported. Only 'center' is currently available.\"\n        )\n</code></pre>"},{"location":"reference/#src.meteors.hsi.HSI.get_image","title":"<code>get_image(apply_mask=True)</code>","text":"<p>Returns the hyperspectral image data with optional masking applied.</p> <p>Parameters:</p> Name Type Description Default <code>apply_mask</code> <code>bool</code> <p>Whether to apply the binary mask to the image. Defaults to True.</p> <code>True</code> <p>Returns:     torch.Tensor: The hyperspectral image data.</p> Notes <ul> <li>If apply_mask is True, the binary mask will be applied to the image based on the <code>binary_mask</code> attribute.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hsi_image = HSI(image=torch.rand(10, 100, 100), wavelengths=np.linspace(400, 1000, 10))\n&gt;&gt;&gt; image = hsi_image.get_image()\n&gt;&gt;&gt; image.shape\ntorch.Size([10, 100, 100])\n&gt;&gt;&gt; image = hsi_image.get_image(apply_mask=False)\n&gt;&gt;&gt; image.shape\ntorch.Size([10, 100, 100])\n</code></pre> Source code in <code>src/meteors/hsi.py</code> <pre><code>def get_image(self, apply_mask: bool = True) -&gt; torch.Tensor:\n    \"\"\"Returns the hyperspectral image data with optional masking applied.\n\n    Args:\n        apply_mask (bool, optional): Whether to apply the binary mask to the image.\n            Defaults to True.\n    Returns:\n        torch.Tensor: The hyperspectral image data.\n\n    Notes:\n        - If apply_mask is True, the binary mask will be applied to the image based on the `binary_mask` attribute.\n\n    Examples:\n        &gt;&gt;&gt; hsi_image = HSI(image=torch.rand(10, 100, 100), wavelengths=np.linspace(400, 1000, 10))\n        &gt;&gt;&gt; image = hsi_image.get_image()\n        &gt;&gt;&gt; image.shape\n        torch.Size([10, 100, 100])\n        &gt;&gt;&gt; image = hsi_image.get_image(apply_mask=False)\n        &gt;&gt;&gt; image.shape\n        torch.Size([10, 100, 100])\n    \"\"\"\n    if apply_mask and self.binary_mask is not None:\n        return self.image * self.binary_mask\n    return self.image\n</code></pre>"},{"location":"reference/#src.meteors.hsi.HSI.get_rgb_image","title":"<code>get_rgb_image(apply_mask=True, apply_min_cutoff=False, output_channel_axis=None)</code>","text":"<p>Extracts an RGB representation from the hyperspectral image data.</p> <p>This method creates a 3-channel RGB image by selecting appropriate bands corresponding to red, green, and blue wavelengths from the hyperspectral data.</p> <p>Parameters:</p> Name Type Description Default <code>apply_mask</code> <code>bool</code> <p>Whether to apply the binary mask to the image. Defaults to True.</p> <code>True</code> <code>apply_min_cutoff</code> <code>bool</code> <p>Whether to apply a minimum intensity cutoff to the image. Defaults to False.</p> <code>False</code> <code>output_channel_axis</code> <code>int | None</code> <p>The axis where the RGB channels should be placed in the output tensor. If None, uses the current spectral axis of the hyperspectral data. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The RGB representation of the hyperspectral image. Shape will be either (H, W, 3), (3, H, W), or (H, 3, W) depending on the specified output_channel_axis, where H is height and W is width.</p> Notes <ul> <li>The RGB bands are extracted using predefined wavelength ranges for R, G, and B.</li> <li>Each band is normalized independently before combining into the RGB image.</li> <li>If apply_mask is True, masked areas will be set to zero in the output.</li> <li>If apply_min_cutoff is True, a minimum intensity threshold is applied to each band.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hsi_image = HSI(image=torch.rand(10, 100, 100), wavelengths=np.linspace(400, 1000, 10))\n&gt;&gt;&gt; rgb_image = hsi_image.get_rgb_image()\n&gt;&gt;&gt; rgb_image.shape\ntorch.Size([100, 100, 3])\n</code></pre> <pre><code>&gt;&gt;&gt; rgb_image = hsi_image.get_rgb_image(output_channel_axis=0)\n&gt;&gt;&gt; rgb_image.shape\ntorch.Size([3, 100, 100])\n</code></pre> <pre><code>&gt;&gt;&gt; rgb_image = hsi_image.get_rgb_image(apply_mask=False, apply_min_cutoff=True)\n&gt;&gt;&gt; rgb_image.shape\ntorch.Size([100, 100, 3])\n</code></pre> Source code in <code>src/meteors/hsi.py</code> <pre><code>def get_rgb_image(\n    self, apply_mask: bool = True, apply_min_cutoff: bool = False, output_channel_axis: int | None = None\n) -&gt; torch.Tensor:\n    \"\"\"Extracts an RGB representation from the hyperspectral image data.\n\n    This method creates a 3-channel RGB image by selecting appropriate bands\n    corresponding to red, green, and blue wavelengths from the hyperspectral data.\n\n    Args:\n        apply_mask (bool, optional): Whether to apply the binary mask to the image.\n            Defaults to True.\n        apply_min_cutoff (bool, optional): Whether to apply a minimum intensity\n            cutoff to the image. Defaults to False.\n        output_channel_axis (int | None, optional): The axis where the RGB channels\n            should be placed in the output tensor. If None, uses the current spectral\n            axis of the hyperspectral data. Defaults to None.\n\n    Returns:\n        torch.Tensor: The RGB representation of the hyperspectral image.\n            Shape will be either (H, W, 3), (3, H, W), or (H, 3, W) depending on\n            the specified output_channel_axis, where H is height and W is width.\n\n    Notes:\n        - The RGB bands are extracted using predefined wavelength ranges for R, G, and B.\n        - Each band is normalized independently before combining into the RGB image.\n        - If apply_mask is True, masked areas will be set to zero in the output.\n        - If apply_min_cutoff is True, a minimum intensity threshold is applied to each band.\n\n    Examples:\n        &gt;&gt;&gt; hsi_image = HSI(image=torch.rand(10, 100, 100), wavelengths=np.linspace(400, 1000, 10))\n        &gt;&gt;&gt; rgb_image = hsi_image.get_rgb_image()\n        &gt;&gt;&gt; rgb_image.shape\n        torch.Size([100, 100, 3])\n\n        &gt;&gt;&gt; rgb_image = hsi_image.get_rgb_image(output_channel_axis=0)\n        &gt;&gt;&gt; rgb_image.shape\n        torch.Size([3, 100, 100])\n\n        &gt;&gt;&gt; rgb_image = hsi_image.get_rgb_image(apply_mask=False, apply_min_cutoff=True)\n        &gt;&gt;&gt; rgb_image.shape\n        torch.Size([100, 100, 3])\n    \"\"\"\n    if output_channel_axis is None:\n        output_channel_axis = self.spectral_axis\n\n    rgb_img = torch.stack(\n        [\n            self.extract_band_by_name(\n                band, apply_mask=apply_mask, apply_min_cutoff=apply_min_cutoff, normalize=True\n            )\n            for band in [\"R\", \"G\", \"B\"]\n        ],\n        dim=self.spectral_axis,\n    )\n\n    return (\n        rgb_img\n        if output_channel_axis == self.spectral_axis\n        else torch.moveaxis(rgb_img, self.spectral_axis, output_channel_axis)\n    )\n</code></pre>"},{"location":"reference/#src.meteors.hsi.HSI.to","title":"<code>to(device)</code>","text":"<p>Moves the image and binary mask (if available) to the specified device.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str or device</code> <p>The device to move the image and binary mask to.</p> required <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>The updated HSI object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Create an HSI object\n&gt;&gt;&gt; hsi_image = HSI(image=torch.rand(10, 10, 10), wavelengths=np.arange(10))\n&gt;&gt;&gt; # Move the image to cpu\n&gt;&gt;&gt; hsi_image = hsi_image.to(\"cpu\")\n&gt;&gt;&gt; hsi_image.device\ndevice(type='cpu')\n&gt;&gt;&gt; # Move the image to cuda\n&gt;&gt;&gt; hsi_image = hsi_image.to(\"cuda\")\n&gt;&gt;&gt; hsi_image.device\ndevice(type='cuda', index=0)\n</code></pre> Source code in <code>src/meteors/hsi.py</code> <pre><code>def to(self, device: str | torch.device) -&gt; Self:\n    \"\"\"Moves the image and binary mask (if available) to the specified device.\n\n    Args:\n        device (str or torch.device): The device to move the image and binary mask to.\n\n    Returns:\n        Self: The updated HSI object.\n\n    Examples:\n        &gt;&gt;&gt; # Create an HSI object\n        &gt;&gt;&gt; hsi_image = HSI(image=torch.rand(10, 10, 10), wavelengths=np.arange(10))\n        &gt;&gt;&gt; # Move the image to cpu\n        &gt;&gt;&gt; hsi_image = hsi_image.to(\"cpu\")\n        &gt;&gt;&gt; hsi_image.device\n        device(type='cpu')\n        &gt;&gt;&gt; # Move the image to cuda\n        &gt;&gt;&gt; hsi_image = hsi_image.to(\"cuda\")\n        &gt;&gt;&gt; hsi_image.device\n        device(type='cuda', index=0)\n    \"\"\"\n    self.image = self.image.to(device)\n    self.binary_mask = self.binary_mask.to(device)\n    self.device = self.image.device\n    return self\n</code></pre>"},{"location":"reference/#visualizations","title":"Visualizations","text":""},{"location":"reference/#src.meteors.visualize.hsi_visualize.visualize_hsi","title":"<code>visualize_hsi(hsi_or_attributes, ax=None, use_mask=True)</code>","text":"<p>Visualizes a Hyperspectral image object on the given axes. It uses either the object from HSI class or a field from the HSIAttributes class.</p> <p>Parameters:</p> Name Type Description Default <code>hsi_or_attributes</code> <code>HSI | HSIAttributes</code> <p>The hyperspectral image, or the attributes to be visualized.</p> required <code>ax</code> <code>Axes | None</code> <p>The axes on which the image will be plotted. If None, the current axes will be used.</p> <code>None</code> <code>use_mask</code> <code>bool</code> <p>Whether to use the image mask if provided for the visualization.</p> <code>True</code> <p>Returns:</p> Type Description <code>Axes</code> <p>matplotlib.figure.Figure | None: If use_pyplot is False, returns the figure and axes objects. If use_pyplot is True, returns None.</p> <p>Raises:     TypeError: If hsi_or_attributes is not an instance of HSI or HSIAttributes.</p> Source code in <code>src/meteors/visualize/hsi_visualize.py</code> <pre><code>def visualize_hsi(hsi_or_attributes: HSI | HSIAttributes, ax: Axes | None = None, use_mask: bool = True) -&gt; Axes:\n    \"\"\"Visualizes a Hyperspectral image object on the given axes. It uses either the object from HSI class or a field\n    from the HSIAttributes class.\n\n    Parameters:\n        hsi_or_attributes (HSI | HSIAttributes): The hyperspectral image, or the attributes to be visualized.\n        ax (matplotlib.axes.Axes | None): The axes on which the image will be plotted.\n            If None, the current axes will be used.\n        use_mask (bool): Whether to use the image mask if provided for the visualization.\n\n\n    Returns:\n        matplotlib.figure.Figure | None:\n            If use_pyplot is False, returns the figure and axes objects.\n            If use_pyplot is True, returns None.\n    Raises:\n        TypeError: If hsi_or_attributes is not an instance of HSI or HSIAttributes.\n    \"\"\"\n    if isinstance(hsi_or_attributes, HSIAttributes):\n        hsi = hsi_or_attributes.hsi\n    else:\n        hsi = hsi_or_attributes\n\n    if not isinstance(hsi, HSI):\n        raise TypeError(\"hsi_or_attributes must be an instance of HSI or HSIAttributes.\")\n\n    hsi = hsi.change_orientation(\"HWC\", inplace=False)\n\n    rgb = hsi.get_rgb_image(output_channel_axis=2, apply_mask=use_mask).cpu().numpy()\n    ax = ax or plt.gca()\n    ax.imshow(rgb)\n\n    return ax\n</code></pre>"},{"location":"reference/#src.meteors.visualize.attr_visualize.visualize_attributes","title":"<code>visualize_attributes(image_attributes, use_pyplot=False)</code>","text":"<p>Visualizes the attributes of an image on the given axes.</p> <p>Parameters:</p> Name Type Description Default <code>image_attributes</code> <code>HSIAttributes</code> <p>The image attributes to be visualized.</p> required <code>use_pyplot</code> <code>bool</code> <p>If True, uses pyplot to display the image. If False, returns the figure and axes objects.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[Figure, Axes] | None</code> <p>matplotlib.figure.Figure | None: If use_pyplot is False, returns the figure and axes objects. If use_pyplot is True, returns None . If all the attributions are zero, returns None.</p> Source code in <code>src/meteors/visualize/attr_visualize.py</code> <pre><code>def visualize_attributes(image_attributes: HSIAttributes, use_pyplot: bool = False) -&gt; tuple[Figure, Axes] | None:\n    \"\"\"Visualizes the attributes of an image on the given axes.\n\n    Parameters:\n        image_attributes (HSIAttributes): The image attributes to be visualized.\n        use_pyplot (bool): If True, uses pyplot to display the image. If False, returns the figure and axes objects.\n\n    Returns:\n        matplotlib.figure.Figure | None:\n            If use_pyplot is False, returns the figure and axes objects.\n            If use_pyplot is True, returns None .\n            If all the attributions are zero, returns None.\n    \"\"\"\n    if image_attributes.hsi.orientation != (\"C\", \"H\", \"W\"):\n        logger.info(\n            f\"The orientation of the image is not (H, W, C): {image_attributes.hsi.orientation}. \"\n            f\"Changing it to (H, W, C) for visualization.\"\n        )\n        rotated_attributes_dataclass = image_attributes.change_orientation(\"CHW\", inplace=False)\n    else:\n        rotated_attributes_dataclass = image_attributes\n\n    rotated_attributes = rotated_attributes_dataclass.attributes.detach().cpu().numpy()\n    if np.all(rotated_attributes == 0):\n        warnings.warn(\"All the attributions are zero. There is nothing to visualize.\")\n        return None\n\n    fig, ax = plt.subplots(2, 2, figsize=(9, 7))\n    ax[0, 0].set_title(\"Attribution Heatmap\")\n    ax[0, 0].grid(False)\n    ax[0, 0].axis(\"off\")\n\n    fig.suptitle(f\"HSI Attributes of: {rotated_attributes_dataclass.attribution_method}\")\n\n    _ = viz.visualize_image_attr(\n        rotated_attributes,\n        method=\"heat_map\",\n        sign=\"all\",\n        plt_fig_axis=(fig, ax[0, 0]),\n        show_colorbar=True,\n        use_pyplot=False,\n    )\n\n    ax[0, 1].set_title(\"Attribution Module Values\")\n    ax[0, 1].grid(False)\n    ax[0, 1].axis(\"off\")\n\n    # Attributions module values\n    _ = viz.visualize_image_attr(\n        rotated_attributes,\n        method=\"heat_map\",\n        sign=\"absolute_value\",\n        plt_fig_axis=(fig, ax[0, 1]),\n        show_colorbar=True,\n        use_pyplot=False,\n    )\n\n    attr_all = rotated_attributes.sum(axis=(1, 2))\n    ax[1, 0].scatter(rotated_attributes_dataclass.hsi.wavelengths, attr_all, c=\"r\")\n    ax[1, 0].set_title(\"Spectral Attribution\")\n    ax[1, 0].set_xlabel(\"Wavelength\")\n    ax[1, 0].set_ylabel(\"Attribution\")\n    ax[1, 0].grid(True)\n\n    attr_abs = np.abs(rotated_attributes).sum(axis=(1, 2))\n    ax[1, 1].scatter(rotated_attributes_dataclass.hsi.wavelengths, attr_abs, c=\"b\")\n    ax[1, 1].set_title(\"Spectral Attribution Absolute Values\")\n    ax[1, 1].set_xlabel(\"Wavelength\")\n    ax[1, 1].set_ylabel(\"Attribution Absolute Value\")\n    ax[1, 1].grid(True)\n\n    plt.tight_layout()\n\n    if use_pyplot:\n        plt.show()  # pragma: no cover\n        return None  # pragma: no cover\n\n    return fig, ax\n</code></pre>"},{"location":"reference/#src.meteors.visualize.lime_visualize.visualize_spectral_attributes_by_waveband","title":"<code>visualize_spectral_attributes_by_waveband(spectral_attributes, ax, color_palette=None, show_not_included=True, show_legend=True)</code>","text":"<p>Visualizes spectral attributes by waveband.</p> <p>Parameters:</p> Name Type Description Default <code>spectral_attributes</code> <code>HSISpectralAttributes | list[HSISpectralAttributes]</code> <p>The spectral attributes to visualize.</p> required <code>ax</code> <code>Axes | None</code> <p>The matplotlib axes to plot the visualization on. If None, a new axes will be created.</p> required <code>color_palette</code> <code>list[str] | None</code> <p>The color palette to use for plotting. If None, a default color palette will be used.</p> <code>None</code> <code>show_not_included</code> <code>bool</code> <p>Whether to show the \"not_included\" band in the visualization. Default is True.</p> <code>True</code> <code>show_legend</code> <code>bool</code> <p>Whether to show the legend in the visualization.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Axes</code> <code>Axes</code> <p>The matplotlib axes object containing the visualization.</p> <p>Raises:     TypeError: If the spectral attributes are not an HSISpectralAttributes object or a list of HSISpectralAttributes objects.</p> Source code in <code>src/meteors/visualize/lime_visualize.py</code> <pre><code>def visualize_spectral_attributes_by_waveband(\n    spectral_attributes: HSISpectralAttributes | list[HSISpectralAttributes],\n    ax: Axes | None,\n    color_palette: list[str] | None = None,\n    show_not_included: bool = True,\n    show_legend: bool = True,\n) -&gt; Axes:\n    \"\"\"Visualizes spectral attributes by waveband.\n\n    Args:\n        spectral_attributes (HSISpectralAttributes | list[HSISpectralAttributes]):\n            The spectral attributes to visualize.\n        ax (Axes | None): The matplotlib axes to plot the visualization on.\n            If None, a new axes will be created.\n        color_palette (list[str] | None): The color palette to use for plotting.\n            If None, a default color palette will be used.\n        show_not_included (bool): Whether to show the \"not_included\" band in the visualization.\n            Default is True.\n        show_legend (bool): Whether to show the legend in the visualization.\n\n    Returns:\n        Axes: The matplotlib axes object containing the visualization.\n    Raises:\n        TypeError: If the spectral attributes are not an HSISpectralAttributes object or a list of HSISpectralAttributes objects.\n    \"\"\"\n    if isinstance(spectral_attributes, HSISpectralAttributes):\n        spectral_attributes = [spectral_attributes]\n    if not (\n        isinstance(spectral_attributes, list)\n        and all(isinstance(attr, HSISpectralAttributes) for attr in spectral_attributes)\n    ):\n        raise TypeError(\n            \"spectral_attributes parameter must be an HSISpectralAttributes object or a list of HSISpectralAttributes objects.\"\n        )\n\n    aggregate_results = False if len(spectral_attributes) == 1 else True\n    band_names = dict(spectral_attributes[0].band_names)\n    wavelengths = spectral_attributes[0].hsi.wavelengths\n    validate_consistent_band_and_wavelengths(band_names, wavelengths, spectral_attributes)\n\n    ax = setup_visualization(ax, \"Attributions by Waveband\", \"Wavelength (nm)\", \"Correlation with Output\")\n\n    if not show_not_included and band_names.get(\"not_included\") is not None:\n        band_names.pop(\"not_included\")\n\n    band_names = _merge_band_names_segments(band_names)  # type: ignore\n\n    if color_palette is None:\n        color_palette = sns.color_palette(\"hsv\", len(band_names.keys()))\n\n    band_mask = spectral_attributes[0].band_mask.cpu()\n    attribution_map = torch.stack([attr.flattened_attributes.cpu() for attr in spectral_attributes])\n\n    for idx, (band_name, segment_id) in enumerate(band_names.items()):\n        current_wavelengths = wavelengths[band_mask == segment_id]\n        current_attribution_map = attribution_map[:, band_mask == segment_id]\n\n        current_mean = current_attribution_map.numpy().mean(axis=0)\n        if aggregate_results:\n            lolims = current_attribution_map.numpy().min(axis=0)\n            uplims = current_attribution_map.numpy().max(axis=0)\n\n            ax.errorbar(\n                current_wavelengths.numpy(),\n                current_mean,\n                yerr=[current_mean - lolims, uplims - current_mean],\n                label=band_name,\n                color=color_palette[idx],\n                linestyle=\"--\",\n                marker=\"o\",\n                markersize=5,\n            )\n        else:\n            ax.scatter(\n                current_wavelengths.numpy(),\n                current_mean,\n                label=band_name,\n                color=color_palette[idx],\n            )\n\n    if show_legend:\n        ax.legend(title=\"SuperBand\")\n\n    return ax\n</code></pre>"},{"location":"reference/#src.meteors.visualize.lime_visualize.visualize_spectral_attributes_by_magnitude","title":"<code>visualize_spectral_attributes_by_magnitude(spectral_attributes, ax, color_palette=None, annotate_bars=True, show_not_included=True)</code>","text":"<p>Visualizes the spectral attributes by magnitude.</p> <p>Parameters:</p> Name Type Description Default <code>spectral_attributes</code> <code>HSISpectralAttributes | list[HSISpectralAttributes]</code> <p>The spectral attributes to visualize.</p> required <code>ax</code> <code>Axes | None</code> <p>The matplotlib Axes object to plot the visualization on. If None, a new Axes object will be created.</p> required <code>color_palette</code> <code>list[str] | None</code> <p>The color palette to use for the visualization. If None, a default color palette will be used.</p> <code>None</code> <code>annotate_bars</code> <code>bool</code> <p>Whether to annotate the bars with their magnitudes. Defaults to True.</p> <code>True</code> <code>show_not_included</code> <code>bool</code> <p>Whether to show the 'not_included' band in the visualization. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Axes</code> <code>Axes</code> <p>The matplotlib Axes object containing the visualization.</p> <p>Raises:     TypeError: If the spectral attributes are not an HSISpectralAttributes object or a list of HSISpectralAttributes objects.</p> Source code in <code>src/meteors/visualize/lime_visualize.py</code> <pre><code>def visualize_spectral_attributes_by_magnitude(\n    spectral_attributes: HSISpectralAttributes | list[HSISpectralAttributes],\n    ax: Axes | None,\n    color_palette: list[str] | None = None,\n    annotate_bars: bool = True,\n    show_not_included: bool = True,\n) -&gt; Axes:\n    \"\"\"Visualizes the spectral attributes by magnitude.\n\n    Args:\n        spectral_attributes (HSISpectralAttributes | list[HSISpectralAttributes]):\n            The spectral attributes to visualize.\n        ax (Axes | None): The matplotlib Axes object to plot the visualization on.\n            If None, a new Axes object will be created.\n        color_palette (list[str] | None): The color palette to use for the visualization.\n            If None, a default color palette will be used.\n        annotate_bars (bool): Whether to annotate the bars with their magnitudes.\n            Defaults to True.\n        show_not_included (bool): Whether to show the 'not_included' band in the visualization.\n            Defaults to True.\n\n    Returns:\n        Axes: The matplotlib Axes object containing the visualization.\n    Raises:\n        TypeError: If the spectral attributes are not an HSISpectralAttributes object or a list of HSISpectralAttributes objects.\n    \"\"\"\n    if isinstance(spectral_attributes, HSISpectralAttributes):\n        spectral_attributes = [spectral_attributes]\n    if not (\n        isinstance(spectral_attributes, list)\n        and all(isinstance(attr, HSISpectralAttributes) for attr in spectral_attributes)\n    ):\n        raise TypeError(\n            \"spectral_attributes parameter must be an HSISpectralAttributes object or a list of HSISpectralAttributes objects.\"\n        )\n\n    aggregate_results = False if len(spectral_attributes) == 1 else True\n    band_names = dict(spectral_attributes[0].band_names)\n    wavelengths = spectral_attributes[0].hsi.wavelengths\n    validate_consistent_band_and_wavelengths(band_names, wavelengths, spectral_attributes)\n\n    ax = setup_visualization(ax, \"Attributions by Magnitude\", \"Group\", \"Average Attribution Magnitude\")\n    ax.tick_params(axis=\"x\", rotation=45)\n\n    band_names = _merge_band_names_segments(band_names)  # type: ignore\n    labels = list(band_names.keys())\n\n    if not show_not_included and band_names.get(\"not_included\") is not None:\n        band_names.pop(\"not_included\")\n        labels = list(band_names.keys())\n\n    if color_palette is None:\n        color_palette = sns.color_palette(\"hsv\", len(band_names.keys()))\n\n    band_mask = spectral_attributes[0].band_mask.cpu()\n    attribution_map = torch.stack([attr.flattened_attributes.cpu() for attr in spectral_attributes])\n    avg_magnitudes = calculate_average_magnitudes(band_names, band_mask, attribution_map)\n\n    if aggregate_results:\n        boxplot = ax.boxplot(avg_magnitudes, labels=labels, patch_artist=True)\n        for patch, color in zip(boxplot[\"boxes\"], color_palette):\n            patch.set_facecolor(color)\n\n    else:\n        bars = ax.bar(labels, avg_magnitudes, color=color_palette)\n        if annotate_bars:\n            for bar in bars:\n                height = bar.get_height()\n                ax.annotate(\n                    f\"{height:.2f}\",\n                    xy=(bar.get_x() + bar.get_width() / 2, height),\n                    xytext=(0, 3),  # 3 points vertical offset\n                    textcoords=\"offset points\",\n                    ha=\"center\",\n                    va=\"bottom\",\n                )\n    return ax\n</code></pre>"},{"location":"reference/#src.meteors.visualize.lime_visualize.visualize_spectral_attributes","title":"<code>visualize_spectral_attributes(spectral_attributes, use_pyplot=False, color_palette=None, show_not_included=True)</code>","text":"<p>Visualizes the spectral attributes of an hsi object or a list of hsi objects.</p> <p>Parameters:</p> Name Type Description Default <code>spectral_attributes</code> <code>HSISpectralAttributes | list[HSISpectralAttributes]</code> <p>The spectral attributes of the image object to visualize.</p> required <code>use_pyplot</code> <code>bool</code> <p>If True, displays the visualization using pyplot. If False, returns the figure and axes objects. Defaults to False.</p> <code>False</code> <code>color_palette</code> <code>list[str] | None</code> <p>The color palette to use for visualizing different spectral bands. If None, a default color palette is used. Defaults to None.</p> <code>None</code> <code>show_not_included</code> <code>bool</code> <p>If True, includes the spectral bands that are not included in the visualization. If False, only includes the spectral bands that are included in the visualization. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple[Figure, Axes] | None</code> <p>tuple[matplotlib.figure.Figure, matplotlib.axes.Axes] | None: If use_pyplot is False, returns the figure and axes objects. If use_pyplot is True, returns None.</p> Source code in <code>src/meteors/visualize/lime_visualize.py</code> <pre><code>def visualize_spectral_attributes(\n    spectral_attributes: HSISpectralAttributes | list[HSISpectralAttributes],\n    use_pyplot: bool = False,\n    color_palette: list[str] | None = None,\n    show_not_included: bool = True,\n) -&gt; tuple[Figure, Axes] | None:\n    \"\"\"Visualizes the spectral attributes of an hsi object or a list of hsi objects.\n\n    Args:\n        spectral_attributes (HSISpectralAttributes | list[HSISpectralAttributes]):\n            The spectral attributes of the image object to visualize.\n        use_pyplot (bool, optional):\n            If True, displays the visualization using pyplot.\n            If False, returns the figure and axes objects. Defaults to False.\n        color_palette (list[str] | None, optional):\n            The color palette to use for visualizing different spectral bands.\n            If None, a default color palette is used.\n            Defaults to None.\n        show_not_included (bool, optional):\n            If True, includes the spectral bands that are not included in the visualization.\n            If False, only includes the spectral bands that are included in the visualization.\n            Defaults to True.\n\n    Returns:\n        tuple[matplotlib.figure.Figure, matplotlib.axes.Axes] | None:\n            If use_pyplot is False, returns the figure and axes objects.\n            If use_pyplot is True, returns None.\n    \"\"\"\n    agg = True if isinstance(spectral_attributes, list) else False\n    band_names = spectral_attributes[0].band_names if agg else spectral_attributes.band_names  # type: ignore\n\n    color_palette = color_palette or sns.color_palette(\"hsv\", len(band_names.keys()))\n\n    fig, ax = plt.subplots(1, 3 if agg else 2, figsize=(15, 5))\n    fig.suptitle(\"Spectral Attributes Visualization\")\n\n    visualize_spectral_attributes_by_waveband(\n        spectral_attributes,\n        ax[0],\n        color_palette=color_palette,\n        show_not_included=show_not_included,\n        show_legend=False,\n    )\n\n    visualize_spectral_attributes_by_magnitude(\n        spectral_attributes,\n        ax[1],\n        color_palette=color_palette,\n        show_not_included=show_not_included,\n    )\n\n    if agg:\n        scores = [attr.score for attr in spectral_attributes]  # type: ignore\n        mean_score = sum(scores) / len(scores)  # type: ignore\n        ax[2].hist(scores, bins=50, color=\"steelblue\", alpha=0.7)\n        ax[2].axvline(mean_score, color=\"darkred\", linestyle=\"dashed\")\n\n        ax[2].set_title(\"Distribution of Score Values\")\n        ax[2].set_xlabel(\"Score\")\n        ax[2].set_ylabel(\"Frequency\")\n\n    if use_pyplot:\n        plt.show()  # pragma: no cover\n        return None  # pragma: no cover\n\n    return fig, ax\n</code></pre>"},{"location":"reference/#src.meteors.visualize.lime_visualize.visualize_spatial_attributes","title":"<code>visualize_spatial_attributes(spatial_attributes, use_pyplot=False)</code>","text":"<p>Visualizes the spatial attributes of an hsi using Lime attribution.</p> <p>Parameters:</p> Name Type Description Default <code>spatial_attributes</code> <code>HSISpatialAttributes</code> <p>The spatial attributes of the image object to visualize.</p> required <code>use_pyplot</code> <code>bool</code> <p>Whether to use pyplot for visualization. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[Figure, Axes] | None</code> <p>tuple[matplotlib.figure.Figure, matplotlib.axes.Axes] | None: If use_pyplot is False, returns the figure and axes objects. If use_pyplot is True, returns None.</p> Source code in <code>src/meteors/visualize/lime_visualize.py</code> <pre><code>def visualize_spatial_attributes(\n    spatial_attributes: HSISpatialAttributes, use_pyplot: bool = False\n) -&gt; tuple[Figure, Axes] | None:\n    \"\"\"Visualizes the spatial attributes of an hsi using Lime attribution.\n\n    Args:\n        spatial_attributes (HSISpatialAttributes):\n            The spatial attributes of the image object to visualize.\n        use_pyplot (bool, optional):\n            Whether to use pyplot for visualization. Defaults to False.\n\n    Returns:\n        tuple[matplotlib.figure.Figure, matplotlib.axes.Axes] | None:\n            If use_pyplot is False, returns the figure and axes objects.\n            If use_pyplot is True, returns None.\n    \"\"\"\n    mask_enabled = spatial_attributes.segmentation_mask is not None\n    fig, ax = plt.subplots(1, 3 if mask_enabled else 2, figsize=(15, 5))\n    fig.suptitle(\"Spatial Attributes Visualization\")\n    spatial_attributes = spatial_attributes.change_orientation(\"HWC\", inplace=False)\n\n    if mask_enabled:\n        mask = spatial_attributes.segmentation_mask.cpu()\n\n        group_names = mask.unique().tolist()\n        colors = sns.color_palette(\"hsv\", len(group_names))\n        color_map = dict(zip(group_names, colors))\n\n        for unique in group_names:\n            segment_indices = torch.argwhere(mask == unique)\n\n            y_center, x_center = segment_indices.numpy().mean(axis=0).astype(int)\n            ax[1].text(x_center, y_center, str(unique), color=color_map[unique], fontsize=8, ha=\"center\", va=\"center\")\n            ax[2].text(x_center, y_center, str(unique), color=color_map[unique], fontsize=8, ha=\"center\", va=\"center\")\n\n        ax[2].imshow(mask.numpy() / mask.max(), cmap=\"gray\")\n        ax[2].set_title(\"Mask\")\n        ax[2].grid(False)\n        ax[2].axis(\"off\")\n\n    ax[0].imshow(spatial_attributes.hsi.get_rgb_image(output_channel_axis=2).cpu())\n    ax[0].set_title(\"Original image\")\n    ax[0].grid(False)\n    ax[0].axis(\"off\")\n\n    attrs = spatial_attributes.attributes.cpu().numpy()\n    if np.all(attrs == 0):\n        logger.warning(\"All spatial attributes are zero.\")\n        cmap = LinearSegmentedColormap.from_list(\"RdWhGn\", [\"red\", \"white\", \"green\"])\n        heat_map = ax[1].imshow(attrs.sum(axis=-1), cmap=cmap, vmin=-1, vmax=1)\n\n        axis_separator = make_axes_locatable(ax[1])\n        colorbar_axis = axis_separator.append_axes(\"bottom\", size=\"5%\", pad=0.1)\n        fig.colorbar(heat_map, orientation=\"horizontal\", cax=colorbar_axis)\n    else:\n        viz.visualize_image_attr(\n            attrs,\n            method=\"heat_map\",\n            sign=\"all\",\n            plt_fig_axis=(fig, ax[1]),\n            show_colorbar=True,\n            use_pyplot=False,\n        )\n    ax[1].set_title(\"Attribution Map\")\n    ax[1].axis(\"off\")\n\n    if use_pyplot:\n        plt.show()  # pragma: no cover\n        return None  # pragma: no cover\n    else:\n        return fig, ax\n</code></pre>"},{"location":"reference/#attribution-methods","title":"Attribution Methods","text":""},{"location":"reference/#src.meteors.attr.attributes.HSIAttributes","title":"<code>HSIAttributes</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents an object that contains Hyperspectral image attributes and explanations.</p> <p>Attributes:</p> Name Type Description <code>hsi</code> <code>HSI</code> <p>Hyperspectral image object for which the explanations were created.</p> <code>attributes</code> <code>Tensor</code> <p>Attributions (explanations) for the hsi.</p> <code>score</code> <code>float</code> <p>The score provided by the interpretable model. Can be None if method don't provide one.</p> <code>device</code> <code>device</code> <p>Device to be used for inference. If None, the device of the input hsi will be used. Defaults to None.</p> <code>attribution_method</code> <code>str | None</code> <p>The method used to generate the explanation. Defaults to None.</p> Source code in <code>src/meteors/attr/attributes.py</code> <pre><code>class HSIAttributes(BaseModel):\n    \"\"\"Represents an object that contains Hyperspectral image attributes and explanations.\n\n    Attributes:\n        hsi (HSI): Hyperspectral image object for which the explanations were created.\n        attributes (torch.Tensor): Attributions (explanations) for the hsi.\n        score (float): The score provided by the interpretable model. Can be None if method don't provide one.\n        device (torch.device): Device to be used for inference. If None, the device of the input hsi will be used.\n            Defaults to None.\n        attribution_method (str | None): The method used to generate the explanation. Defaults to None.\n    \"\"\"\n\n    hsi: Annotated[\n        HSI,\n        Field(\n            description=\"Hyperspectral image object for which the explanations were created.\",\n        ),\n    ]\n    attributes: Annotated[\n        torch.Tensor,\n        BeforeValidator(validate_and_convert_attributes),\n        Field(\n            description=\"Attributions (explanations) for the hsi.\",\n        ),\n    ]\n    attribution_method: Annotated[\n        str | None,\n        AfterValidator(validate_attribution_method),\n        Field(\n            description=\"The method used to generate the explanation.\",\n        ),\n    ] = None\n    score: Annotated[\n        float | None,\n        Field(\n            validate_default=True,\n            description=\"The score provided by the interpretable model. Can be None if method don't provide one.\",\n        ),\n    ] = None\n    mask: Annotated[\n        torch.Tensor | None,\n        BeforeValidator(validate_and_convert_mask),\n        Field(\n            description=\"`superpixel` or `superband` mask used for the explanation.\",\n        ),\n    ] = None\n    device: Annotated[\n        torch.device,\n        BeforeValidator(resolve_inference_device_attributes),\n        Field(\n            validate_default=True,\n            exclude=True,\n            description=(\n                \"Device to be used for inference. If None, the device of the input hsi will be used. \"\n                \"Defaults to None.\"\n            ),\n        ),\n    ] = None\n\n    @property\n    def flattened_attributes(self) -&gt; torch.Tensor:\n        \"\"\"Returns a flattened tensor of attributes.\n\n        This method should be implemented in the subclass.\n\n        Returns:\n            torch.Tensor: A flattened tensor of attributes.\n        \"\"\"\n        raise NotImplementedError(\"The `flattened_attributes` property must be implemented in the subclass\")\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @property\n    def orientation(self) -&gt; tuple[str, str, str]:\n        \"\"\"Returns the orientation of the hsi.\n\n        Returns:\n            tuple[str, str, str]: The orientation of the hsi corresponding to the attributes.\n        \"\"\"\n        return self.hsi.orientation\n\n    def _validate_hsi_attributions_and_mask(self) -&gt; None:\n        \"\"\"Validates the hsi attributions and performs necessary operations to ensure compatibility with the device.\n\n        Raises:\n            ValueError: If the shapes of the attributes and hsi tensors do not match.\n        \"\"\"\n        validate_shapes(self.attributes, self.hsi)\n\n        self.attributes = self.attributes.to(self.device)\n        if self.device != self.hsi.device:\n            self.hsi.to(self.device)\n\n        if self.mask is not None:\n            validate_shapes(self.mask, self.hsi)\n            self.mask = self.mask.to(self.device)\n\n    @model_validator(mode=\"after\")\n    def validate_hsi_attributions(self) -&gt; Self:\n        \"\"\"Validates the hsi attributions.\n\n        This method performs validation on the hsi attributions to ensure they are correct.\n\n        Returns:\n            Self: The current instance of the class.\n        \"\"\"\n        self._validate_hsi_attributions_and_mask()\n        return self\n\n    def to(self, device: str | torch.device) -&gt; Self:\n        \"\"\"Move the hsi and attributes tensors to the specified device.\n\n        Args:\n            device (str or torch.device): The device to move the tensors to.\n\n        Returns:\n            Self: The modified object with tensors moved to the specified device.\n\n        Examples:\n            &gt;&gt;&gt; attrs = HSIAttributes(hsi, attributes, score=0.5)\n            &gt;&gt;&gt; attrs.to(\"cpu\")\n            &gt;&gt;&gt; attrs.hsi.device\n            device(type='cpu')\n            &gt;&gt;&gt; attrs.attributes.device\n            device(type='cpu')\n            &gt;&gt;&gt; attrs.to(\"cuda\")\n            &gt;&gt;&gt; attrs.hsi.device\n            device(type='cuda')\n            &gt;&gt;&gt; attrs.attributes.device\n            device(type='cuda')\n        \"\"\"\n        self.hsi = self.hsi.to(device)\n        self.attributes = self.attributes.to(device)\n        self.device = self.hsi.device\n        return self\n\n    def change_orientation(self, target_orientation: tuple[str, str, str] | list[str] | str, inplace=False) -&gt; Self:\n        \"\"\"Changes the orientation of the image data along with the attributions to the target orientation.\n\n        Args:\n            target_orientation (tuple[str, str, str] | list[str] | str): The target orientation for the attribution data.\n                This should be a tuple of three one-letter strings in any order: \"C\", \"H\", \"W\".\n            inplace (bool, optional): Whether to modify the data in place or return a new object.\n\n        Returns:\n            Self: The updated Image object with the new orientation.\n\n        Raises:\n            OrientationError: If the target orientation is not a valid tuple of three one-letter strings.\n        \"\"\"\n        current_orientation = self.orientation\n        hsi = self.hsi.change_orientation(target_orientation, inplace=inplace)\n        if inplace:\n            attrs = self\n        else:\n            attrs = self.model_copy()\n            attrs.hsi = hsi\n\n        # now change the orientation of the attributes\n        if current_orientation == target_orientation:\n            return attrs\n\n        permute_dims = [current_orientation.index(dim) for dim in target_orientation]\n\n        attrs.attributes = attrs.attributes.permute(permute_dims)\n\n        if attrs.mask is not None:\n            attrs.mask = attrs.mask.permute(permute_dims)\n        return attrs\n</code></pre>"},{"location":"reference/#src.meteors.attr.attributes.HSIAttributes.flattened_attributes","title":"<code>flattened_attributes: torch.Tensor</code>  <code>property</code>","text":"<p>Returns a flattened tensor of attributes.</p> <p>This method should be implemented in the subclass.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A flattened tensor of attributes.</p>"},{"location":"reference/#src.meteors.attr.attributes.HSIAttributes.orientation","title":"<code>orientation: tuple[str, str, str]</code>  <code>property</code>","text":"<p>Returns the orientation of the hsi.</p> <p>Returns:</p> Type Description <code>tuple[str, str, str]</code> <p>tuple[str, str, str]: The orientation of the hsi corresponding to the attributes.</p>"},{"location":"reference/#src.meteors.attr.attributes.HSIAttributes.change_orientation","title":"<code>change_orientation(target_orientation, inplace=False)</code>","text":"<p>Changes the orientation of the image data along with the attributions to the target orientation.</p> <p>Parameters:</p> Name Type Description Default <code>target_orientation</code> <code>tuple[str, str, str] | list[str] | str</code> <p>The target orientation for the attribution data. This should be a tuple of three one-letter strings in any order: \"C\", \"H\", \"W\".</p> required <code>inplace</code> <code>bool</code> <p>Whether to modify the data in place or return a new object.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>The updated Image object with the new orientation.</p> <p>Raises:</p> Type Description <code>OrientationError</code> <p>If the target orientation is not a valid tuple of three one-letter strings.</p> Source code in <code>src/meteors/attr/attributes.py</code> <pre><code>def change_orientation(self, target_orientation: tuple[str, str, str] | list[str] | str, inplace=False) -&gt; Self:\n    \"\"\"Changes the orientation of the image data along with the attributions to the target orientation.\n\n    Args:\n        target_orientation (tuple[str, str, str] | list[str] | str): The target orientation for the attribution data.\n            This should be a tuple of three one-letter strings in any order: \"C\", \"H\", \"W\".\n        inplace (bool, optional): Whether to modify the data in place or return a new object.\n\n    Returns:\n        Self: The updated Image object with the new orientation.\n\n    Raises:\n        OrientationError: If the target orientation is not a valid tuple of three one-letter strings.\n    \"\"\"\n    current_orientation = self.orientation\n    hsi = self.hsi.change_orientation(target_orientation, inplace=inplace)\n    if inplace:\n        attrs = self\n    else:\n        attrs = self.model_copy()\n        attrs.hsi = hsi\n\n    # now change the orientation of the attributes\n    if current_orientation == target_orientation:\n        return attrs\n\n    permute_dims = [current_orientation.index(dim) for dim in target_orientation]\n\n    attrs.attributes = attrs.attributes.permute(permute_dims)\n\n    if attrs.mask is not None:\n        attrs.mask = attrs.mask.permute(permute_dims)\n    return attrs\n</code></pre>"},{"location":"reference/#src.meteors.attr.attributes.HSIAttributes.to","title":"<code>to(device)</code>","text":"<p>Move the hsi and attributes tensors to the specified device.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str or device</code> <p>The device to move the tensors to.</p> required <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>The modified object with tensors moved to the specified device.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; attrs = HSIAttributes(hsi, attributes, score=0.5)\n&gt;&gt;&gt; attrs.to(\"cpu\")\n&gt;&gt;&gt; attrs.hsi.device\ndevice(type='cpu')\n&gt;&gt;&gt; attrs.attributes.device\ndevice(type='cpu')\n&gt;&gt;&gt; attrs.to(\"cuda\")\n&gt;&gt;&gt; attrs.hsi.device\ndevice(type='cuda')\n&gt;&gt;&gt; attrs.attributes.device\ndevice(type='cuda')\n</code></pre> Source code in <code>src/meteors/attr/attributes.py</code> <pre><code>def to(self, device: str | torch.device) -&gt; Self:\n    \"\"\"Move the hsi and attributes tensors to the specified device.\n\n    Args:\n        device (str or torch.device): The device to move the tensors to.\n\n    Returns:\n        Self: The modified object with tensors moved to the specified device.\n\n    Examples:\n        &gt;&gt;&gt; attrs = HSIAttributes(hsi, attributes, score=0.5)\n        &gt;&gt;&gt; attrs.to(\"cpu\")\n        &gt;&gt;&gt; attrs.hsi.device\n        device(type='cpu')\n        &gt;&gt;&gt; attrs.attributes.device\n        device(type='cpu')\n        &gt;&gt;&gt; attrs.to(\"cuda\")\n        &gt;&gt;&gt; attrs.hsi.device\n        device(type='cuda')\n        &gt;&gt;&gt; attrs.attributes.device\n        device(type='cuda')\n    \"\"\"\n    self.hsi = self.hsi.to(device)\n    self.attributes = self.attributes.to(device)\n    self.device = self.hsi.device\n    return self\n</code></pre>"},{"location":"reference/#src.meteors.attr.attributes.HSISpatialAttributes","title":"<code>HSISpatialAttributes</code>","text":"<p>               Bases: <code>HSIAttributes</code></p> <p>Represents spatial attributes of an hsi used for explanation.</p> <p>Attributes:</p> Name Type Description <code>hsi</code> <code>HSI</code> <p>Hyperspectral image object for which the explanations were created.</p> <code>attributes</code> <code>Tensor</code> <p>Attributions (explanations) for the hsi.</p> <code>score</code> <code>float</code> <p>The score provided by the interpretable model. Can be None if method don't provide one.</p> <code>device</code> <code>device</code> <p>Device to be used for inference. If None, the device of the input hsi will be used. Defaults to None.</p> <code>attribution_method</code> <code>str | None</code> <p>The method used to generate the explanation. Defaults to None.</p> <code>segmentation_mask</code> <code>Tensor</code> <p>Spatial (Segmentation) mask used for the explanation.</p> <code>flattened_attributes</code> <code>Tensor</code> <p>Spatial 2D attribution map.</p> Source code in <code>src/meteors/attr/attributes.py</code> <pre><code>class HSISpatialAttributes(HSIAttributes):\n    \"\"\"Represents spatial attributes of an hsi used for explanation.\n\n    Attributes:\n        hsi (HSI): Hyperspectral image object for which the explanations were created.\n        attributes (torch.Tensor): Attributions (explanations) for the hsi.\n        score (float): The score provided by the interpretable model. Can be None if method don't provide one.\n        device (torch.device): Device to be used for inference. If None, the device of the input hsi will be used.\n            Defaults to None.\n        attribution_method (str | None): The method used to generate the explanation. Defaults to None.\n        segmentation_mask (torch.Tensor): Spatial (Segmentation) mask used for the explanation.\n        flattened_attributes (torch.Tensor): Spatial 2D attribution map.\n    \"\"\"\n\n    @property\n    def segmentation_mask(self) -&gt; torch.Tensor:\n        \"\"\"Returns the 2D spatial segmentation mask that has the same size as the hsi image.\n\n        Returns:\n            torch.Tensor: The segmentation mask tensor.\n\n        Raises:\n            HSIAttributesError: If the segmentation mask is not provided in the attributes object.\n        \"\"\"\n        if self.mask is None:\n            raise HSIAttributesError(\"Segmentation mask is not provided in the attributes object\")\n        return self.mask.select(dim=self.hsi.spectral_axis, index=0)\n\n    @property\n    def flattened_attributes(self) -&gt; torch.Tensor:\n        \"\"\"Returns a flattened tensor of attributes, with removed repeated dimensions.\n\n        In the case of spatial attributes, the flattened attributes are 2D spatial attributes of shape (rows, columns) and the spectral dimension is removed.\n\n        Examples:\n            &gt;&gt;&gt; segmentation_mask = torch.zeros((3, 2, 2))\n            &gt;&gt;&gt; attrs = HSISpatialAttributes(hsi, attributes, score=0.5, segmentation_mask=segmentation_mask)\n            &gt;&gt;&gt; attrs.flattened_attributes\n                tensor([[0., 0.],\n                        [0., 0.]])\n\n        Returns:\n            torch.Tensor: A flattened tensor of attributes.\n        \"\"\"\n        return self.attributes.select(dim=self.hsi.spectral_axis, index=0)\n\n    def _validate_hsi_attributions_and_mask(self) -&gt; None:\n        \"\"\"Validates the hsi attributions and performs necessary operations to ensure compatibility with the device.\n\n        Raises:\n            HSIAttributesError: If the segmentation mask is not provided in the attributes object.\n        \"\"\"\n        super()._validate_hsi_attributions_and_mask()\n        if self.mask is None:\n            raise HSIAttributesError(\"Segmentation mask is not provided in the attributes object\")\n</code></pre>"},{"location":"reference/#src.meteors.attr.attributes.HSISpatialAttributes.flattened_attributes","title":"<code>flattened_attributes: torch.Tensor</code>  <code>property</code>","text":"<p>Returns a flattened tensor of attributes, with removed repeated dimensions.</p> <p>In the case of spatial attributes, the flattened attributes are 2D spatial attributes of shape (rows, columns) and the spectral dimension is removed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; segmentation_mask = torch.zeros((3, 2, 2))\n&gt;&gt;&gt; attrs = HSISpatialAttributes(hsi, attributes, score=0.5, segmentation_mask=segmentation_mask)\n&gt;&gt;&gt; attrs.flattened_attributes\n    tensor([[0., 0.],\n            [0., 0.]])\n</code></pre> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A flattened tensor of attributes.</p>"},{"location":"reference/#src.meteors.attr.attributes.HSISpatialAttributes.segmentation_mask","title":"<code>segmentation_mask: torch.Tensor</code>  <code>property</code>","text":"<p>Returns the 2D spatial segmentation mask that has the same size as the hsi image.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The segmentation mask tensor.</p> <p>Raises:</p> Type Description <code>HSIAttributesError</code> <p>If the segmentation mask is not provided in the attributes object.</p>"},{"location":"reference/#src.meteors.attr.attributes.HSISpectralAttributes","title":"<code>HSISpectralAttributes</code>","text":"<p>               Bases: <code>HSIAttributes</code></p> <p>Represents an hsi with spectral attributes used for explanation.</p> <p>Attributes:</p> Name Type Description <code>hsi</code> <code>HSI</code> <p>Hyperspectral hsi object for which the explanations were created.</p> <code>attributes</code> <code>Tensor</code> <p>Attributions (explanations) for the hsi.</p> <code>score</code> <code>float</code> <p>R^2 score of interpretable model used for the explanation.</p> <code>device</code> <code>device</code> <p>Device to be used for inference. If None, the device of the input hsi will be used. Defaults to None.</p> <code>attribution_method</code> <code>str | None</code> <p>The method used to generate the explanation. Defaults to None.</p> <code>band_mask</code> <code>Tensor</code> <p>Band mask used for the explanation.</p> <code>band_names</code> <code>dict[str | tuple[str, ...], int]</code> <p>Dictionary that translates the band names into the band segment ids.</p> <code>flattened_attributes</code> <code>Tensor</code> <p>Spectral 1D attribution map.</p> Source code in <code>src/meteors/attr/attributes.py</code> <pre><code>class HSISpectralAttributes(HSIAttributes):\n    \"\"\"Represents an hsi with spectral attributes used for explanation.\n\n    Attributes:\n        hsi (HSI): Hyperspectral hsi object for which the explanations were created.\n        attributes (torch.Tensor): Attributions (explanations) for the hsi.\n        score (float): R^2 score of interpretable model used for the explanation.\n        device (torch.device): Device to be used for inference. If None, the device of the input hsi will be used.\n            Defaults to None.\n        attribution_method (str | None): The method used to generate the explanation. Defaults to None.\n        band_mask (torch.Tensor): Band mask used for the explanation.\n        band_names (dict[str | tuple[str, ...], int]): Dictionary that translates the band names into the band segment ids.\n        flattened_attributes (torch.Tensor): Spectral 1D attribution map.\n    \"\"\"\n\n    band_names: Annotated[\n        dict[str | tuple[str, ...], int],\n        Field(\n            description=\"Dictionary that translates the band names into the band segment ids.\",\n        ),\n    ]\n\n    @property\n    def band_mask(self) -&gt; torch.Tensor:\n        \"\"\"Returns a 1D band mask - a band mask with removed repeated dimensions (num_bands, ),\n        where num_bands is the number of bands in the hsi image.\n\n        The method selects the appropriate dimensions from the `band_mask` tensor\n        based on the `axis_to_select` and returns a flattened version of the selected\n        tensor.\n\n        Returns:\n            torch.Tensor: The flattened band mask tensor.\n\n        Examples:\n            &gt;&gt;&gt; band_names = {\"R\": 0, \"G\": 1, \"B\": 2}\n            &gt;&gt;&gt; attrs = HSISpectralAttributes(hsi, attributes, score=0.5, mask=band_mask)\n            &gt;&gt;&gt; attrs.flattened_band_mask\n            torch.tensor([0, 1, 2])\n        \"\"\"\n        if self.mask is None:\n            raise ValueError(\"Band mask is not provided\")\n        axis_to_select = [i for i in range(self.hsi.image.ndim) if i != self.hsi.spectral_axis]\n        return self.mask.select(dim=axis_to_select[0], index=0).select(dim=axis_to_select[1] - 1, index=0)\n\n    @property\n    def flattened_attributes(self) -&gt; torch.Tensor:\n        \"\"\"Returns a flattened tensor of attributes with removed repeated dimensions.\n\n        In the case of spectral attributes, the flattened attributes are 1D tensor of shape (num_bands, ), where num_bands is the number of bands in the hsi image.\n\n        Returns:\n            torch.Tensor: A flattened tensor of attributes.\n        \"\"\"\n        axis = [i for i in range(self.attributes.ndim) if i != self.hsi.spectral_axis]\n        return self.attributes.select(dim=axis[0], index=0).select(dim=axis[1] - 1, index=0)\n\n    def _validate_hsi_attributions_and_mask(self) -&gt; None:\n        \"\"\"Validates the hsi attributions and performs necessary operations to ensure compatibility with the device.\n\n        Raises:\n            HSIAttributesError: If the band mask is not provided in the attributes object\n        \"\"\"\n        super()._validate_hsi_attributions_and_mask()\n        if self.mask is None:\n            raise HSIAttributesError(\"Band mask is not provided in the attributes object\")\n\n        self.band_names = align_band_names_with_mask(self.band_names, self.mask)\n</code></pre>"},{"location":"reference/#src.meteors.attr.attributes.HSISpectralAttributes.band_mask","title":"<code>band_mask: torch.Tensor</code>  <code>property</code>","text":"<p>Returns a 1D band mask - a band mask with removed repeated dimensions (num_bands, ), where num_bands is the number of bands in the hsi image.</p> <p>The method selects the appropriate dimensions from the <code>band_mask</code> tensor based on the <code>axis_to_select</code> and returns a flattened version of the selected tensor.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The flattened band mask tensor.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; band_names = {\"R\": 0, \"G\": 1, \"B\": 2}\n&gt;&gt;&gt; attrs = HSISpectralAttributes(hsi, attributes, score=0.5, mask=band_mask)\n&gt;&gt;&gt; attrs.flattened_band_mask\ntorch.tensor([0, 1, 2])\n</code></pre>"},{"location":"reference/#src.meteors.attr.attributes.HSISpectralAttributes.flattened_attributes","title":"<code>flattened_attributes: torch.Tensor</code>  <code>property</code>","text":"<p>Returns a flattened tensor of attributes with removed repeated dimensions.</p> <p>In the case of spectral attributes, the flattened attributes are 1D tensor of shape (num_bands, ), where num_bands is the number of bands in the hsi image.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A flattened tensor of attributes.</p>"},{"location":"reference/#src.meteors.attr.lime.Lime","title":"<code>Lime</code>","text":"<p>               Bases: <code>Explainer</code></p> <p>Lime class is a subclass of Explainer and represents the Lime explainer. Lime is an interpretable model-agnostic explanation method that explains the predictions of a black-box model by approximating it with a simpler interpretable model. The Lime method is based on the <code>captum</code> implementation and is an implementation of an idea coming from the original paper on Lime, where more details about this method can be found.</p> <p>Parameters:</p> Name Type Description Default <code>explainable_model</code> <code>ExplainableModel</code> <p>The explainable model to be explained.</p> required <code>interpretable_model</code> <code>InterpretableModel</code> <p>The interpretable model used to approximate the black-box model. Defaults to <code>SkLearnLasso</code> with alpha parameter set to 0.08.</p> <code>SkLearnLasso(alpha=0.08)</code> <code>similarity_func</code> <code>Callable[[Tensor], Tensor] | None</code> <p>The similarity function used by Lime. Defaults to None.</p> <code>None</code> <code>perturb_func</code> <code>Callable[[Tensor], Tensor] | None</code> <p>The perturbation function used by Lime. Defaults to None.</p> <code>None</code> <code>postprocessing_segmentation_output</code> <code>Callable[[Tensor], Tensor] | None</code> <p>A segmentation postprocessing function for segmentation problem type. This is required for segmentation problem type as attribution methods needs to have 1d output. Defaults to None, which means that the no postprocessing function is used. attribution method is not used. Defaults to None.</p> <code>None</code> Source code in <code>src/meteors/attr/lime.py</code> <pre><code>class Lime(Explainer):\n    \"\"\"Lime class is a subclass of Explainer and represents the Lime explainer. Lime is an interpretable model-agnostic\n    explanation method that explains the predictions of a black-box model by approximating it with a simpler\n    interpretable model. The Lime method is based on the [`captum` implementation](https://captum.ai/api/lime.html)\n    and is an implementation of an idea coming from the [original paper on Lime](https://arxiv.org/abs/1602.04938),\n    where more details about this method can be found.\n\n    Args:\n        explainable_model (ExplainableModel): The explainable model to be explained.\n        interpretable_model (InterpretableModel): The interpretable model used to approximate the black-box model.\n            Defaults to `SkLearnLasso` with alpha parameter set to 0.08.\n        similarity_func (Callable[[torch.Tensor], torch.Tensor] | None, optional): The similarity function used by Lime.\n            Defaults to None.\n        perturb_func (Callable[[torch.Tensor], torch.Tensor] | None, optional): The perturbation function used by Lime.\n            Defaults to None.\n        postprocessing_segmentation_output (Callable[[torch.Tensor], torch.Tensor] | None):\n            A segmentation postprocessing function for segmentation problem type. This is required for segmentation\n            problem type as attribution methods needs to have 1d output. Defaults to None, which means that the\n            no postprocessing function is used. attribution method is not used. Defaults to None.\n    \"\"\"\n\n    def __init__(\n        self,\n        explainable_model: ExplainableModel,\n        interpretable_model: InterpretableModel = SkLearnLasso(alpha=0.08),\n        similarity_func: Callable[[torch.Tensor], torch.Tensor] | None = None,\n        perturb_func: Callable[[torch.Tensor], torch.Tensor] | None = None,\n        postprocessing_segmentation_output: Callable[[torch.Tensor], torch.Tensor] | None = None,\n    ):\n        super().__init__(explainable_model, postprocessing_segmentation_output=postprocessing_segmentation_output)\n        self.interpretable_model = interpretable_model\n        self._attribution_method: LimeBase = self._construct_lime(\n            self.explainable_model.forward_func, interpretable_model, similarity_func, perturb_func\n        )\n\n    @staticmethod\n    def _construct_lime(\n        forward_func: Callable[[torch.Tensor], torch.Tensor],\n        interpretable_model: InterpretableModel,\n        similarity_func: Callable | None,\n        perturb_func: Callable[[torch.Tensor], torch.Tensor] | None,\n    ) -&gt; LimeBase:\n        \"\"\"Constructs the LimeBase object.\n\n        Args:\n            forward_func (Callable[[torch.Tensor], torch.Tensor]): The forward function of the explainable model.\n            interpretable_model (InterpretableModel): The interpretable model used to approximate the black-box model.\n            similarity_func (Callable | None): The similarity function used by Lime.\n            perturb_func (Callable[[torch.Tensor], torch.Tensor] | None): The perturbation function used by Lime.\n\n        Returns:\n            LimeBase: The constructed LimeBase object.\n        \"\"\"\n        return LimeBase(\n            forward_func=forward_func,\n            interpretable_model=interpretable_model,\n            similarity_func=similarity_func,\n            perturb_func=perturb_func,\n        )\n\n    @staticmethod\n    def get_segmentation_mask(\n        hsi: HSI,\n        segmentation_method: Literal[\"patch\", \"slic\"] = \"slic\",\n        **segmentation_method_params: Any,\n    ) -&gt; torch.Tensor:\n        \"\"\"Generates a segmentation mask for the given hsi using the specified segmentation method.\n\n        Args:\n            hsi (HSI): The input hyperspectral image for which the segmentation mask needs to be generated.\n            segmentation_method (Literal[\"patch\", \"slic\"], optional): The segmentation method to be used.\n                Defaults to \"slic\".\n            **segmentation_method_params (Any): Additional parameters specific to the chosen segmentation method.\n\n        Returns:\n            torch.Tensor: The segmentation mask as a tensor.\n\n        Raises:\n            TypeError: If the input hsi is not an instance of the HSI class.\n            ValueError: If an unsupported segmentation method is specified.\n\n        Examples:\n            &gt;&gt;&gt; hsi = meteors.HSI(image=torch.ones((3, 240, 240)), wavelengths=[462.08, 465.27, 468.47])\n            &gt;&gt;&gt; segmentation_mask = mt_lime.Lime.get_segmentation_mask(hsi, segmentation_method=\"slic\")\n            &gt;&gt;&gt; segmentation_mask.shape\n            torch.Size([1, 240, 240])\n            &gt;&gt;&gt; segmentation_mask = meteors.attr.Lime.get_segmentation_mask(hsi, segmentation_method=\"patch\", patch_size=2)\n            &gt;&gt;&gt; segmentation_mask.shape\n            torch.Size([1, 240, 240])\n            &gt;&gt;&gt; segmentation_mask[0, :2, :2]\n            torch.tensor([[1, 1],\n                          [1, 1]])\n            &gt;&gt;&gt; segmentation_mask[0, 2:4, :2]\n            torch.tensor([[2, 2],\n                          [2, 2]])\n        \"\"\"\n        if not isinstance(hsi, HSI):\n            raise TypeError(\"hsi should be an instance of HSI class\")\n\n        try:\n            if segmentation_method == \"slic\":\n                return Lime._get_slic_segmentation_mask(hsi, **segmentation_method_params)\n            elif segmentation_method == \"patch\":\n                return Lime._get_patch_segmentation_mask(hsi, **segmentation_method_params)\n            else:\n                raise ValueError(f\"Unsupported segmentation method: {segmentation_method}\")\n        except Exception as e:\n            raise MaskCreationError(f\"Error creating segmentation mask using method {segmentation_method}: {e}\")\n\n    @staticmethod\n    def get_band_mask(\n        hsi: HSI,\n        band_names: None | list[str | list[str]] | dict[tuple[str, ...] | str, int] = None,\n        band_indices: None | dict[str | tuple[str, ...], ListOfWavelengthsIndices] = None,\n        band_wavelengths: None | dict[str | tuple[str, ...], ListOfWavelengths] = None,\n        device: str | torch.device | None = None,\n        repeat_dimensions: bool = False,\n    ) -&gt; tuple[torch.Tensor, dict[tuple[str, ...] | str, int]]:\n        \"\"\"Generates a band mask based on the provided hsi and band information.\n\n        Remember you need to provide either band_names, band_indices, or band_wavelengths to create the band mask.\n        If you provide more than one, the band mask will be created using only one using the following priority:\n        band_names &gt; band_wavelengths &gt; band_indices.\n\n        Args:\n            hsi (HSI): The input hyperspectral image.\n            band_names (None | list[str | list[str]] | dict[tuple[str, ...] | str, int], optional):\n                The names of the spectral bands to include in the mask. Defaults to None.\n            band_indices (None | dict[str | tuple[str, ...], list[tuple[int, int]] | tuple[int, int] | list[int]], optional):\n                The indices or ranges of indices of the spectral bands to include in the mask. Defaults to None.\n            band_wavelengths (None | dict[str | tuple[str, ...], list[tuple[float, float]] | tuple[float, float], list[float], float], optional):\n                The wavelengths or ranges of wavelengths of the spectral bands to include in the mask. Defaults to None.\n            device (str | torch.device | None, optional):\n                The device to use for computation. Defaults to None.\n            repeat_dimensions (bool, optional):\n                Whether to repeat the dimensions of the mask to match the input hsi shape. Defaults to False.\n\n        Returns:\n            tuple[torch.Tensor, dict[tuple[str, ...] | str, int]]: A tuple containing the band mask tensor and a dictionary\n            mapping band names to segment IDs.\n\n        Raises:\n            TypeError: If the input hsi is not an instance of the HSI class.\n            ValueError: If no band names, indices, or wavelengths are provided.\n\n        Examples:\n            &gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((len(wavelengths), 10, 10)), wavelengths=wavelengths)\n            &gt;&gt;&gt; band_names = [\"R\", \"G\"]\n            &gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_names=band_names)\n            &gt;&gt;&gt; dict_labels_to_segment_ids\n            {\"R\": 1, \"G\": 2}\n            &gt;&gt;&gt; band_indices = {\"RGB\": [0, 1, 2]}\n            &gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_indices=band_indices)\n            &gt;&gt;&gt; dict_labels_to_segment_ids\n            {\"RGB\": 1}\n            &gt;&gt;&gt; band_wavelengths = {\"RGB\": [(462.08, 465.27), (465.27, 468.47), (468.47, 471.68)]}\n            &gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_wavelengths=band_wavelengths)\n            &gt;&gt;&gt; dict_labels_to_segment_ids\n            {\"RGB\": 1}\n        \"\"\"\n        if not isinstance(hsi, HSI):\n            raise TypeError(\"hsi should be an instance of HSI class\")\n\n        try:\n            if not (band_names is not None or band_indices is not None or band_wavelengths is not None):\n                raise ValueError(\"No band names, indices, or wavelengths are provided.\")\n\n            # validate types\n            dict_labels_to_segment_ids = None\n            if band_names is not None:\n                logger.debug(\"Getting band mask from band names of spectral bands\")\n                if band_wavelengths is not None or band_indices is not None:\n                    ignored_params = [\n                        param\n                        for param in [\"band_wavelengths\", \"band_indices\"]\n                        if param in locals() and locals()[param] is not None\n                    ]\n                    ignored_params_str = \" and \".join(ignored_params)\n                    logger.info(\n                        f\"Only the band names will be used to create the band mask. The additional parameters {ignored_params_str} will be ignored.\"\n                    )\n                try:\n                    validate_band_names(band_names)\n                    band_groups, dict_labels_to_segment_ids = Lime._get_band_wavelengths_indices_from_band_names(\n                        hsi.wavelengths, band_names\n                    )\n                except Exception as e:\n                    raise BandSelectionError(f\"Incorrect band names provided: {e}\") from e\n            elif band_wavelengths is not None:\n                logger.debug(\"Getting band mask from band groups given by ranges of wavelengths\")\n                if band_indices is not None:\n                    logger.info(\n                        \"Only the band wavelengths will be used to create the band mask. The band_indices will be ignored.\"\n                    )\n                validate_band_format(band_wavelengths, variable_name=\"band_wavelengths\")\n                try:\n                    band_groups = Lime._get_band_indices_from_band_wavelengths(\n                        hsi.wavelengths,\n                        band_wavelengths,\n                    )\n                except Exception as e:\n                    raise ValueError(\n                        f\"Incorrect band ranges wavelengths provided, please check if provided wavelengths are correct: {e}\"\n                    ) from e\n            elif band_indices is not None:\n                logger.debug(\"Getting band mask from band groups given by ranges of indices\")\n                validate_band_format(band_indices, variable_name=\"band_indices\")\n                try:\n                    band_groups = Lime._get_band_indices_from_input_band_indices(hsi.wavelengths, band_indices)\n                except Exception as e:\n                    raise ValueError(\n                        f\"Incorrect band ranges indices provided, please check if provided indices are correct: {e}\"\n                    ) from e\n\n            return Lime._create_tensor_band_mask(\n                hsi,\n                band_groups,\n                dict_labels_to_segment_ids=dict_labels_to_segment_ids,\n                device=device,\n                repeat_dimensions=repeat_dimensions,\n                return_dict_labels_to_segment_ids=True,\n            )\n        except Exception as e:\n            raise MaskCreationError(f\"Error creating band mask: {e}\") from e\n\n    @staticmethod\n    def _make_band_names_indexable(segment_name: list[str] | tuple[str, ...] | str) -&gt; tuple[str, ...] | str:\n        \"\"\"Converts a list of strings into a tuple of strings if necessary to make it indexable.\n\n        Args:\n            segment_name (list[str] | tuple[str, ...] | str): The segment name to be converted.\n\n        Returns:\n            tuple[str, ...] | str: The converted segment name.\n\n        Raises:\n            TypeError: If the segment_name is not of type list or string.\n        \"\"\"\n        if (\n            isinstance(segment_name, tuple) and all(isinstance(subitem, str) for subitem in segment_name)\n        ) or isinstance(segment_name, str):\n            return segment_name\n        elif isinstance(segment_name, list) and all(isinstance(subitem, str) for subitem in segment_name):\n            return tuple(segment_name)\n        raise TypeError(f\"Incorrect segment {segment_name} type. Should be either a list or string\")\n\n    @staticmethod\n    # @lru_cache(maxsize=32) Can't use with lists as they are not hashable\n    def _extract_bands_from_spyndex(segment_name: list[str] | tuple[str, ...] | str) -&gt; tuple[str, ...] | str:\n        \"\"\"Extracts bands from the given segment name.\n\n        Args:\n            segment_name (list[str] | tuple[str, ...] | str): The name of the segment.\n                Users may pass either band names or indices names, as in the spyndex library.\n\n        Returns:\n            tuple[str, ...] | str: A tuple of band names if multiple bands are extracted,\n                or a single band name if only one band is extracted.\n\n        Raises:\n            BandSelectionError: If the provided band name is invalid.\n                The band name must be either in `spyndex.indices` or `spyndex.bands`.\n        \"\"\"\n        if isinstance(segment_name, str):\n            segment_name = (segment_name,)\n        elif isinstance(segment_name, list):\n            segment_name = tuple(segment_name)\n\n        band_names_segment: list[str] = []\n        for band_name in segment_name:\n            if band_name in spyndex.indices:\n                band_names_segment += list(spyndex.indices[band_name].bands)\n            elif band_name in spyndex.bands:\n                band_names_segment.append(band_name)\n            else:\n                raise BandSelectionError(\n                    f\"Invalid band name {band_name}, band name must be either in `spyndex.indices` or `spyndex.bands`\"\n                )\n\n        return tuple(set(band_names_segment)) if len(band_names_segment) &gt; 1 else band_names_segment[0]\n\n    @staticmethod\n    def _get_indices_from_wavelength_indices_range(\n        wavelengths: torch.Tensor, ranges: list[tuple[int, int]] | tuple[int, int]\n    ) -&gt; list[int]:\n        \"\"\"Converts wavelength indices ranges to list indices.\n\n        Args:\n            wavelengths (torch.Tensor): The tensor containing the wavelengths.\n            ranges (list[tuple[int, int]] | tuple[int, int]): The wavelength indices ranges.\n\n        Returns:\n            list[int]: The indices of bands corresponding to the wavelength indices ranges.\n        \"\"\"\n        validated_ranges_list = validate_segment_format(ranges)\n        validated_ranges_list = adjust_and_validate_segment_ranges(wavelengths, validated_ranges_list)\n\n        return list(\n            set(\n                chain.from_iterable(\n                    [list(range(int(validated_range[0]), int(validated_range[1]))) for validated_range in ranges]  # type: ignore\n                )\n            )\n        )\n\n    @staticmethod\n    def _get_band_wavelengths_indices_from_band_names(\n        wavelengths: torch.Tensor,\n        band_names: list[str | list[str]] | dict[tuple[str, ...] | str, int],\n    ) -&gt; tuple[dict[tuple[str, ...] | str, list[int]], dict[tuple[str, ...] | str, int]]:\n        \"\"\"Extracts band wavelengths indices from the given band names.\n\n        This function takes a list or dictionary of band names or segments and extracts the list of wavelengths indices\n        associated with each segment. It returns a tuple containing a dictionary with mapping segment labels into\n        wavelength indices and a dictionary mapping segment labels into segment ids.\n\n        Args:\n            wavelengths (torch.Tensor): The tensor containing the wavelengths.\n            band_names (list[str | list[str]] | dict[tuple[str, ...] | str, int]):\n                A list or dictionary with band names or segments.\n\n        Returns:\n            tuple[dict[tuple[str, ...] | str, list[int]], dict[tuple[str, ...] | str, int]]:\n                A tuple containing the dictionary with mapping segment labels into wavelength indices and the mapping\n                from segment labels into segment ids.\n\n        Raises:\n            TypeError: If the band names are not in the correct format.\n        \"\"\"\n        if isinstance(band_names, str):\n            band_names = [band_names]\n        if isinstance(band_names, list):\n            logger.debug(\"band_names is a list of segments, creating a dictionary of segments\")\n            band_names_hashed = [Lime._make_band_names_indexable(segment) for segment in band_names]\n            dict_labels_to_segment_ids = {segment: idx + 1 for idx, segment in enumerate(band_names_hashed)}\n            segments_list = band_names_hashed\n        elif isinstance(band_names, dict):\n            dict_labels_to_segment_ids = band_names.copy()\n            segments_list = tuple(band_names.keys())  # type: ignore\n        else:\n            raise TypeError(\"Incorrect band_names type. It should be a dict or a list\")\n        segments_list_after_mapping = [Lime._extract_bands_from_spyndex(segment) for segment in segments_list]\n        band_indices: dict[tuple[str, ...] | str, list[int]] = {}\n        for original_segment, segment in zip(segments_list, segments_list_after_mapping):\n            segment_indices_ranges: list[tuple[int, int]] = []\n            if isinstance(segment, str):\n                segment = (segment,)\n            for band_name in segment:\n                min_wavelength = spyndex.bands[band_name].min_wavelength\n                max_wavelength = spyndex.bands[band_name].max_wavelength\n\n                if min_wavelength &gt; wavelengths.max() or max_wavelength &lt; wavelengths.min():\n                    logger.warning(\n                        f\"Band {band_name} is not present in the given wavelengths. \"\n                        f\"Band ranges from {min_wavelength} nm to {max_wavelength} nm and the HSI wavelengths \"\n                        f\"range from {wavelengths.min():.2f} nm to {wavelengths.max():.2f} nm. The given band will be skipped\"\n                    )\n                else:\n                    segment_indices_ranges += Lime._convert_wavelengths_to_indices(\n                        wavelengths,\n                        (spyndex.bands[band_name].min_wavelength, spyndex.bands[band_name].max_wavelength),\n                    )\n\n            segment_list = Lime._get_indices_from_wavelength_indices_range(wavelengths, segment_indices_ranges)\n            band_indices[original_segment] = segment_list\n        return band_indices, dict_labels_to_segment_ids\n\n    @staticmethod\n    def _convert_wavelengths_to_indices(\n        wavelengths: torch.Tensor, ranges: list[tuple[float, float]] | tuple[float, float]\n    ) -&gt; list[tuple[int, int]]:\n        \"\"\"Converts wavelength ranges to index ranges.\n\n        Args:\n            wavelengths (torch.Tensor): The tensor containing the wavelengths.\n            ranges (list[tuple[float, float]] | tuple[float, float]): The wavelength ranges.\n\n        Returns:\n            list[tuple[int, int]]: The index ranges corresponding to the wavelength ranges.\n        \"\"\"\n        indices = []\n        if isinstance(ranges, tuple):\n            ranges = [ranges]\n\n        for start, end in ranges:\n            start_idx = torch.searchsorted(wavelengths, start, side=\"left\")\n            end_idx = torch.searchsorted(wavelengths, end, side=\"right\")\n            indices.append((start_idx.item(), end_idx.item()))\n        return indices\n\n    @staticmethod\n    def _get_band_indices_from_band_wavelengths(\n        wavelengths: torch.Tensor,\n        band_wavelengths: dict[str | tuple[str, ...], ListOfWavelengths],\n    ) -&gt; dict[str | tuple[str, ...], list[int]]:\n        \"\"\"Converts the ranges or list of wavelengths into indices.\n\n        Args:\n            wavelengths (torch.Tensor): The tensor containing the wavelengths.\n            band_wavelengths (dict): A dictionary mapping segment labels to wavelength list or ranges.\n\n        Returns:\n            dict: A dictionary mapping segment labels to index ranges.\n\n        Raises:\n            TypeError: If band_wavelengths is not a dictionary.\n        \"\"\"\n        if not isinstance(band_wavelengths, dict):\n            raise TypeError(\"band_wavelengths should be a dictionary\")\n\n        band_indices: dict[str | tuple[str, ...], list[int]] = {}\n        for segment_label, segment in band_wavelengths.items():\n            try:\n                dtype = torch_dtype_to_python_dtype(wavelengths.dtype)\n                if isinstance(segment, (float, int)):\n                    segment = [dtype(segment)]  # type: ignore\n                if isinstance(segment, list) and all(isinstance(x, (float, int)) for x in segment):\n                    segment_dtype = change_dtype_of_list(segment, dtype)\n                    indices = Lime._convert_wavelengths_list_to_indices(wavelengths, segment_dtype)  # type: ignore\n                else:\n                    if isinstance(segment, list):\n                        segment_dtype = [\n                            tuple(change_dtype_of_list(list(ranges), dtype))  # type: ignore\n                            for ranges in segment\n                        ]\n                    else:\n                        segment_dtype = tuple(change_dtype_of_list(segment, dtype))\n\n                    valid_segment_range = validate_segment_format(segment_dtype, dtype)\n                    range_indices = Lime._convert_wavelengths_to_indices(wavelengths, valid_segment_range)  # type: ignore\n                    valid_indices_format = validate_segment_format(range_indices)\n                    valid_range_indices = adjust_and_validate_segment_ranges(wavelengths, valid_indices_format)\n                    indices = Lime._get_indices_from_wavelength_indices_range(wavelengths, valid_range_indices)\n            except Exception as e:\n                raise ValueError(f\"Problem with segment {segment_label}: {e}\") from e\n\n            band_indices[segment_label] = indices\n\n        return band_indices\n\n    @staticmethod\n    def _convert_wavelengths_list_to_indices(wavelengths: torch.Tensor, ranges: list[float]) -&gt; list[int]:\n        \"\"\"Converts a list of wavelengths into indices.\n\n        Args:\n            wavelengths (torch.Tensor): The tensor containing the wavelengths.\n            ranges (list[float]): The list of wavelengths.\n\n        Returns:\n            list[int]: The indices corresponding to the wavelengths.\n        \"\"\"\n        indices = []\n        for wavelength in ranges:\n            index = (wavelengths == wavelength).nonzero(as_tuple=False)\n            number_of_elements = torch.numel(index)\n            if number_of_elements == 1:\n                indices.append(index.item())\n            elif number_of_elements == 0:\n                raise ValueError(f\"Couldn't find wavelength of value {wavelength} in list of wavelength\")\n            else:\n                raise ValueError(f\"Wavelength of value {wavelength} was present more than once in list of wavelength\")\n        return indices\n\n    @staticmethod\n    def _get_band_indices_from_input_band_indices(\n        wavelengths: torch.Tensor,\n        input_band_indices: dict[str | tuple[str, ...], ListOfWavelengthsIndices],\n    ) -&gt; dict[str | tuple[str, ...], list[int]]:\n        \"\"\"Get band indices from band list or ranges indices.\n\n        Args:\n            wavelengths (torch.Tensor): The tensor containing the wavelengths.\n            band_indices (dict[str | tuple[str, ...], ListOfWavelengthsIndices]):\n                A dictionary mapping segment labels to a list of wavelength indices.\n\n        Returns:\n            dict[str | tuple[str, ...], list[int]]: A dictionary mapping segment labels to a list of band indices.\n\n        Raises:\n            TypeError: If `band_indices` is not a dictionary.\n        \"\"\"\n        if not isinstance(input_band_indices, dict):\n            raise TypeError(\"band_indices should be a dictionary\")\n\n        band_indices: dict[str | tuple[str, ...], list[int]] = {}\n        for segment_label, indices in input_band_indices.items():\n            try:\n                if isinstance(indices, int):\n                    indices = [indices]  # type: ignore\n                if isinstance(indices, list) and all(isinstance(x, int) for x in indices):\n                    indices: list[int] = indices  # type: ignore\n                else:\n                    valid_indices_format = validate_segment_format(indices)  # type: ignore\n                    valid_range_indices = adjust_and_validate_segment_ranges(wavelengths, valid_indices_format)\n                    indices = Lime._get_indices_from_wavelength_indices_range(wavelengths, valid_range_indices)  # type: ignore\n\n                band_indices[segment_label] = indices  # type: ignore\n            except Exception as e:\n                raise ValueError(f\"Problem with segment {segment_label}\") from e\n\n        return band_indices\n\n    @staticmethod\n    def _check_overlapping_segments(hsi: HSI, dict_labels_to_indices: dict[str | tuple[str, ...], list[int]]) -&gt; None:\n        \"\"\"Check for overlapping segments in the given hsi.\n\n        Args:\n            hsi (HSI): The hsi object containing the wavelengths.\n            dict_labels_to_indices (dict[str | tuple[str, ...], list[int]]):\n                A dictionary mapping segment labels to indices.\n\n        Returns:\n            None\n        \"\"\"\n        overlapping_segments: dict[int, str | tuple[str, ...]] = {}\n        for segment_label, indices in dict_labels_to_indices.items():\n            for idx in indices:\n                if hsi.wavelengths[idx].item() in overlapping_segments.keys():\n                    logger.warning(\n                        (\n                            f\"Segments {overlapping_segments[hsi.wavelengths[idx].item()]} \"\n                            f\"and {segment_label} are overlapping on wavelength {hsi.wavelengths[idx].item()}\"\n                        )\n                    )\n                overlapping_segments[hsi.wavelengths[idx].item()] = segment_label\n\n    @staticmethod\n    def _validate_and_create_dict_labels_to_segment_ids(\n        dict_labels_to_segment_ids: dict[str | tuple[str, ...], int] | None,\n        segment_labels: list[str | tuple[str, ...]],\n    ) -&gt; dict[str | tuple[str, ...], int]:\n        \"\"\"Validates and creates a dictionary mapping segment labels to segment IDs.\n\n        Args:\n            dict_labels_to_segment_ids (dict[str | tuple[str, ...], int] | None):\n                The existing mapping from segment labels to segment IDs, or None if it doesn't exist.\n            segment_labels (list[str | tuple[str, ...]]): The list of segment labels.\n\n        Returns:\n            dict[str | tuple[str, ...], int]: A tuple containing the validated dictionary mapping segment\n            labels to segment IDs and a boolean flag indicating whether the segment labels are hashed.\n\n        Raises:\n            ValueError: If the length of `dict_labels_to_segment_ids` doesn't match the length of `segment_labels`.\n            ValueError: If a segment label is not present in `dict_labels_to_segment_ids`.\n            ValueError: If there are non-unique segment IDs in `dict_labels_to_segment_ids`.\n        \"\"\"\n        if dict_labels_to_segment_ids is None:\n            logger.debug(\"Creating mapping from segment labels into ids\")\n            return {segment: idx + 1 for idx, segment in enumerate(segment_labels)}\n\n        logger.debug(\"Using existing mapping from segment labels into segment ids\")\n\n        if len(dict_labels_to_segment_ids) != len(segment_labels):\n            raise ValueError(\n                (\n                    f\"Incorrect dict_labels_to_segment_ids - length mismatch. Expected: \"\n                    f\"{len(segment_labels)}, Actual: {len(dict_labels_to_segment_ids)}\"\n                )\n            )\n\n        unique_segment_ids = set(dict_labels_to_segment_ids.values())\n        if len(unique_segment_ids) != len(segment_labels):\n            raise ValueError(\"Non unique segment ids in the dict_labels_to_segment_ids\")\n\n        logger.debug(\"Passed mapping is correct\")\n        return dict_labels_to_segment_ids\n\n    @staticmethod\n    def _create_single_dim_band_mask(\n        hsi: HSI,\n        dict_labels_to_indices: dict[str | tuple[str, ...], list[int]],\n        dict_labels_to_segment_ids: dict[str | tuple[str, ...], int],\n        device: torch.device,\n    ) -&gt; torch.Tensor:\n        \"\"\"Create a one-dimensional band mask based on the given image, labels, and segment IDs.\n\n        Args:\n            hsi (HSI): The input hsi.\n            dict_labels_to_indices (dict[str | tuple[str, ...], list[int]]):\n                A dictionary mapping labels or label tuples to lists of indices.\n            dict_labels_to_segment_ids (dict[str | tuple[str, ...], int]):\n                A dictionary mapping labels or label tuples to segment IDs.\n            device (torch.device): The device to use for the tensor.\n\n        Returns:\n            torch.Tensor: The one-dimensional band mask tensor.\n\n        Raises:\n            ValueError: If the indices for a segment are out of bounds for the one-dimensional band mask.\n        \"\"\"\n        band_mask_single_dim = torch.zeros(len(hsi.wavelengths), dtype=torch.int64, device=device)\n\n        segment_labels = list(dict_labels_to_segment_ids.keys())\n\n        for segment_label in segment_labels[::-1]:\n            segment_indices = dict_labels_to_indices[segment_label]\n            segment_id = dict_labels_to_segment_ids[segment_label]\n            are_indices_valid = all(0 &lt;= idx &lt; band_mask_single_dim.shape[0] for idx in segment_indices)\n            if not are_indices_valid:\n                raise ValueError(\n                    (\n                        f\"Indices for segment {segment_label} are out of bounds for the one-dimensional band mask\"\n                        f\"of shape {band_mask_single_dim.shape}\"\n                    )\n                )\n            band_mask_single_dim[segment_indices] = segment_id\n\n        return band_mask_single_dim\n\n    @staticmethod\n    def _create_tensor_band_mask(\n        hsi: HSI,\n        dict_labels_to_indices: dict[str | tuple[str, ...], list[int]],\n        dict_labels_to_segment_ids: dict[str | tuple[str, ...], int] | None = None,\n        device: str | torch.device | None = None,\n        repeat_dimensions: bool = False,\n        return_dict_labels_to_segment_ids: bool = True,\n    ) -&gt; torch.Tensor | tuple[torch.Tensor, dict[tuple[str, ...] | str, int]]:\n        \"\"\"Create a tensor band mask from dictionaries. The band mask is created based on the given hsi, labels, and\n        segment IDs. The band mask is a tensor with the same shape as the input hsi and contains segment IDs, where each\n        segment is represented by a unique ID. The band mask will be used to attribute the hsi using the LIME method.\n\n        Args:\n            hsi (HSI): The input hsi.\n            dict_labels_to_indices (dict[str | tuple[str, ...], list[int]]): A dictionary mapping labels to indices.\n            dict_labels_to_segment_ids (dict[str | tuple[str, ...], int] | None, optional):\n                A dictionary mapping labels to segment IDs. Defaults to None.\n            device (str | torch.device | None, optional): The device to use. Defaults to None.\n            repeat_dimensions (bool, optional): Whether to repeat dimensions. Defaults to False.\n            return_dict_labels_to_segment_ids (bool, optional):\n                Whether to return the dictionary mapping labels to segment IDs. Defaults to True.\n\n        Returns:\n            torch.Tensor | tuple[torch.Tensor, dict[tuple[str, ...] | str, int]]:\n                The tensor band mask or a tuple containing the tensor band mask\n                and the dictionary mapping labels to segment IDs.\n        \"\"\"\n        if device is None:\n            device = hsi.device\n        segment_labels = list(dict_labels_to_indices.keys())\n\n        logger.debug(f\"Creating a band mask on the device {device} using {len(segment_labels)} segments\")\n\n        # Check for overlapping segments\n        Lime._check_overlapping_segments(hsi, dict_labels_to_indices)\n\n        # Create or validate dict_labels_to_segment_ids\n        dict_labels_to_segment_ids = Lime._validate_and_create_dict_labels_to_segment_ids(\n            dict_labels_to_segment_ids, segment_labels\n        )\n\n        # Create single-dimensional band mask\n        band_mask_single_dim = Lime._create_single_dim_band_mask(\n            hsi, dict_labels_to_indices, dict_labels_to_segment_ids, device\n        )\n\n        # Expand band mask to match image dimensions\n        band_mask = expand_spectral_mask(hsi, band_mask_single_dim, repeat_dimensions)\n\n        if return_dict_labels_to_segment_ids:\n            return band_mask, dict_labels_to_segment_ids\n        return band_mask\n\n    def attribute(  # type: ignore\n        self,\n        hsi: list[HSI] | HSI,\n        target: list[int] | int | None = None,\n        attribution_type: Literal[\"spatial\", \"spectral\"] | None = None,\n        additional_forward_args: Any = None,\n        **kwargs: Any,\n    ) -&gt; HSISpatialAttributes | HSISpectralAttributes | list[HSISpatialAttributes] | list[HSISpectralAttributes]:\n        \"\"\"A wrapper function to attribute the image using the LIME method. It executes either the\n        `get_spatial_attributes` or `get_spectral_attributes` method based on the provided `attribution_type`. For more\n        detailed description of the methods, please refer to the respective method documentation.\n\n        Args:\n            hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n                If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n                The output will be a list of HSISpatialAttributes or HSISpectralAttributes objects.\n            target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n                methods assume that the output has only one class. If the output has multiple classes, the target index\n                must be provided. For multiple input images, a list of target indices can be provided, one for each\n                image or single target value will be used for all images. Defaults to None.\n            attribution_type (Literal[\"spatial\", \"spectral\"] | None, optional): The type of attribution to be computed.\n                User can compute spatial or spectral attributions with the LIME method. If None, the method will\n                throw an error. Defaults to None.\n            additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n                the inputs for which attributions should not be computed, this argument can be provided.\n                It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n                containing multiple additional arguments including tensors or any arbitrary python types.\n                These arguments are provided to forward_func in order following the arguments in inputs.\n                Note that attributions are not computed with respect to these arguments. Default: None\n            kwargs (Any): Additional keyword arguments for the LIME method.\n\n        Returns:\n            HSISpectralAttributes | HSISpatialAttributes | list[HSISpectralAttributes | HSISpatialAttributes]:\n                The computed attributions Spectral or Spatial for the input hyperspectral image(s).\n                if a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n\n        Raises:\n            RuntimeError: If the Lime object is not initialized or is not an instance of LimeBase.\n            ValueError: If number of HSI images is not equal to the number of masks provided.\n\n        Examples:\n            &gt;&gt;&gt; simple_model = lambda x: torch.rand((x.shape[0], 2))\n            &gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n            &gt;&gt;&gt; segmentation_mask = torch.randint(1, 4, (1, 240, 240))\n            &gt;&gt;&gt; lime = meteors.attr.Lime(\n                    explainable_model=ExplainableModel(simple_model, \"regression\"), interpretable_model=SkLearnLasso(alpha=0.1)\n                )\n            &gt;&gt;&gt; spatial_attribution = lime.attribute(hsi, segmentation_mask=segmentation_mask, target=0, attribution_type=\"spatial\")\n            &gt;&gt;&gt; spatial_attribution.hsi\n            HSI(shape=(4, 240, 240), dtype=torch.float32)\n            &gt;&gt;&gt; band_mask = torch.randint(1, 4, (4, 1, 1)).repeat(1, 240, 240)\n            &gt;&gt;&gt; band_names = [\"R\", \"G\", \"B\"]\n            &gt;&gt;&gt; spectral_attribution = lime.attribute(\n            ...     hsi, band_mask=band_mask, band_names=band_names, target=0, attribution_type=\"spectral\"\n            ... )\n            &gt;&gt;&gt; spectral_attribution.hsi\n            HSI(shape=(4, 240, 240), dtype=torch.float32)\n        \"\"\"\n        if attribution_type == \"spatial\":\n            return self.get_spatial_attributes(\n                hsi, target=target, additional_forward_args=additional_forward_args, **kwargs\n            )\n        elif attribution_type == \"spectral\":\n            return self.get_spectral_attributes(\n                hsi, target=target, additional_forward_args=additional_forward_args, **kwargs\n            )\n        raise ValueError(f\"Unsupported attribution type: {attribution_type}. Use 'spatial' or 'spectral'\")\n\n    def get_spatial_attributes(\n        self,\n        hsi: list[HSI] | HSI,\n        segmentation_mask: np.ndarray | torch.Tensor | list[np.ndarray | torch.Tensor] | None = None,\n        target: list[int] | int | None = None,\n        n_samples: int = 10,\n        perturbations_per_eval: int = 4,\n        verbose: bool = False,\n        segmentation_method: Literal[\"slic\", \"patch\"] = \"slic\",\n        additional_forward_args: Any = None,\n        **segmentation_method_params: Any,\n    ) -&gt; list[HSISpatialAttributes] | HSISpatialAttributes:\n        \"\"\"\n        Get spatial attributes of an hsi image using the LIME method. Based on the provided hsi and segmentation mask\n        LIME method attributes the `superpixels` provided by the segmentation mask. Please refer to the original paper\n        `https://arxiv.org/abs/1602.04938` for more details or to Christoph Molnar's book\n        `https://christophm.github.io/interpretable-ml-book/lime.html`.\n\n        This function attributes the hyperspectral image using the LIME (Local Interpretable Model-Agnostic Explanations)\n        method for spatial data. It returns an `HSISpatialAttributes` object that contains the hyperspectral image,,\n        the attributions, the segmentation mask, and the score of the interpretable model used for the explanation.\n\n        Args:\n            hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n                If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n                The output will be a list of HSISpatialAttributes objects.\n            segmentation_mask (np.ndarray | torch.Tensor | list[np.ndarray | torch.Tensor] | None, optional):\n                A segmentation mask according to which the attribution should be performed.\n                The segmentation mask should have a 2D or 3D shape, which can be broadcastable to the shape of the\n                input image. The only dimension on which the image and the mask shapes can differ is the spectral\n                dimension, marked with letter `C` in the `image.orientation` parameter. If None, a new segmentation mask\n                is created using the `segmentation_method`. Additional parameters for the segmentation method may be\n                passed as kwargs. If multiple HSI images are provided, a list of segmentation masks can be provided,\n                one for each image. If list is not provided method will assume that the same segmentation mask is used\n                    for all images. Defaults to None.\n            target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n                methods assume that the output has only one class. If the output has multiple classes, the target index\n                must be provided. For multiple input images, a list of target indices can be provided, one for each\n                image or single target value will be used for all images. Defaults to None.\n            n_samples (int, optional): The number of samples to generate/analyze in LIME. The more the better but slower.\n                Defaults to 10.\n            perturbations_per_eval (int, optional): The number of perturbations to evaluate at once\n                (Simply the inner batch size). Defaults to 4.\n            verbose (bool, optional): Whether to show the progress bar. Defaults to False.\n            segmentation_method (Literal[\"slic\", \"patch\"], optional):\n                Segmentation method used only if `segmentation_mask` is None. Defaults to \"slic\".\n            additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n                the inputs for which attributions should not be computed, this argument can be provided.\n                It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n                containing multiple additional arguments including tensors or any arbitrary python types.\n                These arguments are provided to forward_func in order following the arguments in inputs.\n                Note that attributions are not computed with respect to these arguments. Default: None\n            **segmentation_method_params (Any): Additional parameters for the segmentation method.\n\n        Returns:\n            HSISpatialAttributes | list[HSISpatialAttributes]: An object containing the image, the attributions,\n                the segmentation mask, and the score of the interpretable model used for the explanation.\n\n        Raises:\n            RuntimeError: If the Lime object is not initialized or is not an instance of LimeBase.\n            MaskCreationError: If there is an error creating the segmentation mask.\n            ValueError: If the number of segmentation masks is not equal to the number of HSI images provided.\n            HSIAttributesError: If there is an error during creating spatial attribution.\n\n        Examples:\n            &gt;&gt;&gt; simple_model = lambda x: torch.rand((x.shape[0], 2))\n            &gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n            &gt;&gt;&gt; segmentation_mask = torch.randint(1, 4, (1, 240, 240))\n            &gt;&gt;&gt; lime = meteors.attr.Lime(\n                    explainable_model=ExplainableModel(simple_model, \"regression\"), interpretable_model=SkLearnLasso(alpha=0.1)\n                )\n            &gt;&gt;&gt; spatial_attribution = lime.get_spatial_attributes(hsi, segmentation_mask=segmentation_mask, target=0)\n            &gt;&gt;&gt; spatial_attribution.hsi\n            HSI(shape=(4, 240, 240), dtype=torch.float32)\n            &gt;&gt;&gt; spatial_attribution.attributes.shape\n            torch.Size([4, 240, 240])\n            &gt;&gt;&gt; spatial_attribution.segmentation_mask.shape\n            torch.Size([1, 240, 240])\n            &gt;&gt;&gt; spatial_attribution.score\n            1.0\n        \"\"\"\n        if self._attribution_method is None or not isinstance(self._attribution_method, LimeBase):\n            raise RuntimeError(\"Lime object not initialized\")  # pragma: no cover\n\n        if isinstance(hsi, HSI):\n            hsi = [hsi]\n\n        if not all(isinstance(hsi_image, HSI) for hsi_image in hsi):\n            raise TypeError(\"All of the input hyperspectral images must be of type HSI\")\n\n        if segmentation_mask is None:\n            segmentation_mask = self.get_segmentation_mask(hsi[0], segmentation_method, **segmentation_method_params)\n\n            warnings.warn(\n                \"Segmentation mask is created based on the first HSI image provided, this approach may not be optimal as \"\n                \"the same segmentation mask may not be the best suitable for all images\",\n                UserWarning,\n            )\n\n        if isinstance(segmentation_mask, tuple):\n            segmentation_mask = tuple(segmentation_mask)\n        elif not isinstance(segmentation_mask, list):\n            segmentation_mask = [segmentation_mask] * len(hsi)\n\n        if len(hsi) != len(segmentation_mask):\n            raise ValueError(\n                f\"Number of segmentation masks should be equal to the number of HSI images provided, provided {len(segmentation_mask)}\"\n            )\n\n        segmentation_mask = [\n            ensure_torch_tensor(mask, f\"Segmentation mask number {idx+1} should be None, numpy array, or torch tensor\")\n            for idx, mask in enumerate(segmentation_mask)\n        ]\n        segmentation_mask = [\n            mask.unsqueeze(0).moveaxis(0, hsi_img.spectral_axis) if mask.ndim != hsi_img.image.ndim else mask\n            for hsi_img, mask in zip(hsi, segmentation_mask)\n        ]\n        segmentation_mask = [\n            validate_mask_shape(\"segmentation\", hsi_img, mask) for hsi_img, mask in zip(hsi, segmentation_mask)\n        ]\n\n        hsi_input = torch.stack([hsi_img.get_image() for hsi_img in hsi], dim=0)\n        segmentation_mask = torch.stack(segmentation_mask, dim=0)\n\n        assert segmentation_mask.shape == hsi_input.shape\n\n        segmentation_mask = segmentation_mask.to(self.device)\n        hsi_input = hsi_input.to(self.device)\n\n        lime_attributes, score = self._attribution_method.attribute(\n            inputs=hsi_input,\n            target=target,\n            feature_mask=segmentation_mask,\n            n_samples=n_samples,\n            perturbations_per_eval=perturbations_per_eval,\n            additional_forward_args=additional_forward_args,\n            show_progress=verbose,\n            return_input_shape=True,\n        )\n\n        try:\n            spatial_attribution = [\n                HSISpatialAttributes(\n                    hsi=hsi_img,\n                    attributes=lime_attr,\n                    mask=segmentation_mask[idx].expand_as(hsi_img.image),\n                    score=score.item(),\n                    attribution_method=\"Lime\",\n                )\n                for idx, (hsi_img, lime_attr) in enumerate(zip(hsi, lime_attributes))\n            ]\n        except Exception as e:\n            raise HSIAttributesError(f\"Error during creating spatial attribution {e}\") from e\n\n        return spatial_attribution[0] if len(spatial_attribution) == 1 else spatial_attribution\n\n    def get_spectral_attributes(\n        self,\n        hsi: list[HSI] | HSI,\n        band_mask: np.ndarray | torch.Tensor | list[np.ndarray | torch.Tensor] | None = None,\n        target: list[int] | int | None = None,\n        n_samples: int = 10,\n        perturbations_per_eval: int = 4,\n        verbose: bool = False,\n        additional_forward_args: Any = None,\n        band_names: list[str | list[str]] | dict[tuple[str, ...] | str, int] | None = None,\n    ) -&gt; HSISpectralAttributes | list[HSISpectralAttributes]:\n        \"\"\"\n        Attributes the hsi image using LIME method for spectral data. Based on the provided hsi and band mask, the LIME\n        method attributes the hsi based on `superbands` (clustered bands) provided by the band mask.\n        Please refer to the original paper `https://arxiv.org/abs/1602.04938` for more details or to\n        Christoph Molnar's book `https://christophm.github.io/interpretable-ml-book/lime.html`.\n\n        The function returns a HSISpectralAttributes object that contains the image, the attributions, the band mask,\n        the band names, and the score of the interpretable model used for the explanation.\n\n        Args:\n            hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n                If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n                The output will be a list of HSISpatialAttributes objects.\n            band_mask (np.ndarray | torch.Tensor | list[np.ndarray | torch.Tensor] | None, optional): Band mask that\n                is used for the spectral attribution. The band mask should have a 1D or 3D shape, which can be\n                broadcastable to the shape of the input image. The only dimensions on which the image and the mask shapes\n                can differ is the height and width dimensions, marked with letters `H` and `W` in the `image.orientation`\n                parameter. If equals to None, the band mask is created within the function. If multiple HSI images are\n                provided, a list of band masks can be provided, one for each image. If list is not provided method will\n                assume that the same band mask is used for all images. Defaults to None.\n            target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n                methods assume that the output has only one class. If the output has multiple classes, the target index\n                must be provided. For multiple input images, a list of target indices can be provided, one for each\n                image or single target value will be used for all images. Defaults to None.\n            n_samples (int, optional): The number of samples to generate/analyze in LIME. The more the better but slower.\n                Defaults to 10.\n            perturbations_per_eval (int, optional): The number of perturbations to evaluate at once\n                (Simply the inner batch size). Defaults to 4.\n            verbose (bool, optional): Whether to show the progress bar. Defaults to False.\n            segmentation_method (Literal[\"slic\", \"patch\"], optional):\n                Segmentation method used only if `segmentation_mask` is None. Defaults to \"slic\".\n            additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n                the inputs for which attributions should not be computed, this argument can be provided.\n                It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n                containing multiple additional arguments including tensors or any arbitrary python types.\n                These arguments are provided to forward_func in order following the arguments in inputs.\n                Note that attributions are not computed with respect to these arguments. Default: None\n            band_names (list[str] | dict[str | tuple[str, ...], int] | None, optional): Band names. Defaults to None.\n\n        Returns:\n            HSISpectralAttributes | list[HSISpectralAttributes]: An object containing the image, the attributions,\n                the band mask, the band names, and the score of the interpretable model used for the explanation.\n\n        Raises:\n            RuntimeError: If the Lime object is not initialized or is not an instance of LimeBase.\n            MaskCreationError: If there is an error creating the band mask.\n            ValueError: If the number of band masks is not equal to the number of HSI images provided.\n            HSIAttributesError: If there is an error during creating spectral attribution.\n\n        Examples:\n            &gt;&gt;&gt; simple_model = lambda x: torch.rand((x.shape[0], 2))\n            &gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n            &gt;&gt;&gt; band_mask = torch.randint(1, 4, (4, 1, 1)).repeat(1, 240, 240)\n            &gt;&gt;&gt; band_names = [\"R\", \"G\", \"B\"]\n            &gt;&gt;&gt; lime = meteors.attr.Lime(\n                    explainable_model=ExplainableModel(simple_model, \"regression\"), interpretable_model=SkLearnLasso(alpha=0.1)\n                )\n            &gt;&gt;&gt; spectral_attribution = lime.get_spectral_attributes(hsi, band_mask=band_mask, band_names=band_names, target=0)\n            &gt;&gt;&gt; spectral_attribution.hsi\n            HSI(shape=(4, 240, 240), dtype=torch.float32)\n            &gt;&gt;&gt; spectral_attribution.attributes.shape\n            torch.Size([4, 240, 240])\n            &gt;&gt;&gt; spectral_attribution.band_mask.shape\n            torch.Size([4, 240, 240])\n            &gt;&gt;&gt; spectral_attribution.band_names\n            [\"R\", \"G\", \"B\"]\n            &gt;&gt;&gt; spectral_attribution.score\n            1.0\n        \"\"\"\n\n        if self._attribution_method is None or not isinstance(self._attribution_method, LimeBase):\n            raise RuntimeError(\"Lime object not initialized\")  # pragma: no cover\n\n        if isinstance(hsi, HSI):\n            hsi = [hsi]\n\n        if not all(isinstance(hsi_image, HSI) for hsi_image in hsi):\n            raise TypeError(\"All of the input hyperspectral images must be of type HSI\")\n\n        if band_mask is None:\n            created_bands = [self.get_band_mask(hsi_img, band_names) for hsi_img in hsi]\n            band_mask, band_name_list = zip(*created_bands)\n            band_names = band_name_list[0]\n\n        if isinstance(band_mask, tuple):\n            band_mask = list(band_mask)\n        elif not isinstance(band_mask, list):\n            band_mask = [band_mask]\n\n        if len(hsi) != len(band_mask):\n            if len(band_mask) == 1:\n                band_mask = band_mask * len(hsi)\n                logger.info(\"Reusing the same band mask for all images\")\n            else:\n                raise ValueError(\n                    f\"Number of band masks should be equal to the number of HSI images provided, provided {len(band_mask)}\"\n                )\n\n        band_mask = [\n            ensure_torch_tensor(mask, f\"Band mask number {idx+1} should be None, numpy array, or torch tensor\")\n            for idx, mask in enumerate(band_mask)\n        ]\n        band_mask = [\n            mask.unsqueeze(-1).unsqueeze(-1).moveaxis(0, hsi_img.spectral_axis)\n            if mask.ndim != hsi_img.image.ndim\n            else mask\n            for hsi_img, mask in zip(hsi, band_mask)\n        ]\n        band_mask = [validate_mask_shape(\"band\", hsi_img, mask) for hsi_img, mask in zip(hsi, band_mask)]\n\n        hsi_input = torch.stack([hsi_img.get_image() for hsi_img in hsi], dim=0)\n        band_mask = torch.stack(band_mask, dim=0)\n\n        if band_names is None:\n            band_names = {str(segment): idx for idx, segment in enumerate(torch.unique(band_mask))}\n        else:\n            logger.debug(\n                \"Band names are provided and will be used. In the future, there should be an option to validate them.\"\n            )\n\n        assert hsi_input.shape == band_mask.shape\n\n        hsi_input = hsi_input.to(self.device)\n        band_mask = band_mask.to(self.device)\n\n        lime_attributes, score = self._attribution_method.attribute(\n            inputs=hsi_input,\n            target=target,\n            feature_mask=band_mask,\n            n_samples=n_samples,\n            perturbations_per_eval=perturbations_per_eval,\n            additional_forward_args=additional_forward_args,\n            show_progress=verbose,\n            return_input_shape=True,\n        )\n\n        try:\n            spectral_attribution = [\n                HSISpectralAttributes(\n                    hsi=hsi_img,\n                    attributes=lime_attr,\n                    mask=band_mask[idx].expand_as(hsi_img.image),\n                    band_names=band_names,\n                    score=score.item(),\n                    attribution_method=\"Lime\",\n                )\n                for idx, (hsi_img, lime_attr) in enumerate(zip(hsi, lime_attributes))\n            ]\n        except Exception as e:\n            raise HSIAttributesError(f\"Error during creating spectral attribution {e}\") from e\n\n        return spectral_attribution[0] if len(spectral_attribution) == 1 else spectral_attribution\n\n    @staticmethod\n    def _get_slic_segmentation_mask(\n        hsi: HSI, num_interpret_features: int = 10, *args: Any, **kwargs: Any\n    ) -&gt; torch.Tensor:\n        \"\"\"Creates a segmentation mask using the SLIC method.\n\n        Args:\n            hsi (HSI): An HSI object for which the segmentation mask is created.\n            num_interpret_features (int, optional): Number of segments. Defaults to 10.\n            *args: Additional positional arguments to be passed to the SLIC method.\n            **kwargs: Additional keyword arguments to be passed to the SLIC method.\n\n        Returns:\n            torch.Tensor: An output segmentation mask.\n        \"\"\"\n        segmentation_mask = slic(\n            hsi.get_image().cpu().detach().numpy(),\n            n_segments=num_interpret_features,\n            mask=hsi.spatial_binary_mask.cpu().detach().numpy(),\n            channel_axis=hsi.spectral_axis,\n            *args,\n            **kwargs,\n        )\n\n        if segmentation_mask.min() == 1:\n            segmentation_mask -= 1\n\n        segmentation_mask = torch.from_numpy(segmentation_mask)\n        segmentation_mask = segmentation_mask.unsqueeze(dim=hsi.spectral_axis)\n\n        return segmentation_mask\n\n    @staticmethod\n    def _get_patch_segmentation_mask(hsi: HSI, patch_size: int | float = 10, *args: Any, **kwargs: Any) -&gt; torch.Tensor:\n        \"\"\"\n        Creates a segmentation mask using the patch method - creates small squares of the same size\n            and assigns a unique value to each square.\n\n        Args:\n            hsi (HSI): An HSI object for which the segmentation mask is created.\n            patch_size (int, optional): Size of the patch, the hsi size should be divisible by this value.\n                Defaults to 10.\n\n        Returns:\n            torch.Tensor: An output segmentation mask.\n        \"\"\"\n        if patch_size &lt; 1 or not isinstance(patch_size, (int, float)):\n            raise ValueError(\"Invalid patch_size. patch_size must be a positive integer\")\n\n        if hsi.image.shape[1] % patch_size != 0 or hsi.image.shape[2] % patch_size != 0:\n            raise ValueError(\"Invalid patch_size. patch_size must be a factor of both width and height of the hsi\")\n\n        height, width = hsi.image.shape[1], hsi.image.shape[2]\n\n        idx_mask = torch.arange(height // patch_size * width // patch_size, device=hsi.device).reshape(\n            height // patch_size, width // patch_size\n        )\n        idx_mask += 1\n        segmentation_mask = torch.repeat_interleave(idx_mask, patch_size, dim=0)\n        segmentation_mask = torch.repeat_interleave(segmentation_mask, patch_size, dim=1)\n        segmentation_mask = segmentation_mask * hsi.spatial_binary_mask\n        # segmentation_mask = torch.repeat_interleave(\n        # torch.unsqueeze(segmentation_mask, dim=hsi.spectral_axis),\n        # repeats=hsi.image.shape[hsi.spectral_axis], dim=hsi.spectral_axis)\n        segmentation_mask = segmentation_mask.unsqueeze(dim=hsi.spectral_axis)\n\n        mask_idx = np.unique(segmentation_mask).tolist()\n        for idx, mask_val in enumerate(mask_idx):\n            segmentation_mask[segmentation_mask == mask_val] = idx\n\n        return segmentation_mask\n</code></pre>"},{"location":"reference/#src.meteors.attr.lime.Lime.attribute","title":"<code>attribute(hsi, target=None, attribution_type=None, additional_forward_args=None, **kwargs)</code>","text":"<p>A wrapper function to attribute the image using the LIME method. It executes either the <code>get_spatial_attributes</code> or <code>get_spectral_attributes</code> method based on the provided <code>attribution_type</code>. For more detailed description of the methods, please refer to the respective method documentation.</p> <p>Parameters:</p> Name Type Description Default <code>hsi</code> <code>list[HSI] | HSI</code> <p>Input hyperspectral image(s) for which the attributions are to be computed. If a list of HSI objects is provided, the attributions are computed for each HSI object in the list. The output will be a list of HSISpatialAttributes or HSISpectralAttributes objects.</p> required <code>target</code> <code>list[int] | int | None</code> <p>target class index for computing the attributions. If None, methods assume that the output has only one class. If the output has multiple classes, the target index must be provided. For multiple input images, a list of target indices can be provided, one for each image or single target value will be used for all images. Defaults to None.</p> <code>None</code> <code>attribution_type</code> <code>Literal['spatial', 'spectral'] | None</code> <p>The type of attribution to be computed. User can compute spatial or spectral attributions with the LIME method. If None, the method will throw an error. Defaults to None.</p> <code>None</code> <code>additional_forward_args</code> <code>Any</code> <p>If the forward function requires additional arguments other than the inputs for which attributions should not be computed, this argument can be provided. It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple containing multiple additional arguments including tensors or any arbitrary python types. These arguments are provided to forward_func in order following the arguments in inputs. Note that attributions are not computed with respect to these arguments. Default: None</p> <code>None</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments for the LIME method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>HSISpatialAttributes | HSISpectralAttributes | list[HSISpatialAttributes] | list[HSISpectralAttributes]</code> <p>HSISpectralAttributes | HSISpatialAttributes | list[HSISpectralAttributes | HSISpatialAttributes]: The computed attributions Spectral or Spatial for the input hyperspectral image(s). if a list of HSI objects is provided, the attributions are computed for each HSI object in the list.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the Lime object is not initialized or is not an instance of LimeBase.</p> <code>ValueError</code> <p>If number of HSI images is not equal to the number of masks provided.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; simple_model = lambda x: torch.rand((x.shape[0], 2))\n&gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n&gt;&gt;&gt; segmentation_mask = torch.randint(1, 4, (1, 240, 240))\n&gt;&gt;&gt; lime = meteors.attr.Lime(\n        explainable_model=ExplainableModel(simple_model, \"regression\"), interpretable_model=SkLearnLasso(alpha=0.1)\n    )\n&gt;&gt;&gt; spatial_attribution = lime.attribute(hsi, segmentation_mask=segmentation_mask, target=0, attribution_type=\"spatial\")\n&gt;&gt;&gt; spatial_attribution.hsi\nHSI(shape=(4, 240, 240), dtype=torch.float32)\n&gt;&gt;&gt; band_mask = torch.randint(1, 4, (4, 1, 1)).repeat(1, 240, 240)\n&gt;&gt;&gt; band_names = [\"R\", \"G\", \"B\"]\n&gt;&gt;&gt; spectral_attribution = lime.attribute(\n...     hsi, band_mask=band_mask, band_names=band_names, target=0, attribution_type=\"spectral\"\n... )\n&gt;&gt;&gt; spectral_attribution.hsi\nHSI(shape=(4, 240, 240), dtype=torch.float32)\n</code></pre> Source code in <code>src/meteors/attr/lime.py</code> <pre><code>def attribute(  # type: ignore\n    self,\n    hsi: list[HSI] | HSI,\n    target: list[int] | int | None = None,\n    attribution_type: Literal[\"spatial\", \"spectral\"] | None = None,\n    additional_forward_args: Any = None,\n    **kwargs: Any,\n) -&gt; HSISpatialAttributes | HSISpectralAttributes | list[HSISpatialAttributes] | list[HSISpectralAttributes]:\n    \"\"\"A wrapper function to attribute the image using the LIME method. It executes either the\n    `get_spatial_attributes` or `get_spectral_attributes` method based on the provided `attribution_type`. For more\n    detailed description of the methods, please refer to the respective method documentation.\n\n    Args:\n        hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n            If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n            The output will be a list of HSISpatialAttributes or HSISpectralAttributes objects.\n        target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n            methods assume that the output has only one class. If the output has multiple classes, the target index\n            must be provided. For multiple input images, a list of target indices can be provided, one for each\n            image or single target value will be used for all images. Defaults to None.\n        attribution_type (Literal[\"spatial\", \"spectral\"] | None, optional): The type of attribution to be computed.\n            User can compute spatial or spectral attributions with the LIME method. If None, the method will\n            throw an error. Defaults to None.\n        additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n            the inputs for which attributions should not be computed, this argument can be provided.\n            It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n            containing multiple additional arguments including tensors or any arbitrary python types.\n            These arguments are provided to forward_func in order following the arguments in inputs.\n            Note that attributions are not computed with respect to these arguments. Default: None\n        kwargs (Any): Additional keyword arguments for the LIME method.\n\n    Returns:\n        HSISpectralAttributes | HSISpatialAttributes | list[HSISpectralAttributes | HSISpatialAttributes]:\n            The computed attributions Spectral or Spatial for the input hyperspectral image(s).\n            if a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n\n    Raises:\n        RuntimeError: If the Lime object is not initialized or is not an instance of LimeBase.\n        ValueError: If number of HSI images is not equal to the number of masks provided.\n\n    Examples:\n        &gt;&gt;&gt; simple_model = lambda x: torch.rand((x.shape[0], 2))\n        &gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n        &gt;&gt;&gt; segmentation_mask = torch.randint(1, 4, (1, 240, 240))\n        &gt;&gt;&gt; lime = meteors.attr.Lime(\n                explainable_model=ExplainableModel(simple_model, \"regression\"), interpretable_model=SkLearnLasso(alpha=0.1)\n            )\n        &gt;&gt;&gt; spatial_attribution = lime.attribute(hsi, segmentation_mask=segmentation_mask, target=0, attribution_type=\"spatial\")\n        &gt;&gt;&gt; spatial_attribution.hsi\n        HSI(shape=(4, 240, 240), dtype=torch.float32)\n        &gt;&gt;&gt; band_mask = torch.randint(1, 4, (4, 1, 1)).repeat(1, 240, 240)\n        &gt;&gt;&gt; band_names = [\"R\", \"G\", \"B\"]\n        &gt;&gt;&gt; spectral_attribution = lime.attribute(\n        ...     hsi, band_mask=band_mask, band_names=band_names, target=0, attribution_type=\"spectral\"\n        ... )\n        &gt;&gt;&gt; spectral_attribution.hsi\n        HSI(shape=(4, 240, 240), dtype=torch.float32)\n    \"\"\"\n    if attribution_type == \"spatial\":\n        return self.get_spatial_attributes(\n            hsi, target=target, additional_forward_args=additional_forward_args, **kwargs\n        )\n    elif attribution_type == \"spectral\":\n        return self.get_spectral_attributes(\n            hsi, target=target, additional_forward_args=additional_forward_args, **kwargs\n        )\n    raise ValueError(f\"Unsupported attribution type: {attribution_type}. Use 'spatial' or 'spectral'\")\n</code></pre>"},{"location":"reference/#src.meteors.attr.lime.Lime.get_band_mask","title":"<code>get_band_mask(hsi, band_names=None, band_indices=None, band_wavelengths=None, device=None, repeat_dimensions=False)</code>  <code>staticmethod</code>","text":"<p>Generates a band mask based on the provided hsi and band information.</p> <p>Remember you need to provide either band_names, band_indices, or band_wavelengths to create the band mask. If you provide more than one, the band mask will be created using only one using the following priority: band_names &gt; band_wavelengths &gt; band_indices.</p> <p>Parameters:</p> Name Type Description Default <code>hsi</code> <code>HSI</code> <p>The input hyperspectral image.</p> required <code>band_names</code> <code>None | list[str | list[str]] | dict[tuple[str, ...] | str, int]</code> <p>The names of the spectral bands to include in the mask. Defaults to None.</p> <code>None</code> <code>band_indices</code> <code>None | dict[str | tuple[str, ...], list[tuple[int, int]] | tuple[int, int] | list[int]]</code> <p>The indices or ranges of indices of the spectral bands to include in the mask. Defaults to None.</p> <code>None</code> <code>band_wavelengths</code> <code>None | dict[str | tuple[str, ...], list[tuple[float, float]] | tuple[float, float], list[float], float]</code> <p>The wavelengths or ranges of wavelengths of the spectral bands to include in the mask. Defaults to None.</p> <code>None</code> <code>device</code> <code>str | device | None</code> <p>The device to use for computation. Defaults to None.</p> <code>None</code> <code>repeat_dimensions</code> <code>bool</code> <p>Whether to repeat the dimensions of the mask to match the input hsi shape. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>tuple[torch.Tensor, dict[tuple[str, ...] | str, int]]: A tuple containing the band mask tensor and a dictionary</p> <code>dict[tuple[str, ...] | str, int]</code> <p>mapping band names to segment IDs.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input hsi is not an instance of the HSI class.</p> <code>ValueError</code> <p>If no band names, indices, or wavelengths are provided.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((len(wavelengths), 10, 10)), wavelengths=wavelengths)\n&gt;&gt;&gt; band_names = [\"R\", \"G\"]\n&gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_names=band_names)\n&gt;&gt;&gt; dict_labels_to_segment_ids\n{\"R\": 1, \"G\": 2}\n&gt;&gt;&gt; band_indices = {\"RGB\": [0, 1, 2]}\n&gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_indices=band_indices)\n&gt;&gt;&gt; dict_labels_to_segment_ids\n{\"RGB\": 1}\n&gt;&gt;&gt; band_wavelengths = {\"RGB\": [(462.08, 465.27), (465.27, 468.47), (468.47, 471.68)]}\n&gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_wavelengths=band_wavelengths)\n&gt;&gt;&gt; dict_labels_to_segment_ids\n{\"RGB\": 1}\n</code></pre> Source code in <code>src/meteors/attr/lime.py</code> <pre><code>@staticmethod\ndef get_band_mask(\n    hsi: HSI,\n    band_names: None | list[str | list[str]] | dict[tuple[str, ...] | str, int] = None,\n    band_indices: None | dict[str | tuple[str, ...], ListOfWavelengthsIndices] = None,\n    band_wavelengths: None | dict[str | tuple[str, ...], ListOfWavelengths] = None,\n    device: str | torch.device | None = None,\n    repeat_dimensions: bool = False,\n) -&gt; tuple[torch.Tensor, dict[tuple[str, ...] | str, int]]:\n    \"\"\"Generates a band mask based on the provided hsi and band information.\n\n    Remember you need to provide either band_names, band_indices, or band_wavelengths to create the band mask.\n    If you provide more than one, the band mask will be created using only one using the following priority:\n    band_names &gt; band_wavelengths &gt; band_indices.\n\n    Args:\n        hsi (HSI): The input hyperspectral image.\n        band_names (None | list[str | list[str]] | dict[tuple[str, ...] | str, int], optional):\n            The names of the spectral bands to include in the mask. Defaults to None.\n        band_indices (None | dict[str | tuple[str, ...], list[tuple[int, int]] | tuple[int, int] | list[int]], optional):\n            The indices or ranges of indices of the spectral bands to include in the mask. Defaults to None.\n        band_wavelengths (None | dict[str | tuple[str, ...], list[tuple[float, float]] | tuple[float, float], list[float], float], optional):\n            The wavelengths or ranges of wavelengths of the spectral bands to include in the mask. Defaults to None.\n        device (str | torch.device | None, optional):\n            The device to use for computation. Defaults to None.\n        repeat_dimensions (bool, optional):\n            Whether to repeat the dimensions of the mask to match the input hsi shape. Defaults to False.\n\n    Returns:\n        tuple[torch.Tensor, dict[tuple[str, ...] | str, int]]: A tuple containing the band mask tensor and a dictionary\n        mapping band names to segment IDs.\n\n    Raises:\n        TypeError: If the input hsi is not an instance of the HSI class.\n        ValueError: If no band names, indices, or wavelengths are provided.\n\n    Examples:\n        &gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((len(wavelengths), 10, 10)), wavelengths=wavelengths)\n        &gt;&gt;&gt; band_names = [\"R\", \"G\"]\n        &gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_names=band_names)\n        &gt;&gt;&gt; dict_labels_to_segment_ids\n        {\"R\": 1, \"G\": 2}\n        &gt;&gt;&gt; band_indices = {\"RGB\": [0, 1, 2]}\n        &gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_indices=band_indices)\n        &gt;&gt;&gt; dict_labels_to_segment_ids\n        {\"RGB\": 1}\n        &gt;&gt;&gt; band_wavelengths = {\"RGB\": [(462.08, 465.27), (465.27, 468.47), (468.47, 471.68)]}\n        &gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_wavelengths=band_wavelengths)\n        &gt;&gt;&gt; dict_labels_to_segment_ids\n        {\"RGB\": 1}\n    \"\"\"\n    if not isinstance(hsi, HSI):\n        raise TypeError(\"hsi should be an instance of HSI class\")\n\n    try:\n        if not (band_names is not None or band_indices is not None or band_wavelengths is not None):\n            raise ValueError(\"No band names, indices, or wavelengths are provided.\")\n\n        # validate types\n        dict_labels_to_segment_ids = None\n        if band_names is not None:\n            logger.debug(\"Getting band mask from band names of spectral bands\")\n            if band_wavelengths is not None or band_indices is not None:\n                ignored_params = [\n                    param\n                    for param in [\"band_wavelengths\", \"band_indices\"]\n                    if param in locals() and locals()[param] is not None\n                ]\n                ignored_params_str = \" and \".join(ignored_params)\n                logger.info(\n                    f\"Only the band names will be used to create the band mask. The additional parameters {ignored_params_str} will be ignored.\"\n                )\n            try:\n                validate_band_names(band_names)\n                band_groups, dict_labels_to_segment_ids = Lime._get_band_wavelengths_indices_from_band_names(\n                    hsi.wavelengths, band_names\n                )\n            except Exception as e:\n                raise BandSelectionError(f\"Incorrect band names provided: {e}\") from e\n        elif band_wavelengths is not None:\n            logger.debug(\"Getting band mask from band groups given by ranges of wavelengths\")\n            if band_indices is not None:\n                logger.info(\n                    \"Only the band wavelengths will be used to create the band mask. The band_indices will be ignored.\"\n                )\n            validate_band_format(band_wavelengths, variable_name=\"band_wavelengths\")\n            try:\n                band_groups = Lime._get_band_indices_from_band_wavelengths(\n                    hsi.wavelengths,\n                    band_wavelengths,\n                )\n            except Exception as e:\n                raise ValueError(\n                    f\"Incorrect band ranges wavelengths provided, please check if provided wavelengths are correct: {e}\"\n                ) from e\n        elif band_indices is not None:\n            logger.debug(\"Getting band mask from band groups given by ranges of indices\")\n            validate_band_format(band_indices, variable_name=\"band_indices\")\n            try:\n                band_groups = Lime._get_band_indices_from_input_band_indices(hsi.wavelengths, band_indices)\n            except Exception as e:\n                raise ValueError(\n                    f\"Incorrect band ranges indices provided, please check if provided indices are correct: {e}\"\n                ) from e\n\n        return Lime._create_tensor_band_mask(\n            hsi,\n            band_groups,\n            dict_labels_to_segment_ids=dict_labels_to_segment_ids,\n            device=device,\n            repeat_dimensions=repeat_dimensions,\n            return_dict_labels_to_segment_ids=True,\n        )\n    except Exception as e:\n        raise MaskCreationError(f\"Error creating band mask: {e}\") from e\n</code></pre>"},{"location":"reference/#src.meteors.attr.lime.Lime.get_segmentation_mask","title":"<code>get_segmentation_mask(hsi, segmentation_method='slic', **segmentation_method_params)</code>  <code>staticmethod</code>","text":"<p>Generates a segmentation mask for the given hsi using the specified segmentation method.</p> <p>Parameters:</p> Name Type Description Default <code>hsi</code> <code>HSI</code> <p>The input hyperspectral image for which the segmentation mask needs to be generated.</p> required <code>segmentation_method</code> <code>Literal['patch', 'slic']</code> <p>The segmentation method to be used. Defaults to \"slic\".</p> <code>'slic'</code> <code>**segmentation_method_params</code> <code>Any</code> <p>Additional parameters specific to the chosen segmentation method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The segmentation mask as a tensor.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input hsi is not an instance of the HSI class.</p> <code>ValueError</code> <p>If an unsupported segmentation method is specified.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hsi = meteors.HSI(image=torch.ones((3, 240, 240)), wavelengths=[462.08, 465.27, 468.47])\n&gt;&gt;&gt; segmentation_mask = mt_lime.Lime.get_segmentation_mask(hsi, segmentation_method=\"slic\")\n&gt;&gt;&gt; segmentation_mask.shape\ntorch.Size([1, 240, 240])\n&gt;&gt;&gt; segmentation_mask = meteors.attr.Lime.get_segmentation_mask(hsi, segmentation_method=\"patch\", patch_size=2)\n&gt;&gt;&gt; segmentation_mask.shape\ntorch.Size([1, 240, 240])\n&gt;&gt;&gt; segmentation_mask[0, :2, :2]\ntorch.tensor([[1, 1],\n              [1, 1]])\n&gt;&gt;&gt; segmentation_mask[0, 2:4, :2]\ntorch.tensor([[2, 2],\n              [2, 2]])\n</code></pre> Source code in <code>src/meteors/attr/lime.py</code> <pre><code>@staticmethod\ndef get_segmentation_mask(\n    hsi: HSI,\n    segmentation_method: Literal[\"patch\", \"slic\"] = \"slic\",\n    **segmentation_method_params: Any,\n) -&gt; torch.Tensor:\n    \"\"\"Generates a segmentation mask for the given hsi using the specified segmentation method.\n\n    Args:\n        hsi (HSI): The input hyperspectral image for which the segmentation mask needs to be generated.\n        segmentation_method (Literal[\"patch\", \"slic\"], optional): The segmentation method to be used.\n            Defaults to \"slic\".\n        **segmentation_method_params (Any): Additional parameters specific to the chosen segmentation method.\n\n    Returns:\n        torch.Tensor: The segmentation mask as a tensor.\n\n    Raises:\n        TypeError: If the input hsi is not an instance of the HSI class.\n        ValueError: If an unsupported segmentation method is specified.\n\n    Examples:\n        &gt;&gt;&gt; hsi = meteors.HSI(image=torch.ones((3, 240, 240)), wavelengths=[462.08, 465.27, 468.47])\n        &gt;&gt;&gt; segmentation_mask = mt_lime.Lime.get_segmentation_mask(hsi, segmentation_method=\"slic\")\n        &gt;&gt;&gt; segmentation_mask.shape\n        torch.Size([1, 240, 240])\n        &gt;&gt;&gt; segmentation_mask = meteors.attr.Lime.get_segmentation_mask(hsi, segmentation_method=\"patch\", patch_size=2)\n        &gt;&gt;&gt; segmentation_mask.shape\n        torch.Size([1, 240, 240])\n        &gt;&gt;&gt; segmentation_mask[0, :2, :2]\n        torch.tensor([[1, 1],\n                      [1, 1]])\n        &gt;&gt;&gt; segmentation_mask[0, 2:4, :2]\n        torch.tensor([[2, 2],\n                      [2, 2]])\n    \"\"\"\n    if not isinstance(hsi, HSI):\n        raise TypeError(\"hsi should be an instance of HSI class\")\n\n    try:\n        if segmentation_method == \"slic\":\n            return Lime._get_slic_segmentation_mask(hsi, **segmentation_method_params)\n        elif segmentation_method == \"patch\":\n            return Lime._get_patch_segmentation_mask(hsi, **segmentation_method_params)\n        else:\n            raise ValueError(f\"Unsupported segmentation method: {segmentation_method}\")\n    except Exception as e:\n        raise MaskCreationError(f\"Error creating segmentation mask using method {segmentation_method}: {e}\")\n</code></pre>"},{"location":"reference/#src.meteors.attr.lime.Lime.get_spatial_attributes","title":"<code>get_spatial_attributes(hsi, segmentation_mask=None, target=None, n_samples=10, perturbations_per_eval=4, verbose=False, segmentation_method='slic', additional_forward_args=None, **segmentation_method_params)</code>","text":"<p>Get spatial attributes of an hsi image using the LIME method. Based on the provided hsi and segmentation mask LIME method attributes the <code>superpixels</code> provided by the segmentation mask. Please refer to the original paper <code>https://arxiv.org/abs/1602.04938</code> for more details or to Christoph Molnar's book <code>https://christophm.github.io/interpretable-ml-book/lime.html</code>.</p> <p>This function attributes the hyperspectral image using the LIME (Local Interpretable Model-Agnostic Explanations) method for spatial data. It returns an <code>HSISpatialAttributes</code> object that contains the hyperspectral image,, the attributions, the segmentation mask, and the score of the interpretable model used for the explanation.</p> <p>Parameters:</p> Name Type Description Default <code>hsi</code> <code>list[HSI] | HSI</code> <p>Input hyperspectral image(s) for which the attributions are to be computed. If a list of HSI objects is provided, the attributions are computed for each HSI object in the list. The output will be a list of HSISpatialAttributes objects.</p> required <code>segmentation_mask</code> <code>ndarray | Tensor | list[ndarray | Tensor] | None</code> <p>A segmentation mask according to which the attribution should be performed. The segmentation mask should have a 2D or 3D shape, which can be broadcastable to the shape of the input image. The only dimension on which the image and the mask shapes can differ is the spectral dimension, marked with letter <code>C</code> in the <code>image.orientation</code> parameter. If None, a new segmentation mask is created using the <code>segmentation_method</code>. Additional parameters for the segmentation method may be passed as kwargs. If multiple HSI images are provided, a list of segmentation masks can be provided, one for each image. If list is not provided method will assume that the same segmentation mask is used     for all images. Defaults to None.</p> <code>None</code> <code>target</code> <code>list[int] | int | None</code> <p>target class index for computing the attributions. If None, methods assume that the output has only one class. If the output has multiple classes, the target index must be provided. For multiple input images, a list of target indices can be provided, one for each image or single target value will be used for all images. Defaults to None.</p> <code>None</code> <code>n_samples</code> <code>int</code> <p>The number of samples to generate/analyze in LIME. The more the better but slower. Defaults to 10.</p> <code>10</code> <code>perturbations_per_eval</code> <code>int</code> <p>The number of perturbations to evaluate at once (Simply the inner batch size). Defaults to 4.</p> <code>4</code> <code>verbose</code> <code>bool</code> <p>Whether to show the progress bar. Defaults to False.</p> <code>False</code> <code>segmentation_method</code> <code>Literal['slic', 'patch']</code> <p>Segmentation method used only if <code>segmentation_mask</code> is None. Defaults to \"slic\".</p> <code>'slic'</code> <code>additional_forward_args</code> <code>Any</code> <p>If the forward function requires additional arguments other than the inputs for which attributions should not be computed, this argument can be provided. It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple containing multiple additional arguments including tensors or any arbitrary python types. These arguments are provided to forward_func in order following the arguments in inputs. Note that attributions are not computed with respect to these arguments. Default: None</p> <code>None</code> <code>**segmentation_method_params</code> <code>Any</code> <p>Additional parameters for the segmentation method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[HSISpatialAttributes] | HSISpatialAttributes</code> <p>HSISpatialAttributes | list[HSISpatialAttributes]: An object containing the image, the attributions, the segmentation mask, and the score of the interpretable model used for the explanation.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the Lime object is not initialized or is not an instance of LimeBase.</p> <code>MaskCreationError</code> <p>If there is an error creating the segmentation mask.</p> <code>ValueError</code> <p>If the number of segmentation masks is not equal to the number of HSI images provided.</p> <code>HSIAttributesError</code> <p>If there is an error during creating spatial attribution.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; simple_model = lambda x: torch.rand((x.shape[0], 2))\n&gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n&gt;&gt;&gt; segmentation_mask = torch.randint(1, 4, (1, 240, 240))\n&gt;&gt;&gt; lime = meteors.attr.Lime(\n        explainable_model=ExplainableModel(simple_model, \"regression\"), interpretable_model=SkLearnLasso(alpha=0.1)\n    )\n&gt;&gt;&gt; spatial_attribution = lime.get_spatial_attributes(hsi, segmentation_mask=segmentation_mask, target=0)\n&gt;&gt;&gt; spatial_attribution.hsi\nHSI(shape=(4, 240, 240), dtype=torch.float32)\n&gt;&gt;&gt; spatial_attribution.attributes.shape\ntorch.Size([4, 240, 240])\n&gt;&gt;&gt; spatial_attribution.segmentation_mask.shape\ntorch.Size([1, 240, 240])\n&gt;&gt;&gt; spatial_attribution.score\n1.0\n</code></pre> Source code in <code>src/meteors/attr/lime.py</code> <pre><code>def get_spatial_attributes(\n    self,\n    hsi: list[HSI] | HSI,\n    segmentation_mask: np.ndarray | torch.Tensor | list[np.ndarray | torch.Tensor] | None = None,\n    target: list[int] | int | None = None,\n    n_samples: int = 10,\n    perturbations_per_eval: int = 4,\n    verbose: bool = False,\n    segmentation_method: Literal[\"slic\", \"patch\"] = \"slic\",\n    additional_forward_args: Any = None,\n    **segmentation_method_params: Any,\n) -&gt; list[HSISpatialAttributes] | HSISpatialAttributes:\n    \"\"\"\n    Get spatial attributes of an hsi image using the LIME method. Based on the provided hsi and segmentation mask\n    LIME method attributes the `superpixels` provided by the segmentation mask. Please refer to the original paper\n    `https://arxiv.org/abs/1602.04938` for more details or to Christoph Molnar's book\n    `https://christophm.github.io/interpretable-ml-book/lime.html`.\n\n    This function attributes the hyperspectral image using the LIME (Local Interpretable Model-Agnostic Explanations)\n    method for spatial data. It returns an `HSISpatialAttributes` object that contains the hyperspectral image,,\n    the attributions, the segmentation mask, and the score of the interpretable model used for the explanation.\n\n    Args:\n        hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n            If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n            The output will be a list of HSISpatialAttributes objects.\n        segmentation_mask (np.ndarray | torch.Tensor | list[np.ndarray | torch.Tensor] | None, optional):\n            A segmentation mask according to which the attribution should be performed.\n            The segmentation mask should have a 2D or 3D shape, which can be broadcastable to the shape of the\n            input image. The only dimension on which the image and the mask shapes can differ is the spectral\n            dimension, marked with letter `C` in the `image.orientation` parameter. If None, a new segmentation mask\n            is created using the `segmentation_method`. Additional parameters for the segmentation method may be\n            passed as kwargs. If multiple HSI images are provided, a list of segmentation masks can be provided,\n            one for each image. If list is not provided method will assume that the same segmentation mask is used\n                for all images. Defaults to None.\n        target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n            methods assume that the output has only one class. If the output has multiple classes, the target index\n            must be provided. For multiple input images, a list of target indices can be provided, one for each\n            image or single target value will be used for all images. Defaults to None.\n        n_samples (int, optional): The number of samples to generate/analyze in LIME. The more the better but slower.\n            Defaults to 10.\n        perturbations_per_eval (int, optional): The number of perturbations to evaluate at once\n            (Simply the inner batch size). Defaults to 4.\n        verbose (bool, optional): Whether to show the progress bar. Defaults to False.\n        segmentation_method (Literal[\"slic\", \"patch\"], optional):\n            Segmentation method used only if `segmentation_mask` is None. Defaults to \"slic\".\n        additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n            the inputs for which attributions should not be computed, this argument can be provided.\n            It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n            containing multiple additional arguments including tensors or any arbitrary python types.\n            These arguments are provided to forward_func in order following the arguments in inputs.\n            Note that attributions are not computed with respect to these arguments. Default: None\n        **segmentation_method_params (Any): Additional parameters for the segmentation method.\n\n    Returns:\n        HSISpatialAttributes | list[HSISpatialAttributes]: An object containing the image, the attributions,\n            the segmentation mask, and the score of the interpretable model used for the explanation.\n\n    Raises:\n        RuntimeError: If the Lime object is not initialized or is not an instance of LimeBase.\n        MaskCreationError: If there is an error creating the segmentation mask.\n        ValueError: If the number of segmentation masks is not equal to the number of HSI images provided.\n        HSIAttributesError: If there is an error during creating spatial attribution.\n\n    Examples:\n        &gt;&gt;&gt; simple_model = lambda x: torch.rand((x.shape[0], 2))\n        &gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n        &gt;&gt;&gt; segmentation_mask = torch.randint(1, 4, (1, 240, 240))\n        &gt;&gt;&gt; lime = meteors.attr.Lime(\n                explainable_model=ExplainableModel(simple_model, \"regression\"), interpretable_model=SkLearnLasso(alpha=0.1)\n            )\n        &gt;&gt;&gt; spatial_attribution = lime.get_spatial_attributes(hsi, segmentation_mask=segmentation_mask, target=0)\n        &gt;&gt;&gt; spatial_attribution.hsi\n        HSI(shape=(4, 240, 240), dtype=torch.float32)\n        &gt;&gt;&gt; spatial_attribution.attributes.shape\n        torch.Size([4, 240, 240])\n        &gt;&gt;&gt; spatial_attribution.segmentation_mask.shape\n        torch.Size([1, 240, 240])\n        &gt;&gt;&gt; spatial_attribution.score\n        1.0\n    \"\"\"\n    if self._attribution_method is None or not isinstance(self._attribution_method, LimeBase):\n        raise RuntimeError(\"Lime object not initialized\")  # pragma: no cover\n\n    if isinstance(hsi, HSI):\n        hsi = [hsi]\n\n    if not all(isinstance(hsi_image, HSI) for hsi_image in hsi):\n        raise TypeError(\"All of the input hyperspectral images must be of type HSI\")\n\n    if segmentation_mask is None:\n        segmentation_mask = self.get_segmentation_mask(hsi[0], segmentation_method, **segmentation_method_params)\n\n        warnings.warn(\n            \"Segmentation mask is created based on the first HSI image provided, this approach may not be optimal as \"\n            \"the same segmentation mask may not be the best suitable for all images\",\n            UserWarning,\n        )\n\n    if isinstance(segmentation_mask, tuple):\n        segmentation_mask = tuple(segmentation_mask)\n    elif not isinstance(segmentation_mask, list):\n        segmentation_mask = [segmentation_mask] * len(hsi)\n\n    if len(hsi) != len(segmentation_mask):\n        raise ValueError(\n            f\"Number of segmentation masks should be equal to the number of HSI images provided, provided {len(segmentation_mask)}\"\n        )\n\n    segmentation_mask = [\n        ensure_torch_tensor(mask, f\"Segmentation mask number {idx+1} should be None, numpy array, or torch tensor\")\n        for idx, mask in enumerate(segmentation_mask)\n    ]\n    segmentation_mask = [\n        mask.unsqueeze(0).moveaxis(0, hsi_img.spectral_axis) if mask.ndim != hsi_img.image.ndim else mask\n        for hsi_img, mask in zip(hsi, segmentation_mask)\n    ]\n    segmentation_mask = [\n        validate_mask_shape(\"segmentation\", hsi_img, mask) for hsi_img, mask in zip(hsi, segmentation_mask)\n    ]\n\n    hsi_input = torch.stack([hsi_img.get_image() for hsi_img in hsi], dim=0)\n    segmentation_mask = torch.stack(segmentation_mask, dim=0)\n\n    assert segmentation_mask.shape == hsi_input.shape\n\n    segmentation_mask = segmentation_mask.to(self.device)\n    hsi_input = hsi_input.to(self.device)\n\n    lime_attributes, score = self._attribution_method.attribute(\n        inputs=hsi_input,\n        target=target,\n        feature_mask=segmentation_mask,\n        n_samples=n_samples,\n        perturbations_per_eval=perturbations_per_eval,\n        additional_forward_args=additional_forward_args,\n        show_progress=verbose,\n        return_input_shape=True,\n    )\n\n    try:\n        spatial_attribution = [\n            HSISpatialAttributes(\n                hsi=hsi_img,\n                attributes=lime_attr,\n                mask=segmentation_mask[idx].expand_as(hsi_img.image),\n                score=score.item(),\n                attribution_method=\"Lime\",\n            )\n            for idx, (hsi_img, lime_attr) in enumerate(zip(hsi, lime_attributes))\n        ]\n    except Exception as e:\n        raise HSIAttributesError(f\"Error during creating spatial attribution {e}\") from e\n\n    return spatial_attribution[0] if len(spatial_attribution) == 1 else spatial_attribution\n</code></pre>"},{"location":"reference/#src.meteors.attr.lime.Lime.get_spectral_attributes","title":"<code>get_spectral_attributes(hsi, band_mask=None, target=None, n_samples=10, perturbations_per_eval=4, verbose=False, additional_forward_args=None, band_names=None)</code>","text":"<p>Attributes the hsi image using LIME method for spectral data. Based on the provided hsi and band mask, the LIME method attributes the hsi based on <code>superbands</code> (clustered bands) provided by the band mask. Please refer to the original paper <code>https://arxiv.org/abs/1602.04938</code> for more details or to Christoph Molnar's book <code>https://christophm.github.io/interpretable-ml-book/lime.html</code>.</p> <p>The function returns a HSISpectralAttributes object that contains the image, the attributions, the band mask, the band names, and the score of the interpretable model used for the explanation.</p> <p>Parameters:</p> Name Type Description Default <code>hsi</code> <code>list[HSI] | HSI</code> <p>Input hyperspectral image(s) for which the attributions are to be computed. If a list of HSI objects is provided, the attributions are computed for each HSI object in the list. The output will be a list of HSISpatialAttributes objects.</p> required <code>band_mask</code> <code>ndarray | Tensor | list[ndarray | Tensor] | None</code> <p>Band mask that is used for the spectral attribution. The band mask should have a 1D or 3D shape, which can be broadcastable to the shape of the input image. The only dimensions on which the image and the mask shapes can differ is the height and width dimensions, marked with letters <code>H</code> and <code>W</code> in the <code>image.orientation</code> parameter. If equals to None, the band mask is created within the function. If multiple HSI images are provided, a list of band masks can be provided, one for each image. If list is not provided method will assume that the same band mask is used for all images. Defaults to None.</p> <code>None</code> <code>target</code> <code>list[int] | int | None</code> <p>target class index for computing the attributions. If None, methods assume that the output has only one class. If the output has multiple classes, the target index must be provided. For multiple input images, a list of target indices can be provided, one for each image or single target value will be used for all images. Defaults to None.</p> <code>None</code> <code>n_samples</code> <code>int</code> <p>The number of samples to generate/analyze in LIME. The more the better but slower. Defaults to 10.</p> <code>10</code> <code>perturbations_per_eval</code> <code>int</code> <p>The number of perturbations to evaluate at once (Simply the inner batch size). Defaults to 4.</p> <code>4</code> <code>verbose</code> <code>bool</code> <p>Whether to show the progress bar. Defaults to False.</p> <code>False</code> <code>segmentation_method</code> <code>Literal['slic', 'patch']</code> <p>Segmentation method used only if <code>segmentation_mask</code> is None. Defaults to \"slic\".</p> required <code>additional_forward_args</code> <code>Any</code> <p>If the forward function requires additional arguments other than the inputs for which attributions should not be computed, this argument can be provided. It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple containing multiple additional arguments including tensors or any arbitrary python types. These arguments are provided to forward_func in order following the arguments in inputs. Note that attributions are not computed with respect to these arguments. Default: None</p> <code>None</code> <code>band_names</code> <code>list[str] | dict[str | tuple[str, ...], int] | None</code> <p>Band names. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>HSISpectralAttributes | list[HSISpectralAttributes]</code> <p>HSISpectralAttributes | list[HSISpectralAttributes]: An object containing the image, the attributions, the band mask, the band names, and the score of the interpretable model used for the explanation.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the Lime object is not initialized or is not an instance of LimeBase.</p> <code>MaskCreationError</code> <p>If there is an error creating the band mask.</p> <code>ValueError</code> <p>If the number of band masks is not equal to the number of HSI images provided.</p> <code>HSIAttributesError</code> <p>If there is an error during creating spectral attribution.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; simple_model = lambda x: torch.rand((x.shape[0], 2))\n&gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n&gt;&gt;&gt; band_mask = torch.randint(1, 4, (4, 1, 1)).repeat(1, 240, 240)\n&gt;&gt;&gt; band_names = [\"R\", \"G\", \"B\"]\n&gt;&gt;&gt; lime = meteors.attr.Lime(\n        explainable_model=ExplainableModel(simple_model, \"regression\"), interpretable_model=SkLearnLasso(alpha=0.1)\n    )\n&gt;&gt;&gt; spectral_attribution = lime.get_spectral_attributes(hsi, band_mask=band_mask, band_names=band_names, target=0)\n&gt;&gt;&gt; spectral_attribution.hsi\nHSI(shape=(4, 240, 240), dtype=torch.float32)\n&gt;&gt;&gt; spectral_attribution.attributes.shape\ntorch.Size([4, 240, 240])\n&gt;&gt;&gt; spectral_attribution.band_mask.shape\ntorch.Size([4, 240, 240])\n&gt;&gt;&gt; spectral_attribution.band_names\n[\"R\", \"G\", \"B\"]\n&gt;&gt;&gt; spectral_attribution.score\n1.0\n</code></pre> Source code in <code>src/meteors/attr/lime.py</code> <pre><code>def get_spectral_attributes(\n    self,\n    hsi: list[HSI] | HSI,\n    band_mask: np.ndarray | torch.Tensor | list[np.ndarray | torch.Tensor] | None = None,\n    target: list[int] | int | None = None,\n    n_samples: int = 10,\n    perturbations_per_eval: int = 4,\n    verbose: bool = False,\n    additional_forward_args: Any = None,\n    band_names: list[str | list[str]] | dict[tuple[str, ...] | str, int] | None = None,\n) -&gt; HSISpectralAttributes | list[HSISpectralAttributes]:\n    \"\"\"\n    Attributes the hsi image using LIME method for spectral data. Based on the provided hsi and band mask, the LIME\n    method attributes the hsi based on `superbands` (clustered bands) provided by the band mask.\n    Please refer to the original paper `https://arxiv.org/abs/1602.04938` for more details or to\n    Christoph Molnar's book `https://christophm.github.io/interpretable-ml-book/lime.html`.\n\n    The function returns a HSISpectralAttributes object that contains the image, the attributions, the band mask,\n    the band names, and the score of the interpretable model used for the explanation.\n\n    Args:\n        hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n            If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n            The output will be a list of HSISpatialAttributes objects.\n        band_mask (np.ndarray | torch.Tensor | list[np.ndarray | torch.Tensor] | None, optional): Band mask that\n            is used for the spectral attribution. The band mask should have a 1D or 3D shape, which can be\n            broadcastable to the shape of the input image. The only dimensions on which the image and the mask shapes\n            can differ is the height and width dimensions, marked with letters `H` and `W` in the `image.orientation`\n            parameter. If equals to None, the band mask is created within the function. If multiple HSI images are\n            provided, a list of band masks can be provided, one for each image. If list is not provided method will\n            assume that the same band mask is used for all images. Defaults to None.\n        target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n            methods assume that the output has only one class. If the output has multiple classes, the target index\n            must be provided. For multiple input images, a list of target indices can be provided, one for each\n            image or single target value will be used for all images. Defaults to None.\n        n_samples (int, optional): The number of samples to generate/analyze in LIME. The more the better but slower.\n            Defaults to 10.\n        perturbations_per_eval (int, optional): The number of perturbations to evaluate at once\n            (Simply the inner batch size). Defaults to 4.\n        verbose (bool, optional): Whether to show the progress bar. Defaults to False.\n        segmentation_method (Literal[\"slic\", \"patch\"], optional):\n            Segmentation method used only if `segmentation_mask` is None. Defaults to \"slic\".\n        additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n            the inputs for which attributions should not be computed, this argument can be provided.\n            It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n            containing multiple additional arguments including tensors or any arbitrary python types.\n            These arguments are provided to forward_func in order following the arguments in inputs.\n            Note that attributions are not computed with respect to these arguments. Default: None\n        band_names (list[str] | dict[str | tuple[str, ...], int] | None, optional): Band names. Defaults to None.\n\n    Returns:\n        HSISpectralAttributes | list[HSISpectralAttributes]: An object containing the image, the attributions,\n            the band mask, the band names, and the score of the interpretable model used for the explanation.\n\n    Raises:\n        RuntimeError: If the Lime object is not initialized or is not an instance of LimeBase.\n        MaskCreationError: If there is an error creating the band mask.\n        ValueError: If the number of band masks is not equal to the number of HSI images provided.\n        HSIAttributesError: If there is an error during creating spectral attribution.\n\n    Examples:\n        &gt;&gt;&gt; simple_model = lambda x: torch.rand((x.shape[0], 2))\n        &gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n        &gt;&gt;&gt; band_mask = torch.randint(1, 4, (4, 1, 1)).repeat(1, 240, 240)\n        &gt;&gt;&gt; band_names = [\"R\", \"G\", \"B\"]\n        &gt;&gt;&gt; lime = meteors.attr.Lime(\n                explainable_model=ExplainableModel(simple_model, \"regression\"), interpretable_model=SkLearnLasso(alpha=0.1)\n            )\n        &gt;&gt;&gt; spectral_attribution = lime.get_spectral_attributes(hsi, band_mask=band_mask, band_names=band_names, target=0)\n        &gt;&gt;&gt; spectral_attribution.hsi\n        HSI(shape=(4, 240, 240), dtype=torch.float32)\n        &gt;&gt;&gt; spectral_attribution.attributes.shape\n        torch.Size([4, 240, 240])\n        &gt;&gt;&gt; spectral_attribution.band_mask.shape\n        torch.Size([4, 240, 240])\n        &gt;&gt;&gt; spectral_attribution.band_names\n        [\"R\", \"G\", \"B\"]\n        &gt;&gt;&gt; spectral_attribution.score\n        1.0\n    \"\"\"\n\n    if self._attribution_method is None or not isinstance(self._attribution_method, LimeBase):\n        raise RuntimeError(\"Lime object not initialized\")  # pragma: no cover\n\n    if isinstance(hsi, HSI):\n        hsi = [hsi]\n\n    if not all(isinstance(hsi_image, HSI) for hsi_image in hsi):\n        raise TypeError(\"All of the input hyperspectral images must be of type HSI\")\n\n    if band_mask is None:\n        created_bands = [self.get_band_mask(hsi_img, band_names) for hsi_img in hsi]\n        band_mask, band_name_list = zip(*created_bands)\n        band_names = band_name_list[0]\n\n    if isinstance(band_mask, tuple):\n        band_mask = list(band_mask)\n    elif not isinstance(band_mask, list):\n        band_mask = [band_mask]\n\n    if len(hsi) != len(band_mask):\n        if len(band_mask) == 1:\n            band_mask = band_mask * len(hsi)\n            logger.info(\"Reusing the same band mask for all images\")\n        else:\n            raise ValueError(\n                f\"Number of band masks should be equal to the number of HSI images provided, provided {len(band_mask)}\"\n            )\n\n    band_mask = [\n        ensure_torch_tensor(mask, f\"Band mask number {idx+1} should be None, numpy array, or torch tensor\")\n        for idx, mask in enumerate(band_mask)\n    ]\n    band_mask = [\n        mask.unsqueeze(-1).unsqueeze(-1).moveaxis(0, hsi_img.spectral_axis)\n        if mask.ndim != hsi_img.image.ndim\n        else mask\n        for hsi_img, mask in zip(hsi, band_mask)\n    ]\n    band_mask = [validate_mask_shape(\"band\", hsi_img, mask) for hsi_img, mask in zip(hsi, band_mask)]\n\n    hsi_input = torch.stack([hsi_img.get_image() for hsi_img in hsi], dim=0)\n    band_mask = torch.stack(band_mask, dim=0)\n\n    if band_names is None:\n        band_names = {str(segment): idx for idx, segment in enumerate(torch.unique(band_mask))}\n    else:\n        logger.debug(\n            \"Band names are provided and will be used. In the future, there should be an option to validate them.\"\n        )\n\n    assert hsi_input.shape == band_mask.shape\n\n    hsi_input = hsi_input.to(self.device)\n    band_mask = band_mask.to(self.device)\n\n    lime_attributes, score = self._attribution_method.attribute(\n        inputs=hsi_input,\n        target=target,\n        feature_mask=band_mask,\n        n_samples=n_samples,\n        perturbations_per_eval=perturbations_per_eval,\n        additional_forward_args=additional_forward_args,\n        show_progress=verbose,\n        return_input_shape=True,\n    )\n\n    try:\n        spectral_attribution = [\n            HSISpectralAttributes(\n                hsi=hsi_img,\n                attributes=lime_attr,\n                mask=band_mask[idx].expand_as(hsi_img.image),\n                band_names=band_names,\n                score=score.item(),\n                attribution_method=\"Lime\",\n            )\n            for idx, (hsi_img, lime_attr) in enumerate(zip(hsi, lime_attributes))\n        ]\n    except Exception as e:\n        raise HSIAttributesError(f\"Error during creating spectral attribution {e}\") from e\n\n    return spectral_attribution[0] if len(spectral_attribution) == 1 else spectral_attribution\n</code></pre>"},{"location":"reference/#lime-base","title":"Lime Base","text":"<p>The Lime Base class was adapted from the Captum Lime implementation. This adaptation builds upon the original work, extending and customizing it for specific use cases within this project. To see the original implementation, please refer to the Captum repository.</p>"},{"location":"reference/#src.meteors.attr.integrated_gradients.IntegratedGradients","title":"<code>IntegratedGradients</code>","text":"<p>               Bases: <code>Explainer</code></p> <p>IntegratedGradients explainer class for generating attributions using the Integrated Gradients method. The Integrated Gradients method is based on the <code>captum</code> implementation and is an implementation of an idea coming from the original paper on Integrated Gradients, where more details about this method can be found.</p> <p>Attributes:</p> Name Type Description <code>_attribution_method</code> <code>IntegratedGradients</code> <p>The Integrated Gradients method from the <code>captum</code> library.</p> <p>Parameters:</p> Name Type Description Default <code>explainable_model</code> <code>ExplainableModel | Explainer</code> <p>The explainable model to be explained.</p> required <code>postprocessing_segmentation_output</code> <code>Callable[[Tensor], Tensor] | None</code> <p>A segmentation postprocessing function for segmentation problem type. This is required for segmentation problem type as attribution methods needs to have 1d output. Defaults to None, which means that the attribution method is not used.</p> <code>None</code> Source code in <code>src/meteors/attr/integrated_gradients.py</code> <pre><code>class IntegratedGradients(Explainer):\n    \"\"\"\n    IntegratedGradients explainer class for generating attributions using the Integrated Gradients method.\n    The Integrated Gradients method is based on the [`captum` implementation](https://captum.ai/docs/extension/integrated_gradients)\n    and is an implementation of an idea coming from the [original paper on Integrated Gradients](https://arxiv.org/pdf/1703.01365),\n    where more details about this method can be found.\n\n    Attributes:\n        _attribution_method (CaptumIntegratedGradients): The Integrated Gradients method from the `captum` library.\n\n    Args:\n        explainable_model (ExplainableModel | Explainer): The explainable model to be explained.\n        postprocessing_segmentation_output (Callable[[torch.Tensor], torch.Tensor] | None):\n            A segmentation postprocessing function for segmentation problem type. This is required for segmentation\n            problem type as attribution methods needs to have 1d output. Defaults to None, which means that the\n            attribution method is not used.\n    \"\"\"\n\n    def __init__(\n        self,\n        explainable_model: ExplainableModel,\n        postprocessing_segmentation_output: Callable[[torch.Tensor], torch.Tensor] | None = None,\n        multiply_by_inputs: bool = True,\n    ):\n        super().__init__(explainable_model, postprocessing_segmentation_output=postprocessing_segmentation_output)\n        self.multiply_by_inputs = multiply_by_inputs\n\n        self._attribution_method = CaptumIntegratedGradients(\n            explainable_model.forward_func, multiply_by_inputs=self.multiply_by_inputs\n        )\n\n    def attribute(\n        self,\n        hsi: list[HSI] | HSI,\n        baseline: int | float | torch.Tensor | list[int | float | torch.Tensor] = None,\n        target: list[int] | int | None = None,\n        additional_forward_args: Any = None,\n        method: Literal[\n            \"riemann_right\", \"riemann_left\", \"riemann_middle\", \"riemann_trapezoid\", \"gausslegendre\"\n        ] = \"gausslegendre\",\n        return_convergence_delta: bool = False,\n    ) -&gt; HSIAttributes | list[HSIAttributes]:\n        \"\"\"\n        Method for generating attributions using the Integrated Gradients method.\n\n        Args:\n            hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n                If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n                The output will be a list of HSIAttributes objects.\n            baseline (int | float | torch.Tensor | list[int | float | torch.Tensor, optional): Baselines define the\n                starting point from which integral is computed and can be provided as:\n                    - integer or float representing a constant value used as the baseline for all input pixels.\n                    - tensor with the same shape as the input tensor, providing a baseline for each input pixel.\n                        if the input is a list of HSI objects, the baseline can be a tensor with the same shape as\n                        the input tensor for each HSI object.\n                    - list of integers, floats or tensors with the same shape as the input tensor, providing a baseline\n                        for each input pixel. If the input is a list of HSI objects, the baseline can be a list of\n                        tensors with the same shape as the input tensor for each HSI object. Defaults to None.\n            target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n                methods assume that the output has only one class. If the output has multiple classes, the target index\n                must be provided. For multiple input images, a list of target indices can be provided, one for each\n                image or single target value will be used for all images. Defaults to None.\n            additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n                the inputs for which attributions should not be computed, this argument can be provided.\n                It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n                containing multiple additional arguments including tensors or any arbitrary python types.\n                These arguments are provided to forward_func in order following the arguments in inputs.\n                Note that attributions are not computed with respect to these arguments. Default: None\n            method (Literal[\"riemann_right\", \"riemann_left\", \"riemann_middle\", \"riemann_trapezoid\", \"gausslegendre\"],\n                optional): Method for approximating the integral, one of riemann_right, riemann_left, riemann_middle,\n                riemann_trapezoid or gausslegendre. Default: gausslegendre if no method is provided.\n            return_convergence_delta (bool, optional): Indicates whether to return convergence delta or not.\n                If return_convergence_delta is set to True convergence delta will be returned in a tuple following\n                attributions. Default: False\n\n        Returns:\n            HSIAttributes | list[HSIAttributes]: The computed attributions for the input hyperspectral image(s).\n                if a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n\n        Raises:\n            RuntimeError: If the explainer is not initialized.\n            HSIAttributesError: If an error occurs during the generation of the attributions.\n\n\n        Examples:\n            &gt;&gt;&gt; integrated_gradients = IntegratedGradients(explainable_model)\n            &gt;&gt;&gt; hsi = HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n            &gt;&gt;&gt; attributions = integrated_gradients.attribute(hsi, method=\"riemann_right\", baseline=0.0)\n            &gt;&gt;&gt; attributions, approximation_error = integrated_gradients.attribute(hsi, return_convergence_delta=True)\n            &gt;&gt;&gt; approximation_error\n            0.5\n            &gt;&gt;&gt; attributions = integrated_gradients.attribute([hsi, hsi])\n            &gt;&gt;&gt; len(attributions)\n            2\n        \"\"\"\n        if self._attribution_method is None:\n            raise RuntimeError(\"IntegratedGradients explainer is not initialized, INITIALIZATION ERROR\")\n\n        if not isinstance(hsi, list):\n            hsi = [hsi]\n\n        if not all(isinstance(hsi_image, HSI) for hsi_image in hsi):\n            raise TypeError(\"All of the input hyperspectral images must be of type HSI\")\n\n        if not isinstance(baseline, list):\n            baseline = [baseline] * len(hsi)\n\n        baseline = torch.stack(\n            [\n                validate_and_transform_baseline(base, hsi_image).to(hsi_image.device)\n                for hsi_image, base in zip(hsi, baseline)\n            ],\n            dim=0,\n        )\n        input_tensor = torch.stack(\n            [hsi_image.get_image().requires_grad_(True).to(hsi_image.device) for hsi_image in hsi], dim=0\n        )\n\n        ig_attributions = self._attribution_method.attribute(\n            input_tensor,\n            baselines=baseline,\n            target=target,\n            additional_forward_args=additional_forward_args,\n            method=method,\n            return_convergence_delta=return_convergence_delta,\n        )\n\n        if return_convergence_delta:\n            attributions, approximation_error = ig_attributions\n        else:\n            attributions, approximation_error = ig_attributions, [None] * len(hsi)\n\n        try:\n            attributes = [\n                HSIAttributes(hsi=hsi_image, attributes=attribution, score=error, attribution_method=self.get_name())\n                for hsi_image, attribution, error in zip(hsi, attributions, approximation_error)\n            ]\n        except Exception as e:\n            raise HSIAttributesError(f\"Error while creating HSIAttributes: {e}\") from e\n\n        return attributes[0] if len(attributes) == 1 else attributes\n</code></pre>"},{"location":"reference/#src.meteors.attr.integrated_gradients.IntegratedGradients.attribute","title":"<code>attribute(hsi, baseline=None, target=None, additional_forward_args=None, method='gausslegendre', return_convergence_delta=False)</code>","text":"<p>Method for generating attributions using the Integrated Gradients method.</p> <p>Parameters:</p> Name Type Description Default <code>hsi</code> <code>list[HSI] | HSI</code> <p>Input hyperspectral image(s) for which the attributions are to be computed. If a list of HSI objects is provided, the attributions are computed for each HSI object in the list. The output will be a list of HSIAttributes objects.</p> required <code>baseline</code> <code>int | float | torch.Tensor | list[int | float | torch.Tensor</code> <p>Baselines define the starting point from which integral is computed and can be provided as:     - integer or float representing a constant value used as the baseline for all input pixels.     - tensor with the same shape as the input tensor, providing a baseline for each input pixel.         if the input is a list of HSI objects, the baseline can be a tensor with the same shape as         the input tensor for each HSI object.     - list of integers, floats or tensors with the same shape as the input tensor, providing a baseline         for each input pixel. If the input is a list of HSI objects, the baseline can be a list of         tensors with the same shape as the input tensor for each HSI object. Defaults to None.</p> <code>None</code> <code>target</code> <code>list[int] | int | None</code> <p>target class index for computing the attributions. If None, methods assume that the output has only one class. If the output has multiple classes, the target index must be provided. For multiple input images, a list of target indices can be provided, one for each image or single target value will be used for all images. Defaults to None.</p> <code>None</code> <code>additional_forward_args</code> <code>Any</code> <p>If the forward function requires additional arguments other than the inputs for which attributions should not be computed, this argument can be provided. It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple containing multiple additional arguments including tensors or any arbitrary python types. These arguments are provided to forward_func in order following the arguments in inputs. Note that attributions are not computed with respect to these arguments. Default: None</p> <code>None</code> <code>return_convergence_delta</code> <code>bool</code> <p>Indicates whether to return convergence delta or not. If return_convergence_delta is set to True convergence delta will be returned in a tuple following attributions. Default: False</p> <code>False</code> <p>Returns:</p> Type Description <code>HSIAttributes | list[HSIAttributes]</code> <p>HSIAttributes | list[HSIAttributes]: The computed attributions for the input hyperspectral image(s). if a list of HSI objects is provided, the attributions are computed for each HSI object in the list.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the explainer is not initialized.</p> <code>HSIAttributesError</code> <p>If an error occurs during the generation of the attributions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; integrated_gradients = IntegratedGradients(explainable_model)\n&gt;&gt;&gt; hsi = HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n&gt;&gt;&gt; attributions = integrated_gradients.attribute(hsi, method=\"riemann_right\", baseline=0.0)\n&gt;&gt;&gt; attributions, approximation_error = integrated_gradients.attribute(hsi, return_convergence_delta=True)\n&gt;&gt;&gt; approximation_error\n0.5\n&gt;&gt;&gt; attributions = integrated_gradients.attribute([hsi, hsi])\n&gt;&gt;&gt; len(attributions)\n2\n</code></pre> Source code in <code>src/meteors/attr/integrated_gradients.py</code> <pre><code>def attribute(\n    self,\n    hsi: list[HSI] | HSI,\n    baseline: int | float | torch.Tensor | list[int | float | torch.Tensor] = None,\n    target: list[int] | int | None = None,\n    additional_forward_args: Any = None,\n    method: Literal[\n        \"riemann_right\", \"riemann_left\", \"riemann_middle\", \"riemann_trapezoid\", \"gausslegendre\"\n    ] = \"gausslegendre\",\n    return_convergence_delta: bool = False,\n) -&gt; HSIAttributes | list[HSIAttributes]:\n    \"\"\"\n    Method for generating attributions using the Integrated Gradients method.\n\n    Args:\n        hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n            If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n            The output will be a list of HSIAttributes objects.\n        baseline (int | float | torch.Tensor | list[int | float | torch.Tensor, optional): Baselines define the\n            starting point from which integral is computed and can be provided as:\n                - integer or float representing a constant value used as the baseline for all input pixels.\n                - tensor with the same shape as the input tensor, providing a baseline for each input pixel.\n                    if the input is a list of HSI objects, the baseline can be a tensor with the same shape as\n                    the input tensor for each HSI object.\n                - list of integers, floats or tensors with the same shape as the input tensor, providing a baseline\n                    for each input pixel. If the input is a list of HSI objects, the baseline can be a list of\n                    tensors with the same shape as the input tensor for each HSI object. Defaults to None.\n        target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n            methods assume that the output has only one class. If the output has multiple classes, the target index\n            must be provided. For multiple input images, a list of target indices can be provided, one for each\n            image or single target value will be used for all images. Defaults to None.\n        additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n            the inputs for which attributions should not be computed, this argument can be provided.\n            It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n            containing multiple additional arguments including tensors or any arbitrary python types.\n            These arguments are provided to forward_func in order following the arguments in inputs.\n            Note that attributions are not computed with respect to these arguments. Default: None\n        method (Literal[\"riemann_right\", \"riemann_left\", \"riemann_middle\", \"riemann_trapezoid\", \"gausslegendre\"],\n            optional): Method for approximating the integral, one of riemann_right, riemann_left, riemann_middle,\n            riemann_trapezoid or gausslegendre. Default: gausslegendre if no method is provided.\n        return_convergence_delta (bool, optional): Indicates whether to return convergence delta or not.\n            If return_convergence_delta is set to True convergence delta will be returned in a tuple following\n            attributions. Default: False\n\n    Returns:\n        HSIAttributes | list[HSIAttributes]: The computed attributions for the input hyperspectral image(s).\n            if a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n\n    Raises:\n        RuntimeError: If the explainer is not initialized.\n        HSIAttributesError: If an error occurs during the generation of the attributions.\n\n\n    Examples:\n        &gt;&gt;&gt; integrated_gradients = IntegratedGradients(explainable_model)\n        &gt;&gt;&gt; hsi = HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n        &gt;&gt;&gt; attributions = integrated_gradients.attribute(hsi, method=\"riemann_right\", baseline=0.0)\n        &gt;&gt;&gt; attributions, approximation_error = integrated_gradients.attribute(hsi, return_convergence_delta=True)\n        &gt;&gt;&gt; approximation_error\n        0.5\n        &gt;&gt;&gt; attributions = integrated_gradients.attribute([hsi, hsi])\n        &gt;&gt;&gt; len(attributions)\n        2\n    \"\"\"\n    if self._attribution_method is None:\n        raise RuntimeError(\"IntegratedGradients explainer is not initialized, INITIALIZATION ERROR\")\n\n    if not isinstance(hsi, list):\n        hsi = [hsi]\n\n    if not all(isinstance(hsi_image, HSI) for hsi_image in hsi):\n        raise TypeError(\"All of the input hyperspectral images must be of type HSI\")\n\n    if not isinstance(baseline, list):\n        baseline = [baseline] * len(hsi)\n\n    baseline = torch.stack(\n        [\n            validate_and_transform_baseline(base, hsi_image).to(hsi_image.device)\n            for hsi_image, base in zip(hsi, baseline)\n        ],\n        dim=0,\n    )\n    input_tensor = torch.stack(\n        [hsi_image.get_image().requires_grad_(True).to(hsi_image.device) for hsi_image in hsi], dim=0\n    )\n\n    ig_attributions = self._attribution_method.attribute(\n        input_tensor,\n        baselines=baseline,\n        target=target,\n        additional_forward_args=additional_forward_args,\n        method=method,\n        return_convergence_delta=return_convergence_delta,\n    )\n\n    if return_convergence_delta:\n        attributions, approximation_error = ig_attributions\n    else:\n        attributions, approximation_error = ig_attributions, [None] * len(hsi)\n\n    try:\n        attributes = [\n            HSIAttributes(hsi=hsi_image, attributes=attribution, score=error, attribution_method=self.get_name())\n            for hsi_image, attribution, error in zip(hsi, attributions, approximation_error)\n        ]\n    except Exception as e:\n        raise HSIAttributesError(f\"Error while creating HSIAttributes: {e}\") from e\n\n    return attributes[0] if len(attributes) == 1 else attributes\n</code></pre>"},{"location":"reference/#src.meteors.attr.input_x_gradients.InputXGradient","title":"<code>InputXGradient</code>","text":"<p>               Bases: <code>Explainer</code></p> <p>Initializes the InputXGradient explainer. The InputXGradients method is a straightforward approach to computing attribution. It simply multiplies the input image with the gradient with respect to the input. This method is based on the <code>captum</code> implementation</p> <p>Attributes:</p> Name Type Description <code>_attribution_method</code> <code>CaptumIntegratedGradients</code> <p>The InputXGradient method from the <code>captum</code> library.</p> <p>Parameters:</p> Name Type Description Default <code>explainable_model</code> <code>ExplainableModel | Explainer</code> <p>The explainable model to be explained.</p> required <code>postprocessing_segmentation_output</code> <code>Callable[[Tensor], Tensor] | None</code> <p>A segmentation postprocessing function for segmentation problem type. This is required for segmentation problem type as attribution methods needs to have 1d output. Defaults to None, which means that the attribution method is not used.</p> <code>None</code> Source code in <code>src/meteors/attr/input_x_gradients.py</code> <pre><code>class InputXGradient(Explainer):\n    \"\"\"\n    Initializes the InputXGradient explainer. The InputXGradients method is a straightforward approach to\n    computing attribution. It simply multiplies the input image with the gradient with respect to the input.\n    This method is based on the [`captum` implementation](https://captum.ai/api/input_x_gradient.html)\n\n    Attributes:\n        _attribution_method (CaptumIntegratedGradients): The InputXGradient method from the `captum` library.\n\n    Args:\n        explainable_model (ExplainableModel | Explainer): The explainable model to be explained.\n        postprocessing_segmentation_output (Callable[[torch.Tensor], torch.Tensor] | None):\n            A segmentation postprocessing function for segmentation problem type. This is required for segmentation\n            problem type as attribution methods needs to have 1d output. Defaults to None, which means that the\n            attribution method is not used.\n    \"\"\"\n\n    def __init__(\n        self,\n        explainable_model: ExplainableModel,\n        postprocessing_segmentation_output: Callable[[torch.Tensor], torch.Tensor] | None = None,\n    ):\n        super().__init__(explainable_model, postprocessing_segmentation_output=postprocessing_segmentation_output)\n\n        self._attribution_method = CaptumInputXGradient(explainable_model.forward_func)\n\n    def attribute(\n        self,\n        hsi: list[HSI] | HSI,\n        target: list[int] | int | None = None,\n        additional_forward_args: Any = None,\n    ) -&gt; HSIAttributes | list[HSIAttributes]:\n        \"\"\"\n        Method for generating attributions using the InputXGradient method.\n\n        Args:\n            hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n                If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n                The output will be a list of HSIAttributes objects.\n            target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n                methods assume that the output has only one class. If the output has multiple classes, the target index\n                must be provided. For multiple input images, a list of target indices can be provided, one for each\n                image or single target value will be used for all images. Defaults to None.\n            additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n                the inputs for which attributions should not be computed, this argument can be provided.\n                It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n                containing multiple additional arguments including tensors or any arbitrary python types.\n                These arguments are provided to forward_func in order following the arguments in inputs.\n                Note that attributions are not computed with respect to these arguments. Default: None\n\n        Returns:\n            HSIAttributes | list[HSIAttributes]: The computed attributions for the input hyperspectral image(s).\n                if a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n\n        Raises:\n            RuntimeError: If the explainer is not initialized.\n            HSIAttributesError: If an error occurs during the generation of the attributions.\n\n        Examples:\n            &gt;&gt;&gt; input_x_gradient = InputXGradient(explainable_model)\n            &gt;&gt;&gt; hsi = HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n            &gt;&gt;&gt; attributions = input_x_gradient.attribute(hsi)\n            &gt;&gt;&gt; attributions = input_x_gradient.attribute([hsi, hsi])\n            &gt;&gt;&gt; len(attributions)\n            2\n        \"\"\"\n        if self._attribution_method is None:\n            raise RuntimeError(\"InputXGradient explainer is not initialized, INITIALIZATION ERROR\")\n\n        if not isinstance(hsi, list):\n            hsi = [hsi]\n\n        if not all(isinstance(hsi_image, HSI) for hsi_image in hsi):\n            raise TypeError(\"All of the input hyperspectral images must be of type HSI\")\n\n        input_tensor = torch.stack(\n            [hsi_image.get_image().requires_grad_(True).to(hsi_image.device) for hsi_image in hsi], dim=0\n        )\n\n        gradient_attribution = self._attribution_method.attribute(\n            input_tensor, target=target, additional_forward_args=additional_forward_args\n        )\n\n        try:\n            attributes = [\n                HSIAttributes(hsi=hsi_image, attributes=attribution, attribution_method=self.get_name())\n                for hsi_image, attribution in zip(hsi, gradient_attribution)\n            ]\n        except Exception as e:\n            raise HSIAttributesError(f\"Error in generating InputXGradient attributions: {e}\") from e\n\n        return attributes[0] if len(attributes) == 1 else attributes\n</code></pre>"},{"location":"reference/#src.meteors.attr.input_x_gradients.InputXGradient.attribute","title":"<code>attribute(hsi, target=None, additional_forward_args=None)</code>","text":"<p>Method for generating attributions using the InputXGradient method.</p> <p>Parameters:</p> Name Type Description Default <code>hsi</code> <code>list[HSI] | HSI</code> <p>Input hyperspectral image(s) for which the attributions are to be computed. If a list of HSI objects is provided, the attributions are computed for each HSI object in the list. The output will be a list of HSIAttributes objects.</p> required <code>target</code> <code>list[int] | int | None</code> <p>target class index for computing the attributions. If None, methods assume that the output has only one class. If the output has multiple classes, the target index must be provided. For multiple input images, a list of target indices can be provided, one for each image or single target value will be used for all images. Defaults to None.</p> <code>None</code> <code>additional_forward_args</code> <code>Any</code> <p>If the forward function requires additional arguments other than the inputs for which attributions should not be computed, this argument can be provided. It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple containing multiple additional arguments including tensors or any arbitrary python types. These arguments are provided to forward_func in order following the arguments in inputs. Note that attributions are not computed with respect to these arguments. Default: None</p> <code>None</code> <p>Returns:</p> Type Description <code>HSIAttributes | list[HSIAttributes]</code> <p>HSIAttributes | list[HSIAttributes]: The computed attributions for the input hyperspectral image(s). if a list of HSI objects is provided, the attributions are computed for each HSI object in the list.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the explainer is not initialized.</p> <code>HSIAttributesError</code> <p>If an error occurs during the generation of the attributions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; input_x_gradient = InputXGradient(explainable_model)\n&gt;&gt;&gt; hsi = HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n&gt;&gt;&gt; attributions = input_x_gradient.attribute(hsi)\n&gt;&gt;&gt; attributions = input_x_gradient.attribute([hsi, hsi])\n&gt;&gt;&gt; len(attributions)\n2\n</code></pre> Source code in <code>src/meteors/attr/input_x_gradients.py</code> <pre><code>def attribute(\n    self,\n    hsi: list[HSI] | HSI,\n    target: list[int] | int | None = None,\n    additional_forward_args: Any = None,\n) -&gt; HSIAttributes | list[HSIAttributes]:\n    \"\"\"\n    Method for generating attributions using the InputXGradient method.\n\n    Args:\n        hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n            If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n            The output will be a list of HSIAttributes objects.\n        target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n            methods assume that the output has only one class. If the output has multiple classes, the target index\n            must be provided. For multiple input images, a list of target indices can be provided, one for each\n            image or single target value will be used for all images. Defaults to None.\n        additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n            the inputs for which attributions should not be computed, this argument can be provided.\n            It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n            containing multiple additional arguments including tensors or any arbitrary python types.\n            These arguments are provided to forward_func in order following the arguments in inputs.\n            Note that attributions are not computed with respect to these arguments. Default: None\n\n    Returns:\n        HSIAttributes | list[HSIAttributes]: The computed attributions for the input hyperspectral image(s).\n            if a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n\n    Raises:\n        RuntimeError: If the explainer is not initialized.\n        HSIAttributesError: If an error occurs during the generation of the attributions.\n\n    Examples:\n        &gt;&gt;&gt; input_x_gradient = InputXGradient(explainable_model)\n        &gt;&gt;&gt; hsi = HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n        &gt;&gt;&gt; attributions = input_x_gradient.attribute(hsi)\n        &gt;&gt;&gt; attributions = input_x_gradient.attribute([hsi, hsi])\n        &gt;&gt;&gt; len(attributions)\n        2\n    \"\"\"\n    if self._attribution_method is None:\n        raise RuntimeError(\"InputXGradient explainer is not initialized, INITIALIZATION ERROR\")\n\n    if not isinstance(hsi, list):\n        hsi = [hsi]\n\n    if not all(isinstance(hsi_image, HSI) for hsi_image in hsi):\n        raise TypeError(\"All of the input hyperspectral images must be of type HSI\")\n\n    input_tensor = torch.stack(\n        [hsi_image.get_image().requires_grad_(True).to(hsi_image.device) for hsi_image in hsi], dim=0\n    )\n\n    gradient_attribution = self._attribution_method.attribute(\n        input_tensor, target=target, additional_forward_args=additional_forward_args\n    )\n\n    try:\n        attributes = [\n            HSIAttributes(hsi=hsi_image, attributes=attribution, attribution_method=self.get_name())\n            for hsi_image, attribution in zip(hsi, gradient_attribution)\n        ]\n    except Exception as e:\n        raise HSIAttributesError(f\"Error in generating InputXGradient attributions: {e}\") from e\n\n    return attributes[0] if len(attributes) == 1 else attributes\n</code></pre>"},{"location":"reference/#src.meteors.attr.occlusion.Occlusion","title":"<code>Occlusion</code>","text":"<p>               Bases: <code>Explainer</code></p> <p>Occlusion explainer class for generating attributions using the Occlusion method. This attribution method perturbs the input by replacing the contiguous rectangular region with a given baseline and computing the difference in output. In our case, features are located in multiple regions, and attribution from different hyper-rectangles is averaged. The implementation of this method is also based on the <code>captum</code> repository. More details about this approach can be found in the original paper</p> <p>Attributes:</p> Name Type Description <code>_attribution_method</code> <code>Occlusion</code> <p>The Occlusion method from the <code>captum</code> library.</p> <p>Parameters:</p> Name Type Description Default <code>explainable_model</code> <code>ExplainableModel | Explainer</code> <p>The explainable model to be explained.</p> required <code>postprocessing_segmentation_output</code> <code>Callable[[Tensor], Tensor] | None</code> <p>A segmentation postprocessing function for segmentation problem type. This is required for segmentation problem type as attribution methods needs to have 1d output. Defaults to None, which means that the attribution method is not used.</p> <code>None</code> Source code in <code>src/meteors/attr/occlusion.py</code> <pre><code>class Occlusion(Explainer):\n    \"\"\"\n    Occlusion explainer class for generating attributions using the Occlusion method.\n    This attribution method perturbs the input by replacing the contiguous rectangular region\n    with a given baseline and computing the difference in output.\n    In our case, features are located in multiple regions, and attribution from different hyper-rectangles is averaged.\n    The implementation of this method is also based on the [`captum` repository](https://captum.ai/api/occlusion.html).\n    More details about this approach can be found in the [original paper](https://arxiv.org/abs/1311.2901)\n\n    Attributes:\n        _attribution_method (CaptumOcclusion): The Occlusion method from the `captum` library.\n\n    Args:\n        explainable_model (ExplainableModel | Explainer): The explainable model to be explained.\n        postprocessing_segmentation_output (Callable[[torch.Tensor], torch.Tensor] | None):\n            A segmentation postprocessing function for segmentation problem type. This is required for segmentation\n            problem type as attribution methods needs to have 1d output. Defaults to None, which means that the\n            attribution method is not used.\n    \"\"\"\n\n    def __init__(\n        self,\n        explainable_model: ExplainableModel,\n        postprocessing_segmentation_output: Callable[[torch.Tensor], torch.Tensor] | None = None,\n    ):\n        super().__init__(explainable_model, postprocessing_segmentation_output=postprocessing_segmentation_output)\n\n        self._attribution_method = CaptumOcclusion(explainable_model.forward_func)\n\n    def attribute(\n        self,\n        hsi: list[HSI] | HSI,\n        target: list[int] | int | None = None,\n        sliding_window_shapes: int | tuple[int, int, int] = (1, 1, 1),\n        strides: int | tuple[int, int, int] = (1, 1, 1),\n        baseline: int | float | torch.Tensor | list[int | float | torch.Tensor] = None,\n        additional_forward_args: Any = None,\n        perturbations_per_eval: int = 1,\n        show_progress: bool = False,\n    ) -&gt; HSIAttributes | list[HSIAttributes]:\n        \"\"\"\n        Method for generating attributions using the Occlusion method.\n\n        Args:\n            hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n                If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n                The output will be a list of HSIAttributes objects.\n            target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n                methods assume that the output has only one class. If the output has multiple classes, the target index\n                must be provided. For multiple input images, a list of target indices can be provided, one for each\n                image or single target value will be used for all images. Defaults to None.\n            sliding_window_shapes (int | tuple[int, int, int]):\n                The shape of the sliding window. If an integer is provided, it will be used for all dimensions.\n                Defaults to (1, 1, 1).\n            strides (int | tuple[int, int, int], optional): The stride of the sliding window. Defaults to (1, 1, 1).\n                Simply put, the stride is the number of pixels by which the sliding window is moved in each dimension.\n            baseline (int | float | torch.Tensor | list[int | float | torch.Tensor], optional): Baselines define\n                reference value which replaces each feature when occluded is computed and can be provided as:\n                    - integer or float representing a constant value used as the baseline for all input pixels.\n                    - tensor with the same shape as the input tensor, providing a baseline for each input pixel.\n                        if the input is a list of HSI objects, the baseline can be a tensor with the same shape as\n                        the input tensor for each HSI object.\n                    - list of integers, floats or tensors with the same shape as the input tensor, providing a baseline\n                        for each input pixel. If the input is a list of HSI objects, the baseline can be a list of\n                        tensors with the same shape as the input tensor for each HSI object. Defaults to None.\n            additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n                the inputs for which attributions should not be computed, this argument can be provided.\n                It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n                containing multiple additional arguments including tensors or any arbitrary python types.\n                These arguments are provided to forward_func in order following the arguments in inputs.\n                Note that attributions are not computed with respect to these arguments. Default: None\n            perturbations_per_eval (int, optional): Allows multiple occlusions to be included in one batch\n                (one call to forward_fn). By default, perturbations_per_eval is 1, so each occlusion is processed\n                individually. Each forward pass will contain a maximum of perturbations_per_eval * #examples samples.\n                For DataParallel models, each batch is split among the available devices, so evaluations on each\n                available device contain at most (perturbations_per_eval * #examples) / num_devices samples. When\n                working with multiple examples, the number of perturbations per evaluation should be set to at least\n                the number of examples. Defaults to 1.\n            show_progress (bool, optional): If True, displays a progress bar. Defaults to False.\n\n        Returns:\n            HSIAttributes: An object containing the computed attributions.\n\n        Raises:\n            RuntimeError: If the explainer is not initialized.\n            ValueError: If the sliding window shapes or strides are not a tuple of three integers.\n            HSIAttributesError: If an error occurs during the generation of the attributions.\n\n        Example:\n            &gt;&gt;&gt; occlusion = Occlusion(explainable_model)\n            &gt;&gt;&gt; hsi = HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n            &gt;&gt;&gt; attributions = occlusion.attribute(hsi, baseline=0, sliding_window_shapes=(4, 3, 3), strides=(1, 1, 1))\n            &gt;&gt;&gt; attributions = occlusion.attribute([hsi, hsi], baseline=0, sliding_window_shapes=(4, 3, 3), strides=(1, 2, 2))\n            &gt;&gt;&gt; len(attributions)\n            2\n        \"\"\"\n        if self._attribution_method is None:\n            raise RuntimeError(\"Occlusion explainer is not initialized, INITIALIZATION ERROR\")\n\n        if not isinstance(hsi, list):\n            hsi = [hsi]\n\n        if not all(isinstance(hsi_image, HSI) for hsi_image in hsi):\n            raise TypeError(\"All of the input hyperspectral images must be of type HSI\")\n\n        if not isinstance(baseline, list):\n            baseline = [baseline] * len(hsi)\n\n        baseline = torch.stack(\n            [\n                validate_and_transform_baseline(base, hsi_image).to(hsi_image.device)\n                for hsi_image, base in zip(hsi, baseline)\n            ],\n            dim=0,\n        )\n        input_tensor = torch.stack(\n            [hsi_image.get_image().requires_grad_(True).to(hsi_image.device) for hsi_image in hsi], dim=0\n        )\n\n        if isinstance(sliding_window_shapes, int):\n            sliding_window_shapes = (sliding_window_shapes, sliding_window_shapes, sliding_window_shapes)\n        if isinstance(strides, int):\n            strides = (strides, strides, strides)\n\n        if len(strides) != 3:\n            raise ValueError(\"Strides must be a tuple of three integers\")\n        if len(sliding_window_shapes) != 3:\n            raise ValueError(\"Sliding window shapes must be a tuple of three integers\")\n\n        occlusion_attributions = self._attribution_method.attribute(\n            input_tensor,\n            sliding_window_shapes=sliding_window_shapes,\n            strides=strides,\n            target=target,\n            baselines=baseline,\n            additional_forward_args=additional_forward_args,\n            perturbations_per_eval=min(perturbations_per_eval, len(hsi)),\n            show_progress=show_progress,\n        )\n\n        try:\n            attributes = [\n                HSIAttributes(hsi=hsi_image, attributes=attribution, attribution_method=self.get_name())\n                for hsi_image, attribution in zip(hsi, occlusion_attributions)\n            ]\n        except Exception as e:\n            raise HSIAttributesError(f\"Error in generating Occlusion attributions: {e}\") from e\n\n        return attributes[0] if len(attributes) == 1 else attributes\n\n    def get_spatial_attributes(\n        self,\n        hsi: list[HSI] | HSI,\n        target: list[int] | int | None = None,\n        sliding_window_shapes: int | tuple[int, int] = (1, 1),\n        strides: int | tuple[int, int] = 1,\n        baseline: int | float | torch.Tensor | list[int | float | torch.Tensor] = None,\n        additional_forward_args: Any = None,\n        perturbations_per_eval: int = 1,\n        show_progress: bool = False,\n    ) -&gt; HSIAttributes | list[HSIAttributes]:\n        \"\"\"Compute spatial attributions for the input HSI using the Occlusion method. In this case, the sliding window\n        is applied to the spatial dimensions only.\n\n        Args:\n            hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n                If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n                The output will be a list of HSIAttributes objects.\n            target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n                methods assume that the output has only one class. If the output has multiple classes, the target index\n                must be provided. For multiple input images, a list of target indices can be provided, one for each\n                image or single target value will be used for all images. Defaults to None.\n            sliding_window_shapes (int | tuple[int, int]): The shape of the sliding window for spatial dimensions.\n                If an integer is provided, it will be used for both spatial dimensions. Defaults to (1, 1).\n            strides (int | tuple[int, int], optional): The stride of the sliding window for spatial dimensions.\n                Defaults to 1. Simply put, the stride is the number of pixels by which the sliding window is moved\n                in each spatial dimension.\n            baseline (int | float | torch.Tensor | list[int | float | torch.Tensor], optional): Baselines define\n                reference value which replaces each feature when occluded is computed and can be provided as:\n                    - integer or float representing a constant value used as the baseline for all input pixels.\n                    - tensor with the same shape as the input tensor, providing a baseline for each input pixel.\n                        if the input is a list of HSI objects, the baseline can be a tensor with the same shape as\n                        the input tensor for each HSI object.\n                    - list of integers, floats or tensors with the same shape as the input tensor, providing a baseline\n                      for each input pixel. If the input is a list of HSI objects, the baseline can be a list of\n                      tensors with the same shape as the input tensor for each HSI object. Defaults to None.\n            additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n                the inputs for which attributions should not be computed, this argument can be provided.\n                It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n                containing multiple additional arguments including tensors or any arbitrary python types.\n                These arguments are provided to forward_func in order following the arguments in inputs.\n                Note that attributions are not computed with respect to these arguments. Default: None\n            perturbations_per_eval (int, optional): Allows multiple occlusions to be included in one batch\n                (one call to forward_fn). By default, perturbations_per_eval is 1, so each occlusion is processed\n                individually. Each forward pass will contain a maximum of perturbations_per_eval * #examples samples.\n                For DataParallel models, each batch is split among the available devices, so evaluations on each\n                available device contain at most (perturbations_per_eval * #examples) / num_devices samples. When\n                working with multiple examples, the number of perturbations per evaluation should be set to at least\n                the number of examples. Defaults to 1.\n            show_progress (bool, optional): If True, displays a progress bar. Defaults to False.\n\n        Returns:\n            HSIAttributes: An object containing the computed spatial attributions.\n\n        Raises:\n            RuntimeError: If the explainer is not initialized.\n            ValueError: If the sliding window shapes or strides are not a tuple of two integers.\n            HSIAttributesError: If an error occurs during the generation of the attributions\n\n        Example:\n            &gt;&gt;&gt; occlusion = Occlusion(explainable_model)\n            &gt;&gt;&gt; hsi = HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n            &gt;&gt;&gt; attributions = occlusion.get_spatial_attributes(hsi, baseline=0, sliding_window_shapes=(3, 3), strides=(1, 1))\n            &gt;&gt;&gt; attributions = occlusion.get_spatial_attributes([hsi, hsi], baseline=0, sliding_window_shapes=(3, 3), strides=(2, 2))\n            &gt;&gt;&gt; len(attributions)\n            2\n        \"\"\"\n        if self._attribution_method is None:\n            raise RuntimeError(\"Occlusion explainer is not initialized, INITIALIZATION ERROR\")\n\n        if not isinstance(hsi, list):\n            hsi = [hsi]\n\n        if not all(isinstance(hsi_image, HSI) for hsi_image in hsi):\n            raise TypeError(\"All of the input hyperspectral images must be of type HSI\")\n\n        if not isinstance(baseline, list):\n            baseline = [baseline] * len(hsi)\n\n        baseline = torch.stack(\n            [\n                validate_and_transform_baseline(base, hsi_image).to(hsi_image.device)\n                for hsi_image, base in zip(hsi, baseline)\n            ],\n            dim=0,\n        )\n        input_tensor = torch.stack(\n            [hsi_image.get_image().requires_grad_(True).to(hsi_image.device) for hsi_image in hsi], dim=0\n        )\n\n        if isinstance(sliding_window_shapes, int):\n            sliding_window_shapes = (sliding_window_shapes, sliding_window_shapes)\n        if isinstance(strides, int):\n            strides = (strides, strides)\n\n        if len(strides) != 2:\n            raise ValueError(\"Strides must be a tuple of two integers\")\n        if len(sliding_window_shapes) != 2:\n            raise ValueError(\"Sliding window shapes must be a tuple of two integers\")\n\n        list_sliding_window_shapes = list(sliding_window_shapes)\n        list_strides = list(strides)\n        if isinstance(hsi, list):\n            list_sliding_window_shapes.insert(hsi[0].spectral_axis, hsi[0].image.shape[hsi[0].spectral_axis])\n            list_strides.insert(hsi[0].spectral_axis, hsi[0].image.shape[hsi[0].spectral_axis])\n        else:\n            list_sliding_window_shapes.insert(hsi.spectral_axis, hsi.image.shape[hsi.spectral_axis])\n            list_strides.insert(hsi.spectral_axis, hsi.image.shape[hsi.spectral_axis])\n        sliding_window_shapes = tuple(list_sliding_window_shapes)  # type: ignore\n        strides = tuple(list_strides)  # type: ignore\n\n        occlusion_attributions = self._attribution_method.attribute(\n            input_tensor,\n            sliding_window_shapes=sliding_window_shapes,\n            strides=strides,\n            target=target,\n            baselines=baseline,\n            additional_forward_args=additional_forward_args,\n            perturbations_per_eval=min(perturbations_per_eval, len(hsi)),\n            show_progress=show_progress,\n        )\n\n        try:\n            spatial_attributes = [\n                HSIAttributes(hsi=hsi_image, attributes=attribution, attribution_method=self.get_name())\n                for hsi_image, attribution in zip(hsi, occlusion_attributions)\n            ]\n        except Exception as e:\n            raise HSIAttributesError(f\"Error in generating Occlusion attributions: {e}\") from e\n\n        return spatial_attributes[0] if len(spatial_attributes) == 1 else spatial_attributes\n\n    def get_spectral_attributes(\n        self,\n        hsi: list[HSI] | HSI,\n        target: list[int] | int | None = None,\n        sliding_window_shapes: int | tuple[int] = 1,\n        strides: int | tuple[int] = 1,\n        baseline: int | float | torch.Tensor | list[int | float | torch.Tensor] = None,\n        additional_forward_args: Any = None,\n        perturbations_per_eval: int = 1,\n        show_progress: bool = False,\n    ) -&gt; HSIAttributes | list[HSIAttributes]:\n        \"\"\"Compute spectral attributions for the input HSI using the Occlusion method. In this case, the sliding window\n        is applied to the spectral dimension only.\n\n        Args:\n            hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n                If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n                The output will be a list of HSIAttributes objects.\n            target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n                methods assume that the output has only one class. If the output has multiple classes, the target index\n                must be provided. For multiple input images, a list of target indices can be provided, one for each\n                image or single target value will be used for all images. Defaults to None.\n            sliding_window_shapes (int | tuple[int]): The size of the sliding window for the spectral dimension.\n                Defaults to 1.\n            strides (int | tuple[int], optional): The stride of the sliding window for the spectral dimension.\n                Defaults to 1. Simply put, the stride is the number of pixels by which the sliding window is moved\n                in spectral dimension.\n            baseline (int | float | torch.Tensor | list[int | float | torch.Tensor], optional): Baselines define\n                reference value which replaces each feature when occluded is computed and can be provided as:\n                    - integer or float representing a constant value used as the baseline for all input pixels.\n                    - tensor with the same shape as the input tensor, providing a baseline for each input pixel.\n                        if the input is a list of HSI objects, the baseline can be a tensor with the same shape as\n                        the input tensor for each HSI object.\n                    - list of integers, floats or tensors with the same shape as the input tensor, providing a baseline\n                      for each input pixel. If the input is a list of HSI objects, the baseline can be a list of\n                      tensors with the same shape as the input tensor for each HSI object. Defaults to None.\n            additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n                the inputs for which attributions should not be computed, this argument can be provided.\n                It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n                containing multiple additional arguments including tensors or any arbitrary python types.\n                These arguments are provided to forward_func in order following the arguments in inputs.\n                Note that attributions are not computed with respect to these arguments. Default: None\n            perturbations_per_eval (int, optional): Allows multiple occlusions to be included in one batch\n                (one call to forward_fn). By default, perturbations_per_eval is 1, so each occlusion is processed\n                individually. Each forward pass will contain a maximum of perturbations_per_eval * #examples samples.\n                For DataParallel models, each batch is split among the available devices, so evaluations on each\n                available device contain at most (perturbations_per_eval * #examples) / num_devices samples. When\n                working with multiple examples, the number of perturbations per evaluation should be set to at least\n                the number of examples. Defaults to 1.\n            show_progress (bool, optional): If True, displays a progress bar. Defaults to False.\n\n        Returns:\n            HSIAttributes: An object containing the computed spectral attributions.\n\n        Raises:\n            RuntimeError: If the explainer is not initialized.\n            ValueError: If the sliding window shapes or strides are not a tuple of a single integer.\n            TypeError: If the sliding window shapes or strides are not a single integer.\n            HSIAttributesError: If an error occurs during the generation of the attributions\n\n        Example:\n            &gt;&gt;&gt; occlusion = Occlusion(explainable_model)\n            &gt;&gt;&gt; hsi = HSI(image=torch.ones((10, 240, 240)), wavelengths=torch.arange(10))\n            &gt;&gt;&gt; attributions = occlusion.get_spectral_attributes(hsi, baseline=0, sliding_window_shapes=3, strides=1)\n            &gt;&gt;&gt; attributions = occlusion.get_spectral_attributes([hsi, hsi], baseline=0, sliding_window_shapes=3, strides=2)\n            &gt;&gt;&gt; len(attributions)\n            2\n        \"\"\"\n        if self._attribution_method is None:\n            raise RuntimeError(\"Occlusion explainer is not initialized, INITIALIZATION ERROR\")\n\n        if not isinstance(hsi, list):\n            hsi = [hsi]\n\n        if not all(isinstance(hsi_image, HSI) for hsi_image in hsi):\n            raise TypeError(\"All of the input hyperspectral images must be of type HSI\")\n\n        if not isinstance(baseline, list):\n            baseline = [baseline] * len(hsi)\n\n        baseline = torch.stack(\n            [\n                validate_and_transform_baseline(base, hsi_image).to(hsi_image.device)\n                for hsi_image, base in zip(hsi, baseline)\n            ],\n            dim=0,\n        )\n        input_tensor = torch.stack(\n            [hsi_image.get_image().requires_grad_(True).to(hsi_image.device) for hsi_image in hsi], dim=0\n        )\n\n        if isinstance(sliding_window_shapes, tuple):\n            if len(sliding_window_shapes) != 1:\n                raise ValueError(\"Sliding window shapes must be a single integer or a tuple of a single integer\")\n            sliding_window_shapes = sliding_window_shapes[0]\n        if isinstance(strides, tuple):\n            if len(strides) != 1:\n                raise ValueError(\"Strides must be a single integer or a tuple of a single integer\")\n            strides = strides[0]\n\n        if not isinstance(sliding_window_shapes, int):\n            raise TypeError(\"Sliding window shapes must be a single integer\")\n        if not isinstance(strides, int):\n            raise TypeError(\"Strides must be a single integer\")\n\n        if isinstance(hsi, list):\n            full_sliding_window_shapes = list(hsi[0].image.shape)\n            full_sliding_window_shapes[hsi[0].spectral_axis] = sliding_window_shapes\n            full_strides = list(hsi[0].image.shape)\n            full_strides[hsi[0].spectral_axis] = strides\n        else:\n            full_sliding_window_shapes = list(hsi.image.shape)\n            full_sliding_window_shapes[hsi.spectral_axis] = sliding_window_shapes\n            full_strides = list(hsi.image.shape)\n            full_strides[hsi.spectral_axis] = strides\n\n        sliding_window_shapes = tuple(full_sliding_window_shapes)\n        strides = tuple(full_strides)\n\n        occlusion_attributions = self._attribution_method.attribute(\n            input_tensor,\n            sliding_window_shapes=sliding_window_shapes,\n            strides=strides,\n            target=target,\n            baselines=baseline,\n            additional_forward_args=additional_forward_args,\n            perturbations_per_eval=min(perturbations_per_eval, len(hsi)),\n            show_progress=show_progress,\n        )\n\n        try:\n            spectral_attributes = [\n                HSIAttributes(hsi=hsi_image, attributes=attribution, attribution_method=self.get_name())\n                for hsi_image, attribution in zip(hsi, occlusion_attributions)\n            ]\n        except Exception as e:\n            raise HSIAttributesError(f\"Error in generating Occlusion attributions: {e}\") from e\n\n        return spectral_attributes[0] if len(spectral_attributes) == 1 else spectral_attributes\n</code></pre>"},{"location":"reference/#src.meteors.attr.occlusion.Occlusion.attribute","title":"<code>attribute(hsi, target=None, sliding_window_shapes=(1, 1, 1), strides=(1, 1, 1), baseline=None, additional_forward_args=None, perturbations_per_eval=1, show_progress=False)</code>","text":"<p>Method for generating attributions using the Occlusion method.</p> <p>Parameters:</p> Name Type Description Default <code>hsi</code> <code>list[HSI] | HSI</code> <p>Input hyperspectral image(s) for which the attributions are to be computed. If a list of HSI objects is provided, the attributions are computed for each HSI object in the list. The output will be a list of HSIAttributes objects.</p> required <code>target</code> <code>list[int] | int | None</code> <p>target class index for computing the attributions. If None, methods assume that the output has only one class. If the output has multiple classes, the target index must be provided. For multiple input images, a list of target indices can be provided, one for each image or single target value will be used for all images. Defaults to None.</p> <code>None</code> <code>sliding_window_shapes</code> <code>int | tuple[int, int, int]</code> <p>The shape of the sliding window. If an integer is provided, it will be used for all dimensions. Defaults to (1, 1, 1).</p> <code>(1, 1, 1)</code> <code>strides</code> <code>int | tuple[int, int, int]</code> <p>The stride of the sliding window. Defaults to (1, 1, 1). Simply put, the stride is the number of pixels by which the sliding window is moved in each dimension.</p> <code>(1, 1, 1)</code> <code>baseline</code> <code>int | float | Tensor | list[int | float | Tensor]</code> <p>Baselines define reference value which replaces each feature when occluded is computed and can be provided as:     - integer or float representing a constant value used as the baseline for all input pixels.     - tensor with the same shape as the input tensor, providing a baseline for each input pixel.         if the input is a list of HSI objects, the baseline can be a tensor with the same shape as         the input tensor for each HSI object.     - list of integers, floats or tensors with the same shape as the input tensor, providing a baseline         for each input pixel. If the input is a list of HSI objects, the baseline can be a list of         tensors with the same shape as the input tensor for each HSI object. Defaults to None.</p> <code>None</code> <code>additional_forward_args</code> <code>Any</code> <p>If the forward function requires additional arguments other than the inputs for which attributions should not be computed, this argument can be provided. It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple containing multiple additional arguments including tensors or any arbitrary python types. These arguments are provided to forward_func in order following the arguments in inputs. Note that attributions are not computed with respect to these arguments. Default: None</p> <code>None</code> <code>perturbations_per_eval</code> <code>int</code> <p>Allows multiple occlusions to be included in one batch (one call to forward_fn). By default, perturbations_per_eval is 1, so each occlusion is processed individually. Each forward pass will contain a maximum of perturbations_per_eval * #examples samples. For DataParallel models, each batch is split among the available devices, so evaluations on each available device contain at most (perturbations_per_eval * #examples) / num_devices samples. When working with multiple examples, the number of perturbations per evaluation should be set to at least the number of examples. Defaults to 1.</p> <code>1</code> <code>show_progress</code> <code>bool</code> <p>If True, displays a progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>HSIAttributes</code> <code>HSIAttributes | list[HSIAttributes]</code> <p>An object containing the computed attributions.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the explainer is not initialized.</p> <code>ValueError</code> <p>If the sliding window shapes or strides are not a tuple of three integers.</p> <code>HSIAttributesError</code> <p>If an error occurs during the generation of the attributions.</p> Example <p>occlusion = Occlusion(explainable_model) hsi = HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68]) attributions = occlusion.attribute(hsi, baseline=0, sliding_window_shapes=(4, 3, 3), strides=(1, 1, 1)) attributions = occlusion.attribute([hsi, hsi], baseline=0, sliding_window_shapes=(4, 3, 3), strides=(1, 2, 2)) len(attributions) 2</p> Source code in <code>src/meteors/attr/occlusion.py</code> <pre><code>def attribute(\n    self,\n    hsi: list[HSI] | HSI,\n    target: list[int] | int | None = None,\n    sliding_window_shapes: int | tuple[int, int, int] = (1, 1, 1),\n    strides: int | tuple[int, int, int] = (1, 1, 1),\n    baseline: int | float | torch.Tensor | list[int | float | torch.Tensor] = None,\n    additional_forward_args: Any = None,\n    perturbations_per_eval: int = 1,\n    show_progress: bool = False,\n) -&gt; HSIAttributes | list[HSIAttributes]:\n    \"\"\"\n    Method for generating attributions using the Occlusion method.\n\n    Args:\n        hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n            If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n            The output will be a list of HSIAttributes objects.\n        target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n            methods assume that the output has only one class. If the output has multiple classes, the target index\n            must be provided. For multiple input images, a list of target indices can be provided, one for each\n            image or single target value will be used for all images. Defaults to None.\n        sliding_window_shapes (int | tuple[int, int, int]):\n            The shape of the sliding window. If an integer is provided, it will be used for all dimensions.\n            Defaults to (1, 1, 1).\n        strides (int | tuple[int, int, int], optional): The stride of the sliding window. Defaults to (1, 1, 1).\n            Simply put, the stride is the number of pixels by which the sliding window is moved in each dimension.\n        baseline (int | float | torch.Tensor | list[int | float | torch.Tensor], optional): Baselines define\n            reference value which replaces each feature when occluded is computed and can be provided as:\n                - integer or float representing a constant value used as the baseline for all input pixels.\n                - tensor with the same shape as the input tensor, providing a baseline for each input pixel.\n                    if the input is a list of HSI objects, the baseline can be a tensor with the same shape as\n                    the input tensor for each HSI object.\n                - list of integers, floats or tensors with the same shape as the input tensor, providing a baseline\n                    for each input pixel. If the input is a list of HSI objects, the baseline can be a list of\n                    tensors with the same shape as the input tensor for each HSI object. Defaults to None.\n        additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n            the inputs for which attributions should not be computed, this argument can be provided.\n            It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n            containing multiple additional arguments including tensors or any arbitrary python types.\n            These arguments are provided to forward_func in order following the arguments in inputs.\n            Note that attributions are not computed with respect to these arguments. Default: None\n        perturbations_per_eval (int, optional): Allows multiple occlusions to be included in one batch\n            (one call to forward_fn). By default, perturbations_per_eval is 1, so each occlusion is processed\n            individually. Each forward pass will contain a maximum of perturbations_per_eval * #examples samples.\n            For DataParallel models, each batch is split among the available devices, so evaluations on each\n            available device contain at most (perturbations_per_eval * #examples) / num_devices samples. When\n            working with multiple examples, the number of perturbations per evaluation should be set to at least\n            the number of examples. Defaults to 1.\n        show_progress (bool, optional): If True, displays a progress bar. Defaults to False.\n\n    Returns:\n        HSIAttributes: An object containing the computed attributions.\n\n    Raises:\n        RuntimeError: If the explainer is not initialized.\n        ValueError: If the sliding window shapes or strides are not a tuple of three integers.\n        HSIAttributesError: If an error occurs during the generation of the attributions.\n\n    Example:\n        &gt;&gt;&gt; occlusion = Occlusion(explainable_model)\n        &gt;&gt;&gt; hsi = HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n        &gt;&gt;&gt; attributions = occlusion.attribute(hsi, baseline=0, sliding_window_shapes=(4, 3, 3), strides=(1, 1, 1))\n        &gt;&gt;&gt; attributions = occlusion.attribute([hsi, hsi], baseline=0, sliding_window_shapes=(4, 3, 3), strides=(1, 2, 2))\n        &gt;&gt;&gt; len(attributions)\n        2\n    \"\"\"\n    if self._attribution_method is None:\n        raise RuntimeError(\"Occlusion explainer is not initialized, INITIALIZATION ERROR\")\n\n    if not isinstance(hsi, list):\n        hsi = [hsi]\n\n    if not all(isinstance(hsi_image, HSI) for hsi_image in hsi):\n        raise TypeError(\"All of the input hyperspectral images must be of type HSI\")\n\n    if not isinstance(baseline, list):\n        baseline = [baseline] * len(hsi)\n\n    baseline = torch.stack(\n        [\n            validate_and_transform_baseline(base, hsi_image).to(hsi_image.device)\n            for hsi_image, base in zip(hsi, baseline)\n        ],\n        dim=0,\n    )\n    input_tensor = torch.stack(\n        [hsi_image.get_image().requires_grad_(True).to(hsi_image.device) for hsi_image in hsi], dim=0\n    )\n\n    if isinstance(sliding_window_shapes, int):\n        sliding_window_shapes = (sliding_window_shapes, sliding_window_shapes, sliding_window_shapes)\n    if isinstance(strides, int):\n        strides = (strides, strides, strides)\n\n    if len(strides) != 3:\n        raise ValueError(\"Strides must be a tuple of three integers\")\n    if len(sliding_window_shapes) != 3:\n        raise ValueError(\"Sliding window shapes must be a tuple of three integers\")\n\n    occlusion_attributions = self._attribution_method.attribute(\n        input_tensor,\n        sliding_window_shapes=sliding_window_shapes,\n        strides=strides,\n        target=target,\n        baselines=baseline,\n        additional_forward_args=additional_forward_args,\n        perturbations_per_eval=min(perturbations_per_eval, len(hsi)),\n        show_progress=show_progress,\n    )\n\n    try:\n        attributes = [\n            HSIAttributes(hsi=hsi_image, attributes=attribution, attribution_method=self.get_name())\n            for hsi_image, attribution in zip(hsi, occlusion_attributions)\n        ]\n    except Exception as e:\n        raise HSIAttributesError(f\"Error in generating Occlusion attributions: {e}\") from e\n\n    return attributes[0] if len(attributes) == 1 else attributes\n</code></pre>"},{"location":"reference/#src.meteors.attr.occlusion.Occlusion.get_spatial_attributes","title":"<code>get_spatial_attributes(hsi, target=None, sliding_window_shapes=(1, 1), strides=1, baseline=None, additional_forward_args=None, perturbations_per_eval=1, show_progress=False)</code>","text":"<p>Compute spatial attributions for the input HSI using the Occlusion method. In this case, the sliding window is applied to the spatial dimensions only.</p> <p>Parameters:</p> Name Type Description Default <code>hsi</code> <code>list[HSI] | HSI</code> <p>Input hyperspectral image(s) for which the attributions are to be computed. If a list of HSI objects is provided, the attributions are computed for each HSI object in the list. The output will be a list of HSIAttributes objects.</p> required <code>target</code> <code>list[int] | int | None</code> <p>target class index for computing the attributions. If None, methods assume that the output has only one class. If the output has multiple classes, the target index must be provided. For multiple input images, a list of target indices can be provided, one for each image or single target value will be used for all images. Defaults to None.</p> <code>None</code> <code>sliding_window_shapes</code> <code>int | tuple[int, int]</code> <p>The shape of the sliding window for spatial dimensions. If an integer is provided, it will be used for both spatial dimensions. Defaults to (1, 1).</p> <code>(1, 1)</code> <code>strides</code> <code>int | tuple[int, int]</code> <p>The stride of the sliding window for spatial dimensions. Defaults to 1. Simply put, the stride is the number of pixels by which the sliding window is moved in each spatial dimension.</p> <code>1</code> <code>baseline</code> <code>int | float | Tensor | list[int | float | Tensor]</code> <p>Baselines define reference value which replaces each feature when occluded is computed and can be provided as:     - integer or float representing a constant value used as the baseline for all input pixels.     - tensor with the same shape as the input tensor, providing a baseline for each input pixel.         if the input is a list of HSI objects, the baseline can be a tensor with the same shape as         the input tensor for each HSI object.     - list of integers, floats or tensors with the same shape as the input tensor, providing a baseline       for each input pixel. If the input is a list of HSI objects, the baseline can be a list of       tensors with the same shape as the input tensor for each HSI object. Defaults to None.</p> <code>None</code> <code>additional_forward_args</code> <code>Any</code> <p>If the forward function requires additional arguments other than the inputs for which attributions should not be computed, this argument can be provided. It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple containing multiple additional arguments including tensors or any arbitrary python types. These arguments are provided to forward_func in order following the arguments in inputs. Note that attributions are not computed with respect to these arguments. Default: None</p> <code>None</code> <code>perturbations_per_eval</code> <code>int</code> <p>Allows multiple occlusions to be included in one batch (one call to forward_fn). By default, perturbations_per_eval is 1, so each occlusion is processed individually. Each forward pass will contain a maximum of perturbations_per_eval * #examples samples. For DataParallel models, each batch is split among the available devices, so evaluations on each available device contain at most (perturbations_per_eval * #examples) / num_devices samples. When working with multiple examples, the number of perturbations per evaluation should be set to at least the number of examples. Defaults to 1.</p> <code>1</code> <code>show_progress</code> <code>bool</code> <p>If True, displays a progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>HSIAttributes</code> <code>HSIAttributes | list[HSIAttributes]</code> <p>An object containing the computed spatial attributions.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the explainer is not initialized.</p> <code>ValueError</code> <p>If the sliding window shapes or strides are not a tuple of two integers.</p> <code>HSIAttributesError</code> <p>If an error occurs during the generation of the attributions</p> Example <p>occlusion = Occlusion(explainable_model) hsi = HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68]) attributions = occlusion.get_spatial_attributes(hsi, baseline=0, sliding_window_shapes=(3, 3), strides=(1, 1)) attributions = occlusion.get_spatial_attributes([hsi, hsi], baseline=0, sliding_window_shapes=(3, 3), strides=(2, 2)) len(attributions) 2</p> Source code in <code>src/meteors/attr/occlusion.py</code> <pre><code>def get_spatial_attributes(\n    self,\n    hsi: list[HSI] | HSI,\n    target: list[int] | int | None = None,\n    sliding_window_shapes: int | tuple[int, int] = (1, 1),\n    strides: int | tuple[int, int] = 1,\n    baseline: int | float | torch.Tensor | list[int | float | torch.Tensor] = None,\n    additional_forward_args: Any = None,\n    perturbations_per_eval: int = 1,\n    show_progress: bool = False,\n) -&gt; HSIAttributes | list[HSIAttributes]:\n    \"\"\"Compute spatial attributions for the input HSI using the Occlusion method. In this case, the sliding window\n    is applied to the spatial dimensions only.\n\n    Args:\n        hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n            If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n            The output will be a list of HSIAttributes objects.\n        target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n            methods assume that the output has only one class. If the output has multiple classes, the target index\n            must be provided. For multiple input images, a list of target indices can be provided, one for each\n            image or single target value will be used for all images. Defaults to None.\n        sliding_window_shapes (int | tuple[int, int]): The shape of the sliding window for spatial dimensions.\n            If an integer is provided, it will be used for both spatial dimensions. Defaults to (1, 1).\n        strides (int | tuple[int, int], optional): The stride of the sliding window for spatial dimensions.\n            Defaults to 1. Simply put, the stride is the number of pixels by which the sliding window is moved\n            in each spatial dimension.\n        baseline (int | float | torch.Tensor | list[int | float | torch.Tensor], optional): Baselines define\n            reference value which replaces each feature when occluded is computed and can be provided as:\n                - integer or float representing a constant value used as the baseline for all input pixels.\n                - tensor with the same shape as the input tensor, providing a baseline for each input pixel.\n                    if the input is a list of HSI objects, the baseline can be a tensor with the same shape as\n                    the input tensor for each HSI object.\n                - list of integers, floats or tensors with the same shape as the input tensor, providing a baseline\n                  for each input pixel. If the input is a list of HSI objects, the baseline can be a list of\n                  tensors with the same shape as the input tensor for each HSI object. Defaults to None.\n        additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n            the inputs for which attributions should not be computed, this argument can be provided.\n            It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n            containing multiple additional arguments including tensors or any arbitrary python types.\n            These arguments are provided to forward_func in order following the arguments in inputs.\n            Note that attributions are not computed with respect to these arguments. Default: None\n        perturbations_per_eval (int, optional): Allows multiple occlusions to be included in one batch\n            (one call to forward_fn). By default, perturbations_per_eval is 1, so each occlusion is processed\n            individually. Each forward pass will contain a maximum of perturbations_per_eval * #examples samples.\n            For DataParallel models, each batch is split among the available devices, so evaluations on each\n            available device contain at most (perturbations_per_eval * #examples) / num_devices samples. When\n            working with multiple examples, the number of perturbations per evaluation should be set to at least\n            the number of examples. Defaults to 1.\n        show_progress (bool, optional): If True, displays a progress bar. Defaults to False.\n\n    Returns:\n        HSIAttributes: An object containing the computed spatial attributions.\n\n    Raises:\n        RuntimeError: If the explainer is not initialized.\n        ValueError: If the sliding window shapes or strides are not a tuple of two integers.\n        HSIAttributesError: If an error occurs during the generation of the attributions\n\n    Example:\n        &gt;&gt;&gt; occlusion = Occlusion(explainable_model)\n        &gt;&gt;&gt; hsi = HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n        &gt;&gt;&gt; attributions = occlusion.get_spatial_attributes(hsi, baseline=0, sliding_window_shapes=(3, 3), strides=(1, 1))\n        &gt;&gt;&gt; attributions = occlusion.get_spatial_attributes([hsi, hsi], baseline=0, sliding_window_shapes=(3, 3), strides=(2, 2))\n        &gt;&gt;&gt; len(attributions)\n        2\n    \"\"\"\n    if self._attribution_method is None:\n        raise RuntimeError(\"Occlusion explainer is not initialized, INITIALIZATION ERROR\")\n\n    if not isinstance(hsi, list):\n        hsi = [hsi]\n\n    if not all(isinstance(hsi_image, HSI) for hsi_image in hsi):\n        raise TypeError(\"All of the input hyperspectral images must be of type HSI\")\n\n    if not isinstance(baseline, list):\n        baseline = [baseline] * len(hsi)\n\n    baseline = torch.stack(\n        [\n            validate_and_transform_baseline(base, hsi_image).to(hsi_image.device)\n            for hsi_image, base in zip(hsi, baseline)\n        ],\n        dim=0,\n    )\n    input_tensor = torch.stack(\n        [hsi_image.get_image().requires_grad_(True).to(hsi_image.device) for hsi_image in hsi], dim=0\n    )\n\n    if isinstance(sliding_window_shapes, int):\n        sliding_window_shapes = (sliding_window_shapes, sliding_window_shapes)\n    if isinstance(strides, int):\n        strides = (strides, strides)\n\n    if len(strides) != 2:\n        raise ValueError(\"Strides must be a tuple of two integers\")\n    if len(sliding_window_shapes) != 2:\n        raise ValueError(\"Sliding window shapes must be a tuple of two integers\")\n\n    list_sliding_window_shapes = list(sliding_window_shapes)\n    list_strides = list(strides)\n    if isinstance(hsi, list):\n        list_sliding_window_shapes.insert(hsi[0].spectral_axis, hsi[0].image.shape[hsi[0].spectral_axis])\n        list_strides.insert(hsi[0].spectral_axis, hsi[0].image.shape[hsi[0].spectral_axis])\n    else:\n        list_sliding_window_shapes.insert(hsi.spectral_axis, hsi.image.shape[hsi.spectral_axis])\n        list_strides.insert(hsi.spectral_axis, hsi.image.shape[hsi.spectral_axis])\n    sliding_window_shapes = tuple(list_sliding_window_shapes)  # type: ignore\n    strides = tuple(list_strides)  # type: ignore\n\n    occlusion_attributions = self._attribution_method.attribute(\n        input_tensor,\n        sliding_window_shapes=sliding_window_shapes,\n        strides=strides,\n        target=target,\n        baselines=baseline,\n        additional_forward_args=additional_forward_args,\n        perturbations_per_eval=min(perturbations_per_eval, len(hsi)),\n        show_progress=show_progress,\n    )\n\n    try:\n        spatial_attributes = [\n            HSIAttributes(hsi=hsi_image, attributes=attribution, attribution_method=self.get_name())\n            for hsi_image, attribution in zip(hsi, occlusion_attributions)\n        ]\n    except Exception as e:\n        raise HSIAttributesError(f\"Error in generating Occlusion attributions: {e}\") from e\n\n    return spatial_attributes[0] if len(spatial_attributes) == 1 else spatial_attributes\n</code></pre>"},{"location":"reference/#src.meteors.attr.occlusion.Occlusion.get_spectral_attributes","title":"<code>get_spectral_attributes(hsi, target=None, sliding_window_shapes=1, strides=1, baseline=None, additional_forward_args=None, perturbations_per_eval=1, show_progress=False)</code>","text":"<p>Compute spectral attributions for the input HSI using the Occlusion method. In this case, the sliding window is applied to the spectral dimension only.</p> <p>Parameters:</p> Name Type Description Default <code>hsi</code> <code>list[HSI] | HSI</code> <p>Input hyperspectral image(s) for which the attributions are to be computed. If a list of HSI objects is provided, the attributions are computed for each HSI object in the list. The output will be a list of HSIAttributes objects.</p> required <code>target</code> <code>list[int] | int | None</code> <p>target class index for computing the attributions. If None, methods assume that the output has only one class. If the output has multiple classes, the target index must be provided. For multiple input images, a list of target indices can be provided, one for each image or single target value will be used for all images. Defaults to None.</p> <code>None</code> <code>sliding_window_shapes</code> <code>int | tuple[int]</code> <p>The size of the sliding window for the spectral dimension. Defaults to 1.</p> <code>1</code> <code>strides</code> <code>int | tuple[int]</code> <p>The stride of the sliding window for the spectral dimension. Defaults to 1. Simply put, the stride is the number of pixels by which the sliding window is moved in spectral dimension.</p> <code>1</code> <code>baseline</code> <code>int | float | Tensor | list[int | float | Tensor]</code> <p>Baselines define reference value which replaces each feature when occluded is computed and can be provided as:     - integer or float representing a constant value used as the baseline for all input pixels.     - tensor with the same shape as the input tensor, providing a baseline for each input pixel.         if the input is a list of HSI objects, the baseline can be a tensor with the same shape as         the input tensor for each HSI object.     - list of integers, floats or tensors with the same shape as the input tensor, providing a baseline       for each input pixel. If the input is a list of HSI objects, the baseline can be a list of       tensors with the same shape as the input tensor for each HSI object. Defaults to None.</p> <code>None</code> <code>additional_forward_args</code> <code>Any</code> <p>If the forward function requires additional arguments other than the inputs for which attributions should not be computed, this argument can be provided. It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple containing multiple additional arguments including tensors or any arbitrary python types. These arguments are provided to forward_func in order following the arguments in inputs. Note that attributions are not computed with respect to these arguments. Default: None</p> <code>None</code> <code>perturbations_per_eval</code> <code>int</code> <p>Allows multiple occlusions to be included in one batch (one call to forward_fn). By default, perturbations_per_eval is 1, so each occlusion is processed individually. Each forward pass will contain a maximum of perturbations_per_eval * #examples samples. For DataParallel models, each batch is split among the available devices, so evaluations on each available device contain at most (perturbations_per_eval * #examples) / num_devices samples. When working with multiple examples, the number of perturbations per evaluation should be set to at least the number of examples. Defaults to 1.</p> <code>1</code> <code>show_progress</code> <code>bool</code> <p>If True, displays a progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>HSIAttributes</code> <code>HSIAttributes | list[HSIAttributes]</code> <p>An object containing the computed spectral attributions.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the explainer is not initialized.</p> <code>ValueError</code> <p>If the sliding window shapes or strides are not a tuple of a single integer.</p> <code>TypeError</code> <p>If the sliding window shapes or strides are not a single integer.</p> <code>HSIAttributesError</code> <p>If an error occurs during the generation of the attributions</p> Example <p>occlusion = Occlusion(explainable_model) hsi = HSI(image=torch.ones((10, 240, 240)), wavelengths=torch.arange(10)) attributions = occlusion.get_spectral_attributes(hsi, baseline=0, sliding_window_shapes=3, strides=1) attributions = occlusion.get_spectral_attributes([hsi, hsi], baseline=0, sliding_window_shapes=3, strides=2) len(attributions) 2</p> Source code in <code>src/meteors/attr/occlusion.py</code> <pre><code>def get_spectral_attributes(\n    self,\n    hsi: list[HSI] | HSI,\n    target: list[int] | int | None = None,\n    sliding_window_shapes: int | tuple[int] = 1,\n    strides: int | tuple[int] = 1,\n    baseline: int | float | torch.Tensor | list[int | float | torch.Tensor] = None,\n    additional_forward_args: Any = None,\n    perturbations_per_eval: int = 1,\n    show_progress: bool = False,\n) -&gt; HSIAttributes | list[HSIAttributes]:\n    \"\"\"Compute spectral attributions for the input HSI using the Occlusion method. In this case, the sliding window\n    is applied to the spectral dimension only.\n\n    Args:\n        hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n            If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n            The output will be a list of HSIAttributes objects.\n        target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n            methods assume that the output has only one class. If the output has multiple classes, the target index\n            must be provided. For multiple input images, a list of target indices can be provided, one for each\n            image or single target value will be used for all images. Defaults to None.\n        sliding_window_shapes (int | tuple[int]): The size of the sliding window for the spectral dimension.\n            Defaults to 1.\n        strides (int | tuple[int], optional): The stride of the sliding window for the spectral dimension.\n            Defaults to 1. Simply put, the stride is the number of pixels by which the sliding window is moved\n            in spectral dimension.\n        baseline (int | float | torch.Tensor | list[int | float | torch.Tensor], optional): Baselines define\n            reference value which replaces each feature when occluded is computed and can be provided as:\n                - integer or float representing a constant value used as the baseline for all input pixels.\n                - tensor with the same shape as the input tensor, providing a baseline for each input pixel.\n                    if the input is a list of HSI objects, the baseline can be a tensor with the same shape as\n                    the input tensor for each HSI object.\n                - list of integers, floats or tensors with the same shape as the input tensor, providing a baseline\n                  for each input pixel. If the input is a list of HSI objects, the baseline can be a list of\n                  tensors with the same shape as the input tensor for each HSI object. Defaults to None.\n        additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n            the inputs for which attributions should not be computed, this argument can be provided.\n            It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n            containing multiple additional arguments including tensors or any arbitrary python types.\n            These arguments are provided to forward_func in order following the arguments in inputs.\n            Note that attributions are not computed with respect to these arguments. Default: None\n        perturbations_per_eval (int, optional): Allows multiple occlusions to be included in one batch\n            (one call to forward_fn). By default, perturbations_per_eval is 1, so each occlusion is processed\n            individually. Each forward pass will contain a maximum of perturbations_per_eval * #examples samples.\n            For DataParallel models, each batch is split among the available devices, so evaluations on each\n            available device contain at most (perturbations_per_eval * #examples) / num_devices samples. When\n            working with multiple examples, the number of perturbations per evaluation should be set to at least\n            the number of examples. Defaults to 1.\n        show_progress (bool, optional): If True, displays a progress bar. Defaults to False.\n\n    Returns:\n        HSIAttributes: An object containing the computed spectral attributions.\n\n    Raises:\n        RuntimeError: If the explainer is not initialized.\n        ValueError: If the sliding window shapes or strides are not a tuple of a single integer.\n        TypeError: If the sliding window shapes or strides are not a single integer.\n        HSIAttributesError: If an error occurs during the generation of the attributions\n\n    Example:\n        &gt;&gt;&gt; occlusion = Occlusion(explainable_model)\n        &gt;&gt;&gt; hsi = HSI(image=torch.ones((10, 240, 240)), wavelengths=torch.arange(10))\n        &gt;&gt;&gt; attributions = occlusion.get_spectral_attributes(hsi, baseline=0, sliding_window_shapes=3, strides=1)\n        &gt;&gt;&gt; attributions = occlusion.get_spectral_attributes([hsi, hsi], baseline=0, sliding_window_shapes=3, strides=2)\n        &gt;&gt;&gt; len(attributions)\n        2\n    \"\"\"\n    if self._attribution_method is None:\n        raise RuntimeError(\"Occlusion explainer is not initialized, INITIALIZATION ERROR\")\n\n    if not isinstance(hsi, list):\n        hsi = [hsi]\n\n    if not all(isinstance(hsi_image, HSI) for hsi_image in hsi):\n        raise TypeError(\"All of the input hyperspectral images must be of type HSI\")\n\n    if not isinstance(baseline, list):\n        baseline = [baseline] * len(hsi)\n\n    baseline = torch.stack(\n        [\n            validate_and_transform_baseline(base, hsi_image).to(hsi_image.device)\n            for hsi_image, base in zip(hsi, baseline)\n        ],\n        dim=0,\n    )\n    input_tensor = torch.stack(\n        [hsi_image.get_image().requires_grad_(True).to(hsi_image.device) for hsi_image in hsi], dim=0\n    )\n\n    if isinstance(sliding_window_shapes, tuple):\n        if len(sliding_window_shapes) != 1:\n            raise ValueError(\"Sliding window shapes must be a single integer or a tuple of a single integer\")\n        sliding_window_shapes = sliding_window_shapes[0]\n    if isinstance(strides, tuple):\n        if len(strides) != 1:\n            raise ValueError(\"Strides must be a single integer or a tuple of a single integer\")\n        strides = strides[0]\n\n    if not isinstance(sliding_window_shapes, int):\n        raise TypeError(\"Sliding window shapes must be a single integer\")\n    if not isinstance(strides, int):\n        raise TypeError(\"Strides must be a single integer\")\n\n    if isinstance(hsi, list):\n        full_sliding_window_shapes = list(hsi[0].image.shape)\n        full_sliding_window_shapes[hsi[0].spectral_axis] = sliding_window_shapes\n        full_strides = list(hsi[0].image.shape)\n        full_strides[hsi[0].spectral_axis] = strides\n    else:\n        full_sliding_window_shapes = list(hsi.image.shape)\n        full_sliding_window_shapes[hsi.spectral_axis] = sliding_window_shapes\n        full_strides = list(hsi.image.shape)\n        full_strides[hsi.spectral_axis] = strides\n\n    sliding_window_shapes = tuple(full_sliding_window_shapes)\n    strides = tuple(full_strides)\n\n    occlusion_attributions = self._attribution_method.attribute(\n        input_tensor,\n        sliding_window_shapes=sliding_window_shapes,\n        strides=strides,\n        target=target,\n        baselines=baseline,\n        additional_forward_args=additional_forward_args,\n        perturbations_per_eval=min(perturbations_per_eval, len(hsi)),\n        show_progress=show_progress,\n    )\n\n    try:\n        spectral_attributes = [\n            HSIAttributes(hsi=hsi_image, attributes=attribution, attribution_method=self.get_name())\n            for hsi_image, attribution in zip(hsi, occlusion_attributions)\n        ]\n    except Exception as e:\n        raise HSIAttributesError(f\"Error in generating Occlusion attributions: {e}\") from e\n\n    return spectral_attributes[0] if len(spectral_attributes) == 1 else spectral_attributes\n</code></pre>"},{"location":"reference/#src.meteors.attr.saliency.Saliency","title":"<code>Saliency</code>","text":"<p>               Bases: <code>Explainer</code></p> <p>Saliency explainer class for generating attributions using the Saliency method. This baseline method for computing input attribution calculates gradients with respect to inputs. It also has an option to return the absolute value of the gradients, which is the default behaviour. Implementation of this method is based on the <code>captum</code> repository</p> <p>Attributes:</p> Name Type Description <code>_attribution_method</code> <code>Saliency</code> <p>The Saliency method from the <code>captum</code> library.</p> <p>Parameters:</p> Name Type Description Default <code>explainable_model</code> <code>ExplainableModel | Explainer</code> <p>The explainable model to be explained.</p> required <code>postprocessing_segmentation_output</code> <code>Callable[[Tensor], Tensor] | None</code> <p>A segmentation postprocessing function for segmentation problem type. This is required for segmentation problem type as attribution methods needs to have 1d output. Defaults to None, which means that the attribution method is not used.</p> <code>None</code> Source code in <code>src/meteors/attr/saliency.py</code> <pre><code>class Saliency(Explainer):\n    \"\"\"\n    Saliency explainer class for generating attributions using the Saliency method.\n    This baseline method for computing input attribution calculates gradients with respect to inputs.\n    It also has an option to return the absolute value of the gradients, which is the default behaviour.\n    Implementation of this method is based on the [`captum` repository](https://captum.ai/api/saliency.html)\n\n    Attributes:\n        _attribution_method (CaptumSaliency): The Saliency method from the `captum` library.\n\n    Args:\n        explainable_model (ExplainableModel | Explainer): The explainable model to be explained.\n        postprocessing_segmentation_output (Callable[[torch.Tensor], torch.Tensor] | None):\n            A segmentation postprocessing function for segmentation problem type. This is required for segmentation\n            problem type as attribution methods needs to have 1d output. Defaults to None, which means that the\n            attribution method is not used.\n    \"\"\"\n\n    def __init__(\n        self,\n        explainable_model: ExplainableModel,\n        postprocessing_segmentation_output: Callable[[torch.Tensor], torch.Tensor] | None = None,\n    ):\n        super().__init__(explainable_model, postprocessing_segmentation_output=postprocessing_segmentation_output)\n\n        self._attribution_method = CaptumSaliency(explainable_model.forward_func)\n\n    def attribute(\n        self,\n        hsi: list[HSI] | HSI,\n        target: list[int] | int | None = None,\n        abs: bool = True,\n        additional_forward_args: Any = None,\n    ) -&gt; HSIAttributes | list[HSIAttributes]:\n        \"\"\"\n        Method for generating attributions using the Saliency method.\n\n        Args:\n            hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n                If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n                The output will be a list of HSIAttributes objects.\n            target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n                methods assume that the output has only one class. If the output has multiple classes, the target index\n                must be provided. For multiple input images, a list of target indices can be provided, one for each\n                image or single target value will be used for all images. Defaults to None.\n            abs (bool, optional): Returns absolute value of gradients if set to True,\n                otherwise returns the (signed) gradients if False. Default: True\n            additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n                the inputs for which attributions should not be computed, this argument can be provided.\n                It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n                containing multiple additional arguments including tensors or any arbitrary python types.\n                These arguments are provided to forward_func in order following the arguments in inputs.\n                Note that attributions are not computed with respect to these arguments. Default: None\n\n        Returns:\n            HSIAttributes | list[HSIAttributes]: The computed attributions for the input hyperspectral image(s).\n                if a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n\n        Raises:\n            RuntimeError: If the explainer is not initialized.\n            HSIAttributesError: If an error occurs during the generation of the attributions\n\n        Examples:\n            &gt;&gt;&gt; saliency = Saliency(explainable_model)\n            &gt;&gt;&gt; hsi = HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n            &gt;&gt;&gt; attributions = saliency.attribute(hsi)\n            &gt;&gt;&gt; attributions = saliency.attribute([hsi, hsi])\n            &gt;&gt;&gt; len(attributions)\n            2\n        \"\"\"\n        if self._attribution_method is None:\n            raise RuntimeError(\"Saliency explainer is not initialized, INITIALIZATION ERROR\")\n\n        if not isinstance(hsi, list):\n            hsi = [hsi]\n\n        if not all(isinstance(hsi_image, HSI) for hsi_image in hsi):\n            raise TypeError(\"All of the input hyperspectral images must be of type HSI\")\n\n        input_tensor = torch.stack(\n            [hsi_image.get_image().requires_grad_(True).to(hsi_image.device) for hsi_image in hsi], dim=0\n        )\n\n        saliency_attributions = self._attribution_method.attribute(\n            input_tensor, target=target, abs=abs, additional_forward_args=additional_forward_args\n        )\n\n        try:\n            attributes = [\n                HSIAttributes(hsi=hsi_image, attributes=attribution, attribution_method=self.get_name())\n                for hsi_image, attribution in zip(hsi, saliency_attributions)\n            ]\n        except Exception as e:\n            raise HSIAttributesError(f\"Error in generating Saliency attributions: {e}\") from e\n\n        return attributes[0] if len(attributes) == 1 else attributes\n</code></pre>"},{"location":"reference/#src.meteors.attr.saliency.Saliency.attribute","title":"<code>attribute(hsi, target=None, abs=True, additional_forward_args=None)</code>","text":"<p>Method for generating attributions using the Saliency method.</p> <p>Parameters:</p> Name Type Description Default <code>hsi</code> <code>list[HSI] | HSI</code> <p>Input hyperspectral image(s) for which the attributions are to be computed. If a list of HSI objects is provided, the attributions are computed for each HSI object in the list. The output will be a list of HSIAttributes objects.</p> required <code>target</code> <code>list[int] | int | None</code> <p>target class index for computing the attributions. If None, methods assume that the output has only one class. If the output has multiple classes, the target index must be provided. For multiple input images, a list of target indices can be provided, one for each image or single target value will be used for all images. Defaults to None.</p> <code>None</code> <code>abs</code> <code>bool</code> <p>Returns absolute value of gradients if set to True, otherwise returns the (signed) gradients if False. Default: True</p> <code>True</code> <code>additional_forward_args</code> <code>Any</code> <p>If the forward function requires additional arguments other than the inputs for which attributions should not be computed, this argument can be provided. It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple containing multiple additional arguments including tensors or any arbitrary python types. These arguments are provided to forward_func in order following the arguments in inputs. Note that attributions are not computed with respect to these arguments. Default: None</p> <code>None</code> <p>Returns:</p> Type Description <code>HSIAttributes | list[HSIAttributes]</code> <p>HSIAttributes | list[HSIAttributes]: The computed attributions for the input hyperspectral image(s). if a list of HSI objects is provided, the attributions are computed for each HSI object in the list.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the explainer is not initialized.</p> <code>HSIAttributesError</code> <p>If an error occurs during the generation of the attributions</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; saliency = Saliency(explainable_model)\n&gt;&gt;&gt; hsi = HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n&gt;&gt;&gt; attributions = saliency.attribute(hsi)\n&gt;&gt;&gt; attributions = saliency.attribute([hsi, hsi])\n&gt;&gt;&gt; len(attributions)\n2\n</code></pre> Source code in <code>src/meteors/attr/saliency.py</code> <pre><code>def attribute(\n    self,\n    hsi: list[HSI] | HSI,\n    target: list[int] | int | None = None,\n    abs: bool = True,\n    additional_forward_args: Any = None,\n) -&gt; HSIAttributes | list[HSIAttributes]:\n    \"\"\"\n    Method for generating attributions using the Saliency method.\n\n    Args:\n        hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n            If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n            The output will be a list of HSIAttributes objects.\n        target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n            methods assume that the output has only one class. If the output has multiple classes, the target index\n            must be provided. For multiple input images, a list of target indices can be provided, one for each\n            image or single target value will be used for all images. Defaults to None.\n        abs (bool, optional): Returns absolute value of gradients if set to True,\n            otherwise returns the (signed) gradients if False. Default: True\n        additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n            the inputs for which attributions should not be computed, this argument can be provided.\n            It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n            containing multiple additional arguments including tensors or any arbitrary python types.\n            These arguments are provided to forward_func in order following the arguments in inputs.\n            Note that attributions are not computed with respect to these arguments. Default: None\n\n    Returns:\n        HSIAttributes | list[HSIAttributes]: The computed attributions for the input hyperspectral image(s).\n            if a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n\n    Raises:\n        RuntimeError: If the explainer is not initialized.\n        HSIAttributesError: If an error occurs during the generation of the attributions\n\n    Examples:\n        &gt;&gt;&gt; saliency = Saliency(explainable_model)\n        &gt;&gt;&gt; hsi = HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n        &gt;&gt;&gt; attributions = saliency.attribute(hsi)\n        &gt;&gt;&gt; attributions = saliency.attribute([hsi, hsi])\n        &gt;&gt;&gt; len(attributions)\n        2\n    \"\"\"\n    if self._attribution_method is None:\n        raise RuntimeError(\"Saliency explainer is not initialized, INITIALIZATION ERROR\")\n\n    if not isinstance(hsi, list):\n        hsi = [hsi]\n\n    if not all(isinstance(hsi_image, HSI) for hsi_image in hsi):\n        raise TypeError(\"All of the input hyperspectral images must be of type HSI\")\n\n    input_tensor = torch.stack(\n        [hsi_image.get_image().requires_grad_(True).to(hsi_image.device) for hsi_image in hsi], dim=0\n    )\n\n    saliency_attributions = self._attribution_method.attribute(\n        input_tensor, target=target, abs=abs, additional_forward_args=additional_forward_args\n    )\n\n    try:\n        attributes = [\n            HSIAttributes(hsi=hsi_image, attributes=attribution, attribution_method=self.get_name())\n            for hsi_image, attribution in zip(hsi, saliency_attributions)\n        ]\n    except Exception as e:\n        raise HSIAttributesError(f\"Error in generating Saliency attributions: {e}\") from e\n\n    return attributes[0] if len(attributes) == 1 else attributes\n</code></pre>"},{"location":"reference/#hyper-noise-tunnel","title":"Hyper Noise Tunnel","text":""},{"location":"reference/#src.meteors.attr.noise_tunnel.NoiseTunnel","title":"<code>NoiseTunnel</code>","text":"<p>               Bases: <code>BaseNoiseTunnel</code></p> <p>Noise Tunnel is a method that is used to explain the model's predictions by adding noise to the input tensor. The noise is added to the input tensor, and the model's output is computed. The process is repeated multiple times to obtain a distribution of the model's output. The final attribution is computed as the mean of the outputs. For more information about the method, see captum documentation: https://captum.ai/api/noise_tunnel.html.</p> <p>Parameters:</p> Name Type Description Default <code>chained_explainer</code> <p>The explainable method that will be used to compute the attributions.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the callable object is not an instance of the Explainer class</p> Source code in <code>src/meteors/attr/noise_tunnel.py</code> <pre><code>class NoiseTunnel(BaseNoiseTunnel):\n    \"\"\"Noise Tunnel is a method that is used to explain the model's predictions by adding noise to the input tensor.\n    The noise is added to the input tensor, and the model's output is computed. The process is repeated multiple times\n    to obtain a distribution of the model's output. The final attribution is computed as the mean of the outputs.\n    For more information about the method, see captum documentation: https://captum.ai/api/noise_tunnel.html.\n\n    Arguments:\n        chained_explainer: The explainable method that will be used to compute the attributions.\n\n    Raises:\n        RuntimeError: If the callable object is not an instance of the Explainer class\n    \"\"\"\n\n    @staticmethod\n    def perturb_input(\n        input: torch.Tensor,\n        n_samples: int = 1,\n        perturbation_axis: None | tuple[int | slice] = None,\n        stdevs: float = 1,\n        **kwargs: Any,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        The default perturbation function used in the noise tunnel with small enhancement for hyperspectral images.\n        It randomly adds noise to the input tensor from a normal distribution with a given standard deviation.\n        The noise is added to the selected bands (channels) of the input tensor.\n        The bands to be perturbed are selected based on the `perturbation_axis` parameter.\n        By default all bands are perturbed, which is equivalent to the standard noise tunnel method.\n\n        Args:\n            input (torch.Tensor): An input tensor to be perturbed. It should have the shape (C, H, W).\n            n_samples (int): A number of samples to be drawn - number of perturbed inputs to be generated.\n            perturbation_axis (None | tuple[int | slice]): The indices of the bands to be perturbed.\n                If set to None, all bands are perturbed. Defaults to None.\n            stdevs (float): The standard deviation of gaussian noise with zero mean that is added to each input\n                in the batch. Defaults to 1.0.\n\n        Returns:\n            torch.Tensor: A perturbed tensor, which contains `n_samples` perturbed inputs.\n        \"\"\"\n        if n_samples &lt; 1:\n            raise ValueError(\"Number of perturbated samples to be generated must be greater than 0\")\n\n        # the perturbation\n        perturbed_input = input.clone().unsqueeze(0)\n        # repeat the perturbed_input on the first dimension n_samples times\n        perturbed_input = perturbed_input.repeat_interleave(n_samples, dim=0)\n\n        # the perturbation shape\n        if perturbation_axis is None:\n            perturbation_shape = perturbed_input.shape\n        else:\n            perturbation_axis = (slice(None),) + perturbation_axis  # type: ignore\n            perturbation_shape = perturbed_input[perturbation_axis].shape\n\n        # the noise\n        noise = torch.normal(0, stdevs, size=perturbation_shape).to(input.device)\n\n        # add the noise to the perturbed_input\n        if perturbation_axis is None:\n            perturbed_input += noise\n        else:\n            perturbed_input[perturbation_axis] += noise\n\n        perturbed_input.requires_grad_(True)\n\n        return perturbed_input\n\n    def attribute(\n        self,\n        hsi: list[HSI] | HSI,\n        target: list[int] | int | None = None,\n        additional_forward_args: Any = None,\n        n_samples: int = 5,\n        steps_per_batch: int = 1,\n        perturbation_axis: None | tuple[int | slice] = None,\n        stdevs: float | tuple[float, ...] = 1.0,\n        method: Literal[\"smoothgrad\", \"smoothgrad_sq\", \"vargrad\"] = \"smoothgrad\",\n    ) -&gt; HSIAttributes | list[HSIAttributes]:\n        \"\"\"\n        Method for generating attributions using the Noise Tunnel method.\n\n        hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n                If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n                The output will be a list of HSIAttributes objects.\n            baseline (int | float | torch.Tensor, optional): Baselines define reference value which replaces each\n                feature when occluded is computed and can be provided as:\n                    - integer or float representing a constant value used as the baseline for all input pixels.\n                    - tensor with the same shape as the input tensor, providing a baseline for each input pixel.\n                        if the input is a list of HSI objects, the baseline can be a tensor with the same shape as\n                        the input tensor for each HSI object.\n            target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n                methods assume that the output has only one class. If the output has multiple classes, the target index\n                must be provided. For multiple input images, a list of target indices can be provided, one for each\n                image or single target value will be used for all images. Defaults to None.\n            additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n                the inputs for which attributions should not be computed, this argument can be provided.\n                It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n                containing multiple additional arguments including tensors or any arbitrary python types.\n                These arguments are provided to forward_func in order following the arguments in inputs.\n                Note that attributions are not computed with respect to these arguments. Default: None\n            n_samples (int, optional): The number of randomly generated examples per sample in the input batch.\n                Random examples are generated by adding gaussian random noise to each sample.\n                Default: 5 if nt_samples is not provided.\n            steps_per_batch (int, optional): The number of the n_samples that will be processed together.\n                With the help of this parameter we can avoid out of memory situation and reduce the number of randomly\n                generated examples per sample in each batch. Default: None if steps_per_batch is not provided.\n                In this case all nt_samples will be processed together.\n            perturbation_axis (None | tuple[int | slice], optional): The indices of the input image to be perturbed.\n                If set to None, all bands are perturbed, which corresponds to a traditional noise tunnel method.\n                Defaults to None.\n            stdevs (float | tuple[float, ...], optional): The standard deviation of gaussian noise with zero mean that\n                is added to each input in the batch. If stdevs is a single float value then that same value is used\n                for all inputs. If stdevs is a tuple, then the length of the tuple must match the number of inputs as\n                each value in the tuple is used for the corresponding input. Default: 1.0\n            method (Literal[\"smoothgrad\", \"smoothgrad_sq\", \"vargrad\"], optional): Smoothing type of the attributions.\n                smoothgrad, smoothgrad_sq or vargrad Default: smoothgrad if type is not provided.\n\n        Returns:\n            HSIAttributes | list[HSIAttributes]: The computed attributions for the input hyperspectral image(s).\n                if a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n\n        Raises:\n            HSIAttributesError: If an error occurs during the generation of the attributions.\n\n        Examples:\n            &gt;&gt;&gt; noise_tunnel = NoiseTunnel(explainable_model)\n            &gt;&gt;&gt; hsi = HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n            &gt;&gt;&gt; attributions = noise_tunnel.attribute(hsi)\n            &gt;&gt;&gt; attributions = noise_tunnel.attribute([hsi, hsi])\n            &gt;&gt;&gt; len(attributions)\n            2\n        \"\"\"\n        if isinstance(stdevs, list):\n            stdevs = tuple(stdevs)\n\n        if not isinstance(hsi, list):\n            hsi = [hsi]\n\n        if not all([isinstance(input, HSI) for input in hsi]):\n            raise TypeError(\"All inputs must be HSI objects\")\n\n        if isinstance(stdevs, tuple):\n            if len(stdevs) != len(hsi):\n                raise ValueError(\n                    \"The number of stdevs must match the number of input images, number of stdevs:\"\n                    f\"{len(stdevs)}, number of input images: {len(hsi)}\"\n                )\n        else:\n            stdevs = tuple([stdevs] * len(hsi))\n\n        if not isinstance(target, list):\n            target = [target] * len(hsi)  # type: ignore\n\n        nt_attributes = torch.empty((n_samples, len(hsi)) + hsi[0].image.shape, device=hsi[0].device)\n\n        for batch in range(0, len(hsi)):\n            input = hsi[batch]\n            targeted = target[batch]\n            stdev = stdevs[batch]\n            perturbed_input = self.perturb_input(input.image, n_samples, perturbation_axis, stdev)\n            nt_attributes[:, batch] = self._forward_loop(\n                perturbed_input, input, targeted, additional_forward_args, n_samples, steps_per_batch\n            )\n\n        nt_attributes = self._aggregate_attributions(nt_attributes, method)\n\n        try:\n            attributes = [\n                HSIAttributes(hsi=hsi_image, attributes=attribution, attribution_method=self.get_name())\n                for hsi_image, attribution in zip(hsi, nt_attributes)\n            ]\n        except Exception as e:\n            raise HSIAttributesError(f\"Error in generating NoiseTunnel attributions: {e}\") from e\n\n        return attributes[0] if len(attributes) == 1 else attributes\n</code></pre>"},{"location":"reference/#src.meteors.attr.noise_tunnel.NoiseTunnel.attribute","title":"<code>attribute(hsi, target=None, additional_forward_args=None, n_samples=5, steps_per_batch=1, perturbation_axis=None, stdevs=1.0, method='smoothgrad')</code>","text":"<p>Method for generating attributions using the Noise Tunnel method.</p> <p>hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.         If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.         The output will be a list of HSIAttributes objects.     baseline (int | float | torch.Tensor, optional): Baselines define reference value which replaces each         feature when occluded is computed and can be provided as:             - integer or float representing a constant value used as the baseline for all input pixels.             - tensor with the same shape as the input tensor, providing a baseline for each input pixel.                 if the input is a list of HSI objects, the baseline can be a tensor with the same shape as                 the input tensor for each HSI object.     target (list[int] | int | None, optional): target class index for computing the attributions. If None,         methods assume that the output has only one class. If the output has multiple classes, the target index         must be provided. For multiple input images, a list of target indices can be provided, one for each         image or single target value will be used for all images. Defaults to None.     additional_forward_args (Any, optional): If the forward function requires additional arguments other than         the inputs for which attributions should not be computed, this argument can be provided.         It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple         containing multiple additional arguments including tensors or any arbitrary python types.         These arguments are provided to forward_func in order following the arguments in inputs.         Note that attributions are not computed with respect to these arguments. Default: None     n_samples (int, optional): The number of randomly generated examples per sample in the input batch.         Random examples are generated by adding gaussian random noise to each sample.         Default: 5 if nt_samples is not provided.     steps_per_batch (int, optional): The number of the n_samples that will be processed together.         With the help of this parameter we can avoid out of memory situation and reduce the number of randomly         generated examples per sample in each batch. Default: None if steps_per_batch is not provided.         In this case all nt_samples will be processed together.     perturbation_axis (None | tuple[int | slice], optional): The indices of the input image to be perturbed.         If set to None, all bands are perturbed, which corresponds to a traditional noise tunnel method.         Defaults to None.     stdevs (float | tuple[float, ...], optional): The standard deviation of gaussian noise with zero mean that         is added to each input in the batch. If stdevs is a single float value then that same value is used         for all inputs. If stdevs is a tuple, then the length of the tuple must match the number of inputs as         each value in the tuple is used for the corresponding input. Default: 1.0     method (Literal[\"smoothgrad\", \"smoothgrad_sq\", \"vargrad\"], optional): Smoothing type of the attributions.         smoothgrad, smoothgrad_sq or vargrad Default: smoothgrad if type is not provided.</p> <p>Returns:</p> Type Description <code>HSIAttributes | list[HSIAttributes]</code> <p>HSIAttributes | list[HSIAttributes]: The computed attributions for the input hyperspectral image(s). if a list of HSI objects is provided, the attributions are computed for each HSI object in the list.</p> <p>Raises:</p> Type Description <code>HSIAttributesError</code> <p>If an error occurs during the generation of the attributions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; noise_tunnel = NoiseTunnel(explainable_model)\n&gt;&gt;&gt; hsi = HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n&gt;&gt;&gt; attributions = noise_tunnel.attribute(hsi)\n&gt;&gt;&gt; attributions = noise_tunnel.attribute([hsi, hsi])\n&gt;&gt;&gt; len(attributions)\n2\n</code></pre> Source code in <code>src/meteors/attr/noise_tunnel.py</code> <pre><code>def attribute(\n    self,\n    hsi: list[HSI] | HSI,\n    target: list[int] | int | None = None,\n    additional_forward_args: Any = None,\n    n_samples: int = 5,\n    steps_per_batch: int = 1,\n    perturbation_axis: None | tuple[int | slice] = None,\n    stdevs: float | tuple[float, ...] = 1.0,\n    method: Literal[\"smoothgrad\", \"smoothgrad_sq\", \"vargrad\"] = \"smoothgrad\",\n) -&gt; HSIAttributes | list[HSIAttributes]:\n    \"\"\"\n    Method for generating attributions using the Noise Tunnel method.\n\n    hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n            If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n            The output will be a list of HSIAttributes objects.\n        baseline (int | float | torch.Tensor, optional): Baselines define reference value which replaces each\n            feature when occluded is computed and can be provided as:\n                - integer or float representing a constant value used as the baseline for all input pixels.\n                - tensor with the same shape as the input tensor, providing a baseline for each input pixel.\n                    if the input is a list of HSI objects, the baseline can be a tensor with the same shape as\n                    the input tensor for each HSI object.\n        target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n            methods assume that the output has only one class. If the output has multiple classes, the target index\n            must be provided. For multiple input images, a list of target indices can be provided, one for each\n            image or single target value will be used for all images. Defaults to None.\n        additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n            the inputs for which attributions should not be computed, this argument can be provided.\n            It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n            containing multiple additional arguments including tensors or any arbitrary python types.\n            These arguments are provided to forward_func in order following the arguments in inputs.\n            Note that attributions are not computed with respect to these arguments. Default: None\n        n_samples (int, optional): The number of randomly generated examples per sample in the input batch.\n            Random examples are generated by adding gaussian random noise to each sample.\n            Default: 5 if nt_samples is not provided.\n        steps_per_batch (int, optional): The number of the n_samples that will be processed together.\n            With the help of this parameter we can avoid out of memory situation and reduce the number of randomly\n            generated examples per sample in each batch. Default: None if steps_per_batch is not provided.\n            In this case all nt_samples will be processed together.\n        perturbation_axis (None | tuple[int | slice], optional): The indices of the input image to be perturbed.\n            If set to None, all bands are perturbed, which corresponds to a traditional noise tunnel method.\n            Defaults to None.\n        stdevs (float | tuple[float, ...], optional): The standard deviation of gaussian noise with zero mean that\n            is added to each input in the batch. If stdevs is a single float value then that same value is used\n            for all inputs. If stdevs is a tuple, then the length of the tuple must match the number of inputs as\n            each value in the tuple is used for the corresponding input. Default: 1.0\n        method (Literal[\"smoothgrad\", \"smoothgrad_sq\", \"vargrad\"], optional): Smoothing type of the attributions.\n            smoothgrad, smoothgrad_sq or vargrad Default: smoothgrad if type is not provided.\n\n    Returns:\n        HSIAttributes | list[HSIAttributes]: The computed attributions for the input hyperspectral image(s).\n            if a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n\n    Raises:\n        HSIAttributesError: If an error occurs during the generation of the attributions.\n\n    Examples:\n        &gt;&gt;&gt; noise_tunnel = NoiseTunnel(explainable_model)\n        &gt;&gt;&gt; hsi = HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n        &gt;&gt;&gt; attributions = noise_tunnel.attribute(hsi)\n        &gt;&gt;&gt; attributions = noise_tunnel.attribute([hsi, hsi])\n        &gt;&gt;&gt; len(attributions)\n        2\n    \"\"\"\n    if isinstance(stdevs, list):\n        stdevs = tuple(stdevs)\n\n    if not isinstance(hsi, list):\n        hsi = [hsi]\n\n    if not all([isinstance(input, HSI) for input in hsi]):\n        raise TypeError(\"All inputs must be HSI objects\")\n\n    if isinstance(stdevs, tuple):\n        if len(stdevs) != len(hsi):\n            raise ValueError(\n                \"The number of stdevs must match the number of input images, number of stdevs:\"\n                f\"{len(stdevs)}, number of input images: {len(hsi)}\"\n            )\n    else:\n        stdevs = tuple([stdevs] * len(hsi))\n\n    if not isinstance(target, list):\n        target = [target] * len(hsi)  # type: ignore\n\n    nt_attributes = torch.empty((n_samples, len(hsi)) + hsi[0].image.shape, device=hsi[0].device)\n\n    for batch in range(0, len(hsi)):\n        input = hsi[batch]\n        targeted = target[batch]\n        stdev = stdevs[batch]\n        perturbed_input = self.perturb_input(input.image, n_samples, perturbation_axis, stdev)\n        nt_attributes[:, batch] = self._forward_loop(\n            perturbed_input, input, targeted, additional_forward_args, n_samples, steps_per_batch\n        )\n\n    nt_attributes = self._aggregate_attributions(nt_attributes, method)\n\n    try:\n        attributes = [\n            HSIAttributes(hsi=hsi_image, attributes=attribution, attribution_method=self.get_name())\n            for hsi_image, attribution in zip(hsi, nt_attributes)\n        ]\n    except Exception as e:\n        raise HSIAttributesError(f\"Error in generating NoiseTunnel attributions: {e}\") from e\n\n    return attributes[0] if len(attributes) == 1 else attributes\n</code></pre>"},{"location":"reference/#src.meteors.attr.noise_tunnel.NoiseTunnel.perturb_input","title":"<code>perturb_input(input, n_samples=1, perturbation_axis=None, stdevs=1, **kwargs)</code>  <code>staticmethod</code>","text":"<p>The default perturbation function used in the noise tunnel with small enhancement for hyperspectral images. It randomly adds noise to the input tensor from a normal distribution with a given standard deviation. The noise is added to the selected bands (channels) of the input tensor. The bands to be perturbed are selected based on the <code>perturbation_axis</code> parameter. By default all bands are perturbed, which is equivalent to the standard noise tunnel method.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>An input tensor to be perturbed. It should have the shape (C, H, W).</p> required <code>n_samples</code> <code>int</code> <p>A number of samples to be drawn - number of perturbed inputs to be generated.</p> <code>1</code> <code>perturbation_axis</code> <code>None | tuple[int | slice]</code> <p>The indices of the bands to be perturbed. If set to None, all bands are perturbed. Defaults to None.</p> <code>None</code> <code>stdevs</code> <code>float</code> <p>The standard deviation of gaussian noise with zero mean that is added to each input in the batch. Defaults to 1.0.</p> <code>1</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A perturbed tensor, which contains <code>n_samples</code> perturbed inputs.</p> Source code in <code>src/meteors/attr/noise_tunnel.py</code> <pre><code>@staticmethod\ndef perturb_input(\n    input: torch.Tensor,\n    n_samples: int = 1,\n    perturbation_axis: None | tuple[int | slice] = None,\n    stdevs: float = 1,\n    **kwargs: Any,\n) -&gt; torch.Tensor:\n    \"\"\"\n    The default perturbation function used in the noise tunnel with small enhancement for hyperspectral images.\n    It randomly adds noise to the input tensor from a normal distribution with a given standard deviation.\n    The noise is added to the selected bands (channels) of the input tensor.\n    The bands to be perturbed are selected based on the `perturbation_axis` parameter.\n    By default all bands are perturbed, which is equivalent to the standard noise tunnel method.\n\n    Args:\n        input (torch.Tensor): An input tensor to be perturbed. It should have the shape (C, H, W).\n        n_samples (int): A number of samples to be drawn - number of perturbed inputs to be generated.\n        perturbation_axis (None | tuple[int | slice]): The indices of the bands to be perturbed.\n            If set to None, all bands are perturbed. Defaults to None.\n        stdevs (float): The standard deviation of gaussian noise with zero mean that is added to each input\n            in the batch. Defaults to 1.0.\n\n    Returns:\n        torch.Tensor: A perturbed tensor, which contains `n_samples` perturbed inputs.\n    \"\"\"\n    if n_samples &lt; 1:\n        raise ValueError(\"Number of perturbated samples to be generated must be greater than 0\")\n\n    # the perturbation\n    perturbed_input = input.clone().unsqueeze(0)\n    # repeat the perturbed_input on the first dimension n_samples times\n    perturbed_input = perturbed_input.repeat_interleave(n_samples, dim=0)\n\n    # the perturbation shape\n    if perturbation_axis is None:\n        perturbation_shape = perturbed_input.shape\n    else:\n        perturbation_axis = (slice(None),) + perturbation_axis  # type: ignore\n        perturbation_shape = perturbed_input[perturbation_axis].shape\n\n    # the noise\n    noise = torch.normal(0, stdevs, size=perturbation_shape).to(input.device)\n\n    # add the noise to the perturbed_input\n    if perturbation_axis is None:\n        perturbed_input += noise\n    else:\n        perturbed_input[perturbation_axis] += noise\n\n    perturbed_input.requires_grad_(True)\n\n    return perturbed_input\n</code></pre>"},{"location":"reference/#src.meteors.attr.noise_tunnel.HyperNoiseTunnel","title":"<code>HyperNoiseTunnel</code>","text":"<p>               Bases: <code>BaseNoiseTunnel</code></p> <p>Hyper Noise Tunnel is our novel method, designed specifically to explain hyperspectral satellite images. It is inspired by the behaviour of the classical Noise Tunnel (Smooth Grad) method, but instead of sampling noise into the original image, it randomly masks some of the bands with the baseline. In the process, the created noised samples are close to the distribution of the original image yet differ enough to smoothen the produced attribution map.</p> <p>Parameters:</p> Name Type Description Default <code>chained_explainer</code> <p>The explainable method that will be used to compute the attributions.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the callable object is not an instance of the Explainer class</p> Source code in <code>src/meteors/attr/noise_tunnel.py</code> <pre><code>class HyperNoiseTunnel(BaseNoiseTunnel):\n    \"\"\"Hyper Noise Tunnel is our novel method, designed specifically to explain hyperspectral satellite images. It is\n    inspired by the behaviour of the classical Noise Tunnel (Smooth Grad) method, but instead of sampling noise into the\n    original image, it randomly masks some of the bands with the baseline. In the process, the created _noised_ samples\n    are close to the distribution of the original image yet differ enough to smoothen the produced attribution map.\n\n    Arguments:\n        chained_explainer: The explainable method that will be used to compute the attributions.\n\n    Raises:\n        RuntimeError: If the callable object is not an instance of the Explainer class\n    \"\"\"\n\n    @staticmethod\n    def perturb_input(\n        input: torch.Tensor,\n        baseline: torch.Tensor | None = None,\n        n_samples: int = 1,\n        perturbation_prob: float = 0.5,\n        num_perturbed_bands: int | None = None,\n        **kwargs: Any,\n    ) -&gt; torch.Tensor:\n        \"\"\"The perturbation function used in the hyper noise tunnel. It randomly selects a subset of the input bands\n        that will be masked out and replaced with the baseline. The parameters `num_perturbed_bands` and\n        `perturbation_prob` control the number of bands that will be perturbed (masked). If `num_perturbed_bands` is\n        set, it will be used as the number of bands to perturb, which will be randomly selected. Otherwise, the number\n        of bands will be drawn from a binomial distribution with `perturbation_prob` as the probability of success.\n\n        Args:\n            input (torch.Tensor): An input tensor to be perturbed. It should have the shape (C, H, W).\n            baseline (torch.Tensor | None, optional): A tensor that will be used to replace the perturbed bands.\n            n_samples (int): A number of samples to be drawn - number of perturbed inputs to be generated.\n            perturbation_prob (float, optional): A probability that each band will be perturbed intependently.\n                Defaults to 0.5.\n            num_perturbed_bands (int | None, optional): A number of perturbed bands in the whole image.\n                If set to None, the bands are perturbed with probability `perturbation_prob` each. Defaults to None.\n\n        Returns:\n            torch.Tensor: A perturbed tensor, which contains `n_samples` perturbed inputs.\n        \"\"\"\n        # validate the baseline against the input\n        if baseline is None:\n            raise ValueError(\"Baseline must be provided for the HyperNoiseTunnel method\")\n\n        if baseline.shape != input.shape:\n            raise ShapeMismatchError(f\"Baseline shape {baseline.shape} does not match input shape {input.shape}\")\n\n        if n_samples &lt; 1:\n            raise ValueError(\"Number of perturbated samples to be generated must be greater than 0\")\n\n        if perturbation_prob &lt; 0 or perturbation_prob &gt; 1:\n            raise ValueError(\"Perturbation probability must be in the range [0, 1]\")\n\n        # the perturbation\n        perturbed_input = input.clone().unsqueeze(0)\n        # repeat the perturbed_input on the first dimension n_samples times\n        perturbed_input = perturbed_input.repeat_interleave(n_samples, dim=0)\n\n        n_samples_x_channels_shape = (\n            n_samples,\n            input.shape[0],\n        )  # shape of the tensor containing the perturbed channels for each sample\n\n        channels_to_be_perturbed: torch.Tensor = torch.zeros(n_samples_x_channels_shape, device=input.device).bool()\n\n        if num_perturbed_bands is None:\n            channel_perturbation_probabilities = (\n                torch.ones(n_samples_x_channels_shape, device=input.device) * perturbation_prob\n            )\n            channels_to_be_perturbed = torch.bernoulli(channel_perturbation_probabilities).bool()\n\n        else:\n            if num_perturbed_bands &lt; 0 or num_perturbed_bands &gt; input.shape[0]:\n                raise ValueError(\n                    f\"Cannot perturb {num_perturbed_bands} bands in the input with {input.shape[0]} channels. The number of perturbed bands must be in the range [0, {input.shape[0]}]\"\n                )\n\n            channels_to_be_perturbed = torch_random_choice(\n                input.shape[0], num_perturbed_bands, n_samples, device=input.device\n            )\n\n        # now having chosen the perturbed channels, we can replace them with the baseline\n        reshaped_baseline = baseline.unsqueeze(0).repeat_interleave(n_samples, dim=0)\n        perturbed_input[channels_to_be_perturbed] = reshaped_baseline[channels_to_be_perturbed]\n\n        perturbed_input.requires_grad_(True)\n\n        return perturbed_input\n\n    def attribute(\n        self,\n        hsi: list[HSI] | HSI,\n        baselines: int | float | torch.Tensor | list[int | float | torch.Tensor] | None = None,\n        target: list[int] | int | None = None,\n        additional_forward_args: Any = None,\n        n_samples: int = 5,\n        steps_per_batch: int = 1,\n        perturbation_prob: float = 0.5,\n        num_perturbed_bands: int | None = None,\n        method: Literal[\"smoothgrad\", \"smoothgrad_sq\", \"vargrad\"] = \"smoothgrad\",\n    ) -&gt; HSIAttributes | list[HSIAttributes]:\n        \"\"\"\n        Method for generating attributions using the Hyper Noise Tunnel method.\n\n        Args:\n            hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n                If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n                The output will be a list of HSIAttributes objects.\n            baseline (int | float | torch.Tensor | list[int | float | torch.Tensor], optional): Baselines define reference value which\n                replaces each feature when occluded is computed and can be provided as:\n                    - integer or float representing a constant value used as the baseline for all input pixels.\n                    - tensor with the same shape as the input tensor, providing a baseline for each input pixel.\n                        if the input is a list of HSI objects, the baseline can be a tensor with the same shape as\n                        the input tensor for each HSI object or a list of tensors with the same length as the input list.\n            target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n                methods assume that the output has only one class. If the output has multiple classes, the target index\n                must be provided. For multiple input images, a list of target indices can be provided, one for each\n                image or single target value will be used for all images. Defaults to None.\n            additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n                the inputs for which attributions should not be computed, this argument can be provided.\n                It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n                containing multiple additional arguments including tensors or any arbitrary python types.\n                These arguments are provided to forward_func in order following the arguments in inputs.\n                Note that attributions are not computed with respect to these arguments. Default: None\n            n_samples (int, optional):The number of randomly generated examples per sample in the input batch.\n                Random examples are generated by adding gaussian random noise to each sample.\n                Default: 5 if nt_samples is not provided.\n            steps_per_batch (int, optional): The number of the n_samples that will be processed together.\n                With the help of this parameter we can avoid out of memory situation and reduce the number of randomly\n                generated examples per sample in each batch. Default: None if steps_per_batch is not provided.\n                In this case all nt_samples will be processed together.\n            perturbation_prob (float, optional): The probability that each band will be perturbed independently.\n                Defaults to 0.5.\n            num_perturbed_bands (int | None, optional): The number of perturbed bands in the whole image.\n                The bands to be perturbed are selected randomly with no replacement.\n                If set to None, the bands are perturbed with probability `perturbation_prob` each. Defaults to None.\n            method (Literal[\"smoothgrad\", \"smoothgrad_sq\", \"vargrad\"], optional): Smoothing type of the attributions.\n                smoothgrad, smoothgrad_sq or vargrad Default: smoothgrad if type is not provided.\n\n        Returns:\n            HSIAttributes | list[HSIAttributes]: The computed attributions for the input hyperspectral image(s).\n                if a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n\n        Raises:\n            HSIAttributesError: If an error occurs during the generation of the attributions.\n\n        Examples:\n            &gt;&gt;&gt; hyper_noise_tunnel = HyperNoiseTunnel(explainable_model)\n            &gt;&gt;&gt; hsi = HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n            &gt;&gt;&gt; attributions = hyper_noise_tunnel.attribute(hsi)\n            &gt;&gt;&gt; attributions = hyper_noise_tunnel.attribute([hsi, hsi])\n            &gt;&gt;&gt; len(attributions)\n            2\n        \"\"\"\n        change_orientation = []\n        original_orientation = []\n\n        if not isinstance(hsi, list):\n            hsi = [hsi]\n\n        if not all([isinstance(input, HSI) for input in hsi]):\n            raise TypeError(\"All inputs must be HSI objects\")\n\n        for i in range(len(hsi)):\n            if hsi[i].orientation != (\"C\", \"H\", \"W\"):\n                change_orientation.append(True)\n                original_orientation.append(hsi[i].orientation)\n                hsi[i] = hsi[i].change_orientation(\"CHW\")\n            else:\n                change_orientation.append(False)\n\n        if not isinstance(baselines, list):\n            baselines = [baselines] * len(hsi)\n        elif len(baselines) != len(hsi):\n            raise ValueError(\"The number of baselines must match the number of input images\")\n\n        baselines = [\n            validate_and_transform_baseline(baseline, hsi_image) for baseline, hsi_image in zip(baselines, hsi)\n        ]\n\n        if not isinstance(target, list):\n            target = [target] * len(hsi)  # type: ignore\n\n        hnt_attributes = torch.empty((n_samples, len(hsi)) + hsi[0].image.shape, device=hsi[0].device)\n        for batch in range(0, len(hsi)):\n            input = hsi[batch]\n            targeted = target[batch]\n            baseline = baselines[batch]\n            perturbed_input = self.perturb_input(\n                input.image, baseline, n_samples, perturbation_prob, num_perturbed_bands\n            )\n            hnt_attributes[:, batch] = self._forward_loop(\n                perturbed_input, input, targeted, additional_forward_args, n_samples, steps_per_batch\n            )\n\n        hnt_attributes = self._aggregate_attributions(hnt_attributes, method)\n\n        try:\n            attributes = [\n                HSIAttributes(hsi=hsi_image, attributes=attribution, attribution_method=self.get_name())\n                for hsi_image, attribution in zip(hsi, hnt_attributes)\n            ]\n        except Exception as e:\n            raise HSIAttributesError(f\"Error in generating HyperNoiseTunnel attributions: {e}\") from e\n\n        for i in range(len(change_orientation)):\n            if change_orientation[i]:\n                attributes[i].hsi = attributes[i].hsi.change_orientation(original_orientation[i])\n\n        return attributes[0] if len(attributes) == 1 else attributes\n</code></pre>"},{"location":"reference/#src.meteors.attr.noise_tunnel.HyperNoiseTunnel.attribute","title":"<code>attribute(hsi, baselines=None, target=None, additional_forward_args=None, n_samples=5, steps_per_batch=1, perturbation_prob=0.5, num_perturbed_bands=None, method='smoothgrad')</code>","text":"<p>Method for generating attributions using the Hyper Noise Tunnel method.</p> <p>Parameters:</p> Name Type Description Default <code>hsi</code> <code>list[HSI] | HSI</code> <p>Input hyperspectral image(s) for which the attributions are to be computed. If a list of HSI objects is provided, the attributions are computed for each HSI object in the list. The output will be a list of HSIAttributes objects.</p> required <code>baseline</code> <code>int | float | Tensor | list[int | float | Tensor]</code> <p>Baselines define reference value which replaces each feature when occluded is computed and can be provided as:     - integer or float representing a constant value used as the baseline for all input pixels.     - tensor with the same shape as the input tensor, providing a baseline for each input pixel.         if the input is a list of HSI objects, the baseline can be a tensor with the same shape as         the input tensor for each HSI object or a list of tensors with the same length as the input list.</p> required <code>target</code> <code>list[int] | int | None</code> <p>target class index for computing the attributions. If None, methods assume that the output has only one class. If the output has multiple classes, the target index must be provided. For multiple input images, a list of target indices can be provided, one for each image or single target value will be used for all images. Defaults to None.</p> <code>None</code> <code>additional_forward_args</code> <code>Any</code> <p>If the forward function requires additional arguments other than the inputs for which attributions should not be computed, this argument can be provided. It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple containing multiple additional arguments including tensors or any arbitrary python types. These arguments are provided to forward_func in order following the arguments in inputs. Note that attributions are not computed with respect to these arguments. Default: None</p> <code>None</code> <code>n_samples</code> <code>int</code> <p>The number of randomly generated examples per sample in the input batch. Random examples are generated by adding gaussian random noise to each sample. Default: 5 if nt_samples is not provided.</p> <code>5</code> <code>steps_per_batch</code> <code>int</code> <p>The number of the n_samples that will be processed together. With the help of this parameter we can avoid out of memory situation and reduce the number of randomly generated examples per sample in each batch. Default: None if steps_per_batch is not provided. In this case all nt_samples will be processed together.</p> <code>1</code> <code>perturbation_prob</code> <code>float</code> <p>The probability that each band will be perturbed independently. Defaults to 0.5.</p> <code>0.5</code> <code>num_perturbed_bands</code> <code>int | None</code> <p>The number of perturbed bands in the whole image. The bands to be perturbed are selected randomly with no replacement. If set to None, the bands are perturbed with probability <code>perturbation_prob</code> each. Defaults to None.</p> <code>None</code> <code>method</code> <code>Literal['smoothgrad', 'smoothgrad_sq', 'vargrad']</code> <p>Smoothing type of the attributions. smoothgrad, smoothgrad_sq or vargrad Default: smoothgrad if type is not provided.</p> <code>'smoothgrad'</code> <p>Returns:</p> Type Description <code>HSIAttributes | list[HSIAttributes]</code> <p>HSIAttributes | list[HSIAttributes]: The computed attributions for the input hyperspectral image(s). if a list of HSI objects is provided, the attributions are computed for each HSI object in the list.</p> <p>Raises:</p> Type Description <code>HSIAttributesError</code> <p>If an error occurs during the generation of the attributions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hyper_noise_tunnel = HyperNoiseTunnel(explainable_model)\n&gt;&gt;&gt; hsi = HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n&gt;&gt;&gt; attributions = hyper_noise_tunnel.attribute(hsi)\n&gt;&gt;&gt; attributions = hyper_noise_tunnel.attribute([hsi, hsi])\n&gt;&gt;&gt; len(attributions)\n2\n</code></pre> Source code in <code>src/meteors/attr/noise_tunnel.py</code> <pre><code>def attribute(\n    self,\n    hsi: list[HSI] | HSI,\n    baselines: int | float | torch.Tensor | list[int | float | torch.Tensor] | None = None,\n    target: list[int] | int | None = None,\n    additional_forward_args: Any = None,\n    n_samples: int = 5,\n    steps_per_batch: int = 1,\n    perturbation_prob: float = 0.5,\n    num_perturbed_bands: int | None = None,\n    method: Literal[\"smoothgrad\", \"smoothgrad_sq\", \"vargrad\"] = \"smoothgrad\",\n) -&gt; HSIAttributes | list[HSIAttributes]:\n    \"\"\"\n    Method for generating attributions using the Hyper Noise Tunnel method.\n\n    Args:\n        hsi (list[HSI] | HSI): Input hyperspectral image(s) for which the attributions are to be computed.\n            If a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n            The output will be a list of HSIAttributes objects.\n        baseline (int | float | torch.Tensor | list[int | float | torch.Tensor], optional): Baselines define reference value which\n            replaces each feature when occluded is computed and can be provided as:\n                - integer or float representing a constant value used as the baseline for all input pixels.\n                - tensor with the same shape as the input tensor, providing a baseline for each input pixel.\n                    if the input is a list of HSI objects, the baseline can be a tensor with the same shape as\n                    the input tensor for each HSI object or a list of tensors with the same length as the input list.\n        target (list[int] | int | None, optional): target class index for computing the attributions. If None,\n            methods assume that the output has only one class. If the output has multiple classes, the target index\n            must be provided. For multiple input images, a list of target indices can be provided, one for each\n            image or single target value will be used for all images. Defaults to None.\n        additional_forward_args (Any, optional): If the forward function requires additional arguments other than\n            the inputs for which attributions should not be computed, this argument can be provided.\n            It must be either a single additional argument of a Tensor or arbitrary (non-tuple) type or a tuple\n            containing multiple additional arguments including tensors or any arbitrary python types.\n            These arguments are provided to forward_func in order following the arguments in inputs.\n            Note that attributions are not computed with respect to these arguments. Default: None\n        n_samples (int, optional):The number of randomly generated examples per sample in the input batch.\n            Random examples are generated by adding gaussian random noise to each sample.\n            Default: 5 if nt_samples is not provided.\n        steps_per_batch (int, optional): The number of the n_samples that will be processed together.\n            With the help of this parameter we can avoid out of memory situation and reduce the number of randomly\n            generated examples per sample in each batch. Default: None if steps_per_batch is not provided.\n            In this case all nt_samples will be processed together.\n        perturbation_prob (float, optional): The probability that each band will be perturbed independently.\n            Defaults to 0.5.\n        num_perturbed_bands (int | None, optional): The number of perturbed bands in the whole image.\n            The bands to be perturbed are selected randomly with no replacement.\n            If set to None, the bands are perturbed with probability `perturbation_prob` each. Defaults to None.\n        method (Literal[\"smoothgrad\", \"smoothgrad_sq\", \"vargrad\"], optional): Smoothing type of the attributions.\n            smoothgrad, smoothgrad_sq or vargrad Default: smoothgrad if type is not provided.\n\n    Returns:\n        HSIAttributes | list[HSIAttributes]: The computed attributions for the input hyperspectral image(s).\n            if a list of HSI objects is provided, the attributions are computed for each HSI object in the list.\n\n    Raises:\n        HSIAttributesError: If an error occurs during the generation of the attributions.\n\n    Examples:\n        &gt;&gt;&gt; hyper_noise_tunnel = HyperNoiseTunnel(explainable_model)\n        &gt;&gt;&gt; hsi = HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n        &gt;&gt;&gt; attributions = hyper_noise_tunnel.attribute(hsi)\n        &gt;&gt;&gt; attributions = hyper_noise_tunnel.attribute([hsi, hsi])\n        &gt;&gt;&gt; len(attributions)\n        2\n    \"\"\"\n    change_orientation = []\n    original_orientation = []\n\n    if not isinstance(hsi, list):\n        hsi = [hsi]\n\n    if not all([isinstance(input, HSI) for input in hsi]):\n        raise TypeError(\"All inputs must be HSI objects\")\n\n    for i in range(len(hsi)):\n        if hsi[i].orientation != (\"C\", \"H\", \"W\"):\n            change_orientation.append(True)\n            original_orientation.append(hsi[i].orientation)\n            hsi[i] = hsi[i].change_orientation(\"CHW\")\n        else:\n            change_orientation.append(False)\n\n    if not isinstance(baselines, list):\n        baselines = [baselines] * len(hsi)\n    elif len(baselines) != len(hsi):\n        raise ValueError(\"The number of baselines must match the number of input images\")\n\n    baselines = [\n        validate_and_transform_baseline(baseline, hsi_image) for baseline, hsi_image in zip(baselines, hsi)\n    ]\n\n    if not isinstance(target, list):\n        target = [target] * len(hsi)  # type: ignore\n\n    hnt_attributes = torch.empty((n_samples, len(hsi)) + hsi[0].image.shape, device=hsi[0].device)\n    for batch in range(0, len(hsi)):\n        input = hsi[batch]\n        targeted = target[batch]\n        baseline = baselines[batch]\n        perturbed_input = self.perturb_input(\n            input.image, baseline, n_samples, perturbation_prob, num_perturbed_bands\n        )\n        hnt_attributes[:, batch] = self._forward_loop(\n            perturbed_input, input, targeted, additional_forward_args, n_samples, steps_per_batch\n        )\n\n    hnt_attributes = self._aggregate_attributions(hnt_attributes, method)\n\n    try:\n        attributes = [\n            HSIAttributes(hsi=hsi_image, attributes=attribution, attribution_method=self.get_name())\n            for hsi_image, attribution in zip(hsi, hnt_attributes)\n        ]\n    except Exception as e:\n        raise HSIAttributesError(f\"Error in generating HyperNoiseTunnel attributions: {e}\") from e\n\n    for i in range(len(change_orientation)):\n        if change_orientation[i]:\n            attributes[i].hsi = attributes[i].hsi.change_orientation(original_orientation[i])\n\n    return attributes[0] if len(attributes) == 1 else attributes\n</code></pre>"},{"location":"reference/#src.meteors.attr.noise_tunnel.HyperNoiseTunnel.perturb_input","title":"<code>perturb_input(input, baseline=None, n_samples=1, perturbation_prob=0.5, num_perturbed_bands=None, **kwargs)</code>  <code>staticmethod</code>","text":"<p>The perturbation function used in the hyper noise tunnel. It randomly selects a subset of the input bands that will be masked out and replaced with the baseline. The parameters <code>num_perturbed_bands</code> and <code>perturbation_prob</code> control the number of bands that will be perturbed (masked). If <code>num_perturbed_bands</code> is set, it will be used as the number of bands to perturb, which will be randomly selected. Otherwise, the number of bands will be drawn from a binomial distribution with <code>perturbation_prob</code> as the probability of success.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>An input tensor to be perturbed. It should have the shape (C, H, W).</p> required <code>baseline</code> <code>Tensor | None</code> <p>A tensor that will be used to replace the perturbed bands.</p> <code>None</code> <code>n_samples</code> <code>int</code> <p>A number of samples to be drawn - number of perturbed inputs to be generated.</p> <code>1</code> <code>perturbation_prob</code> <code>float</code> <p>A probability that each band will be perturbed intependently. Defaults to 0.5.</p> <code>0.5</code> <code>num_perturbed_bands</code> <code>int | None</code> <p>A number of perturbed bands in the whole image. If set to None, the bands are perturbed with probability <code>perturbation_prob</code> each. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A perturbed tensor, which contains <code>n_samples</code> perturbed inputs.</p> Source code in <code>src/meteors/attr/noise_tunnel.py</code> <pre><code>@staticmethod\ndef perturb_input(\n    input: torch.Tensor,\n    baseline: torch.Tensor | None = None,\n    n_samples: int = 1,\n    perturbation_prob: float = 0.5,\n    num_perturbed_bands: int | None = None,\n    **kwargs: Any,\n) -&gt; torch.Tensor:\n    \"\"\"The perturbation function used in the hyper noise tunnel. It randomly selects a subset of the input bands\n    that will be masked out and replaced with the baseline. The parameters `num_perturbed_bands` and\n    `perturbation_prob` control the number of bands that will be perturbed (masked). If `num_perturbed_bands` is\n    set, it will be used as the number of bands to perturb, which will be randomly selected. Otherwise, the number\n    of bands will be drawn from a binomial distribution with `perturbation_prob` as the probability of success.\n\n    Args:\n        input (torch.Tensor): An input tensor to be perturbed. It should have the shape (C, H, W).\n        baseline (torch.Tensor | None, optional): A tensor that will be used to replace the perturbed bands.\n        n_samples (int): A number of samples to be drawn - number of perturbed inputs to be generated.\n        perturbation_prob (float, optional): A probability that each band will be perturbed intependently.\n            Defaults to 0.5.\n        num_perturbed_bands (int | None, optional): A number of perturbed bands in the whole image.\n            If set to None, the bands are perturbed with probability `perturbation_prob` each. Defaults to None.\n\n    Returns:\n        torch.Tensor: A perturbed tensor, which contains `n_samples` perturbed inputs.\n    \"\"\"\n    # validate the baseline against the input\n    if baseline is None:\n        raise ValueError(\"Baseline must be provided for the HyperNoiseTunnel method\")\n\n    if baseline.shape != input.shape:\n        raise ShapeMismatchError(f\"Baseline shape {baseline.shape} does not match input shape {input.shape}\")\n\n    if n_samples &lt; 1:\n        raise ValueError(\"Number of perturbated samples to be generated must be greater than 0\")\n\n    if perturbation_prob &lt; 0 or perturbation_prob &gt; 1:\n        raise ValueError(\"Perturbation probability must be in the range [0, 1]\")\n\n    # the perturbation\n    perturbed_input = input.clone().unsqueeze(0)\n    # repeat the perturbed_input on the first dimension n_samples times\n    perturbed_input = perturbed_input.repeat_interleave(n_samples, dim=0)\n\n    n_samples_x_channels_shape = (\n        n_samples,\n        input.shape[0],\n    )  # shape of the tensor containing the perturbed channels for each sample\n\n    channels_to_be_perturbed: torch.Tensor = torch.zeros(n_samples_x_channels_shape, device=input.device).bool()\n\n    if num_perturbed_bands is None:\n        channel_perturbation_probabilities = (\n            torch.ones(n_samples_x_channels_shape, device=input.device) * perturbation_prob\n        )\n        channels_to_be_perturbed = torch.bernoulli(channel_perturbation_probabilities).bool()\n\n    else:\n        if num_perturbed_bands &lt; 0 or num_perturbed_bands &gt; input.shape[0]:\n            raise ValueError(\n                f\"Cannot perturb {num_perturbed_bands} bands in the input with {input.shape[0]} channels. The number of perturbed bands must be in the range [0, {input.shape[0]}]\"\n            )\n\n        channels_to_be_perturbed = torch_random_choice(\n            input.shape[0], num_perturbed_bands, n_samples, device=input.device\n        )\n\n    # now having chosen the perturbed channels, we can replace them with the baseline\n    reshaped_baseline = baseline.unsqueeze(0).repeat_interleave(n_samples, dim=0)\n    perturbed_input[channels_to_be_perturbed] = reshaped_baseline[channels_to_be_perturbed]\n\n    perturbed_input.requires_grad_(True)\n\n    return perturbed_input\n</code></pre>"},{"location":"tutorials/introduction/","title":"\ud83c\udf93 Introduction to Tutorials","text":"<p>Welcome to the Meteors tutorials! These tutorials are designed to help you get started with using Meteors for explaining and visualizing hyperspectral and multispectral images. Whether you're new to Meteors or have some experience, these tutorials will guide you through various features and techniques step by step.</p>"},{"location":"tutorials/introduction/#tutorial-list","title":"\ud83d\udcda Tutorial List","text":"<ol> <li> <p>LIME</p> </li> <li> <p>HYPERVIEW Challenge</p> </li> <li>Local Interpretable Model-agnostic Explanations (LIME) for hyperspectral images</li> <li>Vision Transformer (ViT) model for hyperspectral image regression</li> </ol>"},{"location":"tutorials/introduction/#getting-started","title":"\ud83d\ude80 Getting Started","text":"<p>To get started with the tutorials, make sure you have Meteors installed and set up on your machine. If you haven't done so already, follow the installation instructions in the Quickstart tutorial.</p> <p>Each tutorial is self-contained and focuses on a specific topic or technique. You can follow the tutorials in order or jump to the ones that interest you the most. The tutorials provide step-by-step instructions, code examples, and explanations to help you understand and apply the concepts effectively.</p>"},{"location":"tutorials/introduction/#tips-and-tricks","title":"\ud83d\udca1 Tips and Tricks","text":"<ul> <li>Take your time and experiment with the code examples provided in the tutorials.</li> <li>Don't hesitate to modify the code and try different parameters to see how they affect the results.</li> <li>Refer to the API Reference for detailed information on the Meteors functions and classes used in the tutorials.</li> <li>If you encounter any issues or have questions, feel free to reach out to the Meteors community or open an issue on the GitHub repository.</li> </ul>"},{"location":"tutorials/introduction/#lets-get-started","title":"\ud83c\udf89 Let's Get Started!","text":"<p>Ready to dive into the world of hyperspectral image explanation with Meteors? Choose a tutorial from the list above and start your journey!</p> <p>Happy learning and exploring with Meteors! \u2604\ufe0f\ud83d\udd0d\u2728</p>"},{"location":"tutorials/lime/","title":"Explanations for cloud segmentation","text":""},{"location":"tutorials/lime/#introduction","title":"Introduction","text":"<p>The selected model for performing the cloud segmentation is UNetMobV2_V1, which is work based on the paper <code>CloudSEN12</code>, a global dataset for semantic understanding of cloud and cloud shadow in <code>Sentinel-2</code> Aybar et. al.</p>"},{"location":"tutorials/lime/#environment-preparation","title":"Environment preparation","text":"<p>In the folder <code>cloudsen12_models</code> there exists code sourced from repository https://github.com/Fersoil/cloudsen12_models/tree/main in which there is slightly modified model source that has been tailored to better work with torch tensors.</p> <p>The only requirement that should suffice for running the code below is installed <code>meteors</code> package.</p> <p>Additionally, in order to download the Sentinel-2 images, you may need to authentificate using the Google Earth Engine.</p> <p>The image and model preparation was sourced from this notebook</p> <pre><code>import os\nfrom cloudsen12_models import cloudsen12\nimport ee\nimport matplotlib.pyplot as plt\nfrom georeader import plot\nfrom shapely.geometry import box\nfrom georeader.readers import ee_image\n\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n</code></pre> <pre><code>c:\\Users\\tymot\\miniconda3\\envs\\xai-tutorial\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</code></pre> <pre><code># ee.Authenticate() # if running the notebook for the first time, uncomment this line\nee.Initialize(project=\"cloud-segmentation-tkwiecinski\")  # you probably would like to provide your own project name\n</code></pre> <pre><code>collection_name = \"COPERNICUS/S2_HARMONIZED\"\ntile = \"S2A_MSIL1C_20240417T064631_N0510_R020_T40RCN_20240417T091941\"\nimg_col = ee.ImageCollection(collection_name)\nimage = img_col.filter(ee.Filter.eq(\"PRODUCT_ID\", tile)).first()\ninfo_img = image.getInfo()\n</code></pre> <pre><code>%%time\n\n# projgee = image.select(\"B2\").projection().getInfo()\n\naoi = box(55.325, 25.225, 55.415, 25.28)\n\nbands = [\"B1\", \"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"B8\", \"B8A\", \"B9\", \"B10\", \"B11\", \"B12\"]\ncrs = info_img[\"bands\"][1][\"crs\"]\ntransform = info_img[\"bands\"][1][\"crs_transform\"]\nprojgee = {\"crs\": crs, \"transform\": transform}\n# img_local = ee_image.export_image_fast(image=image, geometry=aoi)\nimg_local = ee_image.export_image_getpixels(asset_id=info_img[\"id\"], proj=projgee, bands_gee=bands, geometry=aoi)\n</code></pre> <pre><code>CPU times: total: 93.8 ms\nWall time: 3.52 s\n</code></pre> <pre><code>swirnirred = (img_local.isel({\"band\": [bands.index(b) for b in [\"B11\", \"B8\", \"B4\"]]}) / 4_500.0).clip(0, 1)\n\nplot.show(swirnirred)\n</code></pre> <pre><code>&lt;Axes: &gt;\n</code></pre> <p></p>"},{"location":"tutorials/lime/#load-the-model-unetmobv2_v1","title":"Load the model - UNetMobV2_V1","text":"<pre><code>model = cloudsen12.load_model_by_name(name=\"UNetMobV2_V1\", weights_folder=\"cloudsen12_models\")\n</code></pre> <pre><code>c:\\Users\\tymot\\Documents\\praca\\pineapple\\meteors\\examples\\lime\\cloudsen12_models\\cloudsen12.py:193: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  weights = torch.load(weights_file, map_location=device)\n</code></pre> <pre><code>cloudsen12.MODELS_CLOUDSEN12.keys()\n</code></pre> <pre><code>dict_keys(['cloudsen12', 'UNetMobV2_V1', 'UNetMobV2_V2', 'cloudsen12l2a', 'dtacs4bands', 'landsat30'])\n</code></pre> <pre><code>cloudmask = model.predict(img_local / 10_000)\n</code></pre> <pre><code>fig, ax = plt.subplots(1, 2, figsize=(14, 5), sharey=True, tight_layout=True)\n\nplot.show(swirnirred, ax=ax[0])\ncloudsen12.plot_cloudSEN12mask(cloudmask, ax=ax[1])\nfig.suptitle(\"Cloud mask generated by UNetMobV2_V1\")\n</code></pre> <pre><code>Text(0.5, 0.98, 'Cloud mask generated by UNetMobV2_V1')\n</code></pre>"},{"location":"tutorials/lime/#explanations","title":"Explanations","text":"<pre><code>import meteors as mt\nimport torch\nimport numpy as np\n\nsentinel_central_wavelengths = [\n    442.7,\n    492.4,\n    559.8,\n    664.6,\n    704.1,\n    740.5,\n    782.8,\n    832.8,\n    864.7,\n    945.1,\n    1373.5,\n    1613.7,\n    2202.4,\n]\n</code></pre> <pre><code>hsi_image = mt.HSI(\n    image=img_local.values.astype(float) / img_local.values.max(), wavelengths=sentinel_central_wavelengths\n)\n\nmt.visualize.visualize_hsi(hsi_image)\n</code></pre> <pre><code>&lt;Axes: &gt;\n</code></pre> <pre><code>explainable_model = mt.models.ExplainableModel(model.predict, problem_type=\"segmentation\")\n</code></pre> <pre><code>segmentation_mask_raw = explainable_model(hsi_image.image)\nsegmentation_mask_raw = torch.tensor(segmentation_mask_raw)\n\nplt.imshow(segmentation_mask_raw[0])\nplt.axis(\"off\")\n</code></pre> <pre><code>C:\\Users\\tymot\\AppData\\Local\\Temp\\ipykernel_34972\\2061039508.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  segmentation_mask_raw = torch.tensor(segmentation_mask_raw)\n\n\n\n\n\n(-0.5, 915.5, 621.5, -0.5)\n</code></pre> <p>In order to create explanations we need to transform the problem into a simpler regression problem using a postprocessing function.</p> <pre><code>from meteors.utils import agg_segmentation_postprocessing\n\npostprocessing = agg_segmentation_postprocessing(classes_numb=4)\n</code></pre> <pre><code>binary_mask = torch.ones_like(segmentation_mask_raw).to(torch.bool)\n\naggregated_segmentation_mask = postprocessing(segmentation_mask_raw, binary_mask)\n</code></pre> <pre><code>plt.bar(np.arange(4), aggregated_segmentation_mask)\nplt.xticks(np.arange(4), [\"clear\", \"thick cloud\", \"thin cloud\", \"shadow\"])\nplt.title(\"Aggregated cloudSEN12 segmentation\")\nplt.ylabel(\"Number of pixels\")\nplt.xlabel(\"Class\")\n</code></pre> <pre><code>Text(0.5, 0, 'Class')\n</code></pre> <p></p> <p>having transformed the segmentation mask produced by the model into a 1d tensor, we are ready to create explanations for this problem type!</p>"},{"location":"tutorials/lime/#spatial-analysis","title":"Spatial analysis","text":"<pre><code>lime = mt.attr.Lime(explainable_model=explainable_model)\n\nseg_mask = lime.get_segmentation_mask(hsi_image, segmentation_method=\"slic\", num_interpret_features=100)\n\nattributes = lime.get_spatial_attributes(\n    hsi_image, target=0, segmentation_mask=seg_mask, postprocessing_segmentation_output=postprocessing\n)\n</code></pre> <pre><code>\u001b[32m2024-09-25 21:57:53.233\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.attr.explainer\u001b[0m:\u001b[36mdevice\u001b[0m:\u001b[36m153\u001b[0m - \u001b[33m\u001b[1mNot a torch model, setting device to cpu\u001b[0m\n</code></pre> <pre><code>fig, ax = mt.visualize.visualize_spatial_attributes(attributes, use_pyplot=False)\nfig.suptitle(\"LIME explanation for the UNetMobV2_V1 model and class 0 (clear)\")\nax[2].clear()\nax[2].imshow(segmentation_mask_raw[0])\nax[2].axis(\"off\")\nax[2].set_title(\"UNetMobV2_V1 segmentation\")\nprint(f\"Score of the attribution: {attributes.score}\")\n</code></pre> <pre><code>Score of the attribution: 1.0\n</code></pre> <pre><code>attributes = lime.get_spatial_attributes(\n    hsi_image, target=1, segmentation_mask=seg_mask, postprocessing_segmentation_output=postprocessing, num_samples=100\n)\n\nfig, ax = mt.visualize.visualize_spatial_attributes(attributes, use_pyplot=False)\nfig.suptitle(\"LIME explanation for the UNetMobV2_V1 model and class 1 (thick cloud)\")\nax[2].clear()\nax[2].imshow(segmentation_mask_raw[0])\nax[2].axis(\"off\")\nax[2].set_title(\"UNetMobV2_V1 segmentation\")\nprint(f\"Score of the attribution: {attributes.score}\")\n</code></pre> <pre><code>c:\\Users\\tymot\\miniconda3\\envs\\xai-tutorial\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.912e+04, tolerance: 4.610e+04\n  model = cd_fast.enet_coordinate_descent(\n\n\nScore of the attribution: 1.0\n</code></pre>"},{"location":"tutorials/lime/#spectral-analysis","title":"Spectral analysis","text":"<pre><code>band_indices = {\n    \"B01\": 0,\n    \"B02\": 1,\n    \"B03\": 2,\n    \"B04\": 3,\n    \"B05\": 4,\n    \"B06\": 5,\n    \"B07\": 6,\n    \"B08\": 7,\n    \"B8A\": 8,\n    \"B09\": 9,\n    \"B10\": 10,\n    \"B11\": 11,\n    \"B12\": 12,\n}\n\nband_mask, band_names = lime.get_band_mask(hsi_image, band_indices=band_indices)\n\nattributes = lime.get_spectral_attributes(\n    hsi_image, target=0, band_mask=band_mask, band_names=band_names, postprocessing_segmentation_output=postprocessing\n)\n</code></pre> <pre><code>c:\\Users\\tymot\\miniconda3\\envs\\xai-tutorial\\Lib\\site-packages\\meteors\\attr\\lime_base.py:738: UserWarning: Minimum element in feature mask is not 0, shifting indices to start at 0.\n  warnings.warn(\"Minimum element in feature mask is not 0, shifting indices to\" \" start at 0.\")\n\n\n\u001b[32m2024-09-25 22:01:37.043\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.utils\u001b[0m:\u001b[36madjust_shape\u001b[0m:\u001b[36m112\u001b[0m - \u001b[33m\u001b[1mThe source tensor could not be broadcasted to match the target tensor shape. Broadcasting target tensor to match the source tensor shape.\u001b[0m\n\u001b[32m2024-09-25 22:01:39.004\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.utils\u001b[0m:\u001b[36madjust_shape\u001b[0m:\u001b[36m112\u001b[0m - \u001b[33m\u001b[1mThe source tensor could not be broadcasted to match the target tensor shape. Broadcasting target tensor to match the source tensor shape.\u001b[0m\n\u001b[32m2024-09-25 22:01:40.036\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.utils\u001b[0m:\u001b[36madjust_shape\u001b[0m:\u001b[36m112\u001b[0m - \u001b[33m\u001b[1mThe source tensor could not be broadcasted to match the target tensor shape. Broadcasting target tensor to match the source tensor shape.\u001b[0m\n</code></pre> <pre><code>mt.visualize.visualize_spectral_attributes(attributes)\nprint(f\"Score of the attribution: {attributes.score}\")\nfig = plt.gcf()\nfig.suptitle(\"LIME explanation for the UNetMobV2_V1 model and class 0 (clear)\")\nplt.show()\n</code></pre> <pre><code>Score of the attribution: 1.0\n</code></pre> <p>the bands B11, B08 and B04 are used usually to segment the clouds. This hints that possibly, this method of explanations is valid</p> <pre><code>attributes = lime.get_spectral_attributes(\n    hsi_image,\n    target=1,\n    band_mask=band_mask,\n    band_names=band_names,\n    postprocessing_segmentation_output=postprocessing,\n    n_samples=100,\n    perturbations_per_eval=20,\n)\n\nmt.visualize.visualize_spectral_attributes(attributes)\nprint(f\"Score of the attribution: {attributes.score}\")\nfig = plt.gcf()\nfig.suptitle(\"LIME explanation for the UNetMobV2_V1 model and class 1 (thick cloud)\")\nplt.show()\n</code></pre> <pre><code>c:\\Users\\tymot\\miniconda3\\envs\\xai-tutorial\\Lib\\site-packages\\meteors\\attr\\lime_base.py:738: UserWarning: Minimum element in feature mask is not 0, shifting indices to start at 0.\n  warnings.warn(\"Minimum element in feature mask is not 0, shifting indices to\" \" start at 0.\")\n\n\n\u001b[32m2024-09-25 22:00:54.608\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.utils\u001b[0m:\u001b[36madjust_shape\u001b[0m:\u001b[36m112\u001b[0m - \u001b[33m\u001b[1mThe source tensor could not be broadcasted to match the target tensor shape. Broadcasting target tensor to match the source tensor shape.\u001b[0m\n\u001b[32m2024-09-25 22:01:04.547\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.utils\u001b[0m:\u001b[36madjust_shape\u001b[0m:\u001b[36m112\u001b[0m - \u001b[33m\u001b[1mThe source tensor could not be broadcasted to match the target tensor shape. Broadcasting target tensor to match the source tensor shape.\u001b[0m\n\u001b[32m2024-09-25 22:01:14.198\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.utils\u001b[0m:\u001b[36madjust_shape\u001b[0m:\u001b[36m112\u001b[0m - \u001b[33m\u001b[1mThe source tensor could not be broadcasted to match the target tensor shape. Broadcasting target tensor to match the source tensor shape.\u001b[0m\n\u001b[32m2024-09-25 22:01:23.991\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.utils\u001b[0m:\u001b[36madjust_shape\u001b[0m:\u001b[36m112\u001b[0m - \u001b[33m\u001b[1mThe source tensor could not be broadcasted to match the target tensor shape. Broadcasting target tensor to match the source tensor shape.\u001b[0m\n\u001b[32m2024-09-25 22:01:33.421\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmeteors.utils\u001b[0m:\u001b[36madjust_shape\u001b[0m:\u001b[36m112\u001b[0m - \u001b[33m\u001b[1mThe source tensor could not be broadcasted to match the target tensor shape. Broadcasting target tensor to match the source tensor shape.\u001b[0m\nScore of the attribution: 0.5755875110626221\n</code></pre> <p></p> <pre><code>\n</code></pre>"}]}